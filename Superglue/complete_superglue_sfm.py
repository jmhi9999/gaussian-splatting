import glob
from tkinter import Image
import numpy as np
import cv2
import torch
from pathlib import Path
from scipy.optimize import least_squares
import matplotlib.pyplot as plt
from collections import defaultdict
import sys
import time
import gc
from scipy.spatial.distance import cdist
import networkx as nx

# Import separated modules
from utils.imports_utils import get_3dgs_imports, test_pipeline_availability, CLIP_AVAILABLE
from utils.track_manager import TrackManager
from utils.pose_optimization import PoseGraphOptimizer
from utils.bundle_adjustment import BundleAdjuster
from utils.parallel_utils import ParallelExecutor
from utils.auto_tuner import AutoTuner
from utils.adaptive_matcher import AdaptiveMatcher
from utils.feature_matching import FeatureExtractor, Matcher

try:
    from models.matching import Matching
    from models.utils import frame2tensor
    SUPERGLUE_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  SuperGlue modules not available: {e}")
    SUPERGLUE_AVAILABLE = False

# íŒŒì´í”„ë¼ì¸ ê°€ìš©ì„± í…ŒìŠ¤íŠ¸ ì‹¤í–‰
PIPELINE_AVAILABLE = test_pipeline_availability()

class SuperGlue3DGSPipeline:
    def __init__(self, config=None, device='cuda'):
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.config = config or {}
        
        # SuperGlue ëª¨ë¸ ë¡œë“œ
        self.superglue_available = SUPERGLUE_AVAILABLE
        if self.superglue_available:
            try:
                self.matching = Matching(self.config).eval().to(self.device)
                print(f"âœ“ SuperGlue matching model loaded on {self.device}")
            except Exception as e:
                print(f"âš ï¸  SuperGlue model not available: {e}")
                self.matching = None
                self.superglue_available = False
        else:
            self.matching = None
            print("âš ï¸  SuperGlue not available")
        
        # ê° ë‹¨ê³„ë³„ ê°ì²´ ìƒì„± (ë¶„ë¦¬ëœ ëª¨ë“ˆë“¤ ì‚¬ìš©)
        self.feature_extractor = FeatureExtractor(self.config, self.device, self.matching)
        self.matcher = Matcher(self.config, self.device, self.matching)
        self.adaptive_matcher = AdaptiveMatcher(self.config, self.device)
        self.track_manager = TrackManager()
        self.pose_graph_optimizer = PoseGraphOptimizer()
        self.bundle_adjuster = BundleAdjuster(loss_type=self.config.get('ba_loss', 'huber'), max_iter=self.config.get('ba_max_iter', 50))
        self.parallel_executor = ParallelExecutor(max_workers=self.config.get('max_workers', 4))
        self.auto_tuner = AutoTuner(self.config)
        # ê¸°ì¡´ ë³€ìˆ˜ë“¤ ìœ ì§€
        self.cameras = {}
        self.points_3d = {}
        self.image_features = {}
        self.matches = {}
        self.camera_graph = defaultdict(list)
        self.point_observations = defaultdict(list)
        self.quality_metrics = {}
        
        print(f'âœ… SuperGlue 3DGS Pipeline initialized on {self.device}')
        if not self.superglue_available:
            print('   Running in fallback mode (SuperGlue not available)')
    
    def process_images_to_3dgs(self, image_dir, output_dir, max_images=120):
        """ì´ë¯¸ì§€ë“¤ì„ 3DGS í˜•ì‹ìœ¼ë¡œ ì²˜ë¦¬ (êµ¬ì¡°í™”/ë¦¬íŒ©í† ë§ ë²„ì „)"""
        # ì‹œì‘ ì‹œê°„ ê¸°ë¡
        self.start_time = time.time()
        
        # 1. ì´ë¯¸ì§€ ìˆ˜ì§‘
        image_paths = self._collect_images(image_dir, max_images)
        if not image_paths:
            raise RuntimeError("No images found")
        print(f"Found {len(image_paths)} images")

        # 2. íŠ¹ì§•ì  ì¶”ì¶œ
        print("  Extracting features...")
        self._extract_all_features(image_paths)

        # 3. ë§¤ì¹­ 
        print("  Matching images...")
        self._intelligent_matching()

        # 4. Track ìƒì„± ë° í•„í„°ë§
        self.track_manager.build_tracks(self.matches, self.image_features)
        self.track_manager.filter_tracks(min_views=3)

        # 5. ğŸ”§ FIX: ì¹´ë©”ë¼ ê·¸ë˜í”„ êµ¬ì¶• (ë§¤ì¹­ ê²°ê³¼ë¡œë¶€í„°)
        print("  Building camera graph from matches...")
        self._build_camera_graph_from_matches()

        # 6. ğŸ”§ FIX: ì¹´ë©”ë¼ í¬ì¦ˆ ì¶”ì • (ê°œì„ ëœ ë²„ì „ ì‚¬ìš©)
        print("  Estimating camera poses...")
        self._estimate_camera_poses_improved()
        
        # ğŸ”§ DEBUG: ì¹´ë©”ë¼ í¬ì¦ˆ ì¶”ì • ê²°ê³¼ í™•ì¸
        print(f"    DEBUG: After pose estimation, cameras dictionary has {len(self.cameras)} cameras")
        if len(self.cameras) > 0:
            print(f"    DEBUG: First camera keys: {list(self.cameras[0].keys()) if 0 in self.cameras else 'Camera 0 not found'}")
        else:
            print("    DEBUG: âš ï¸ Cameras dictionary is empty! This will cause triangulation to fail.")
            print("    DEBUG: Creating basic camera poses as fallback...")
            # ğŸ”§ FIX: ê¸°ë³¸ ì¹´ë©”ë¼ í¬ì¦ˆ ìƒì„±
            for cam_id in range(len(self.image_features)):
                angle = cam_id * (2 * np.pi / len(self.image_features))
                radius = 3.0
                
                R = np.array([[np.cos(angle), 0, np.sin(angle)],
                             [0, 1, 0],
                             [-np.sin(angle), 0, np.cos(angle)]], dtype=np.float32)
                T = np.array([radius * np.sin(angle), 0, radius * (1 - np.cos(angle))], dtype=np.float32)
                
                self.cameras[cam_id] = {
                    'R': R,
                    'T': T,
                    'K': self._estimate_intrinsics(cam_id)
                }
            print(f"    DEBUG: Created {len(self.cameras)} fallback camera poses")

        # 7. Multi-view Triangulation (ì´ì œ ì¹´ë©”ë¼ í¬ì¦ˆê°€ ìˆìŒ)
        print("  Triangulating tracks...")
        triangulated_points = self.track_manager.triangulate_tracks(self.cameras, self.image_features)
        # triangulated_pointsë¥¼ self.points_3dì— ë°˜ì˜
        self.points_3d.update(triangulated_points)
        
        # ğŸ”§ DEBUG: ì‚¼ê°ì¸¡ëŸ‰ ê²°ê³¼ í™•ì¸
        print(f"    DEBUG: Triangulation returned {len(triangulated_points)} points")
        if len(triangulated_points) == 0:
            print("    DEBUG: âš ï¸ No points triangulated! Using fallback method...")
            # ğŸ”§ FIX: ê¸°ì¡´ ë°©ì‹ì˜ ì‚¼ê°ì¸¡ëŸ‰ ì‚¬ìš©
            n_points = self._triangulate_all_points_robust()
            print(f"    DEBUG: Fallback triangulation created {n_points} points")
            
            # ğŸ”§ FIX: í¬ì¸íŠ¸ ê´€ì°° ë°ì´í„° ì„¤ì •
            if len(self.points_3d) > 0:
                for point_id, point_data in self.points_3d.items():
                    if point_id not in self.point_observations:
                        self.point_observations[point_id] = point_data.get('observations', [])

        # 7.5. ğŸ”§ FIX: Point observations ì„¤ì • (Bundle Adjustmentìš©)
        print("  Setting up point observations...")
        self._setup_point_observations_from_triangulation(triangulated_points)

        # 8. Pose Graph Optimization
        self.pose_graph_optimizer.build_pose_graph(self.matches, self.cameras)
        self.pose_graph_optimizer.remove_outlier_edges(min_matches=10)
        pose_tree = self.pose_graph_optimizer.get_spanning_tree()

        # 9. Bundle Adjustment
        ba_result = self.bundle_adjuster.run(self.cameras, self.points_3d, self.point_observations, callback=self.bundle_adjuster.monitor_quality)

        # 10. í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
        self._compute_quality_metrics()

        # 11. ê²°ê³¼ ì €ì¥ ë° ë§ˆë¬´ë¦¬
        scene_info = self._create_3dgs_scene_info(image_paths)
        self._save_3dgs_format(scene_info, output_dir)
        self._cleanup_memory()
        return scene_info
    
    def _monitor_memory(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ (psutil ì—†ì´)"""
        try:
            # ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ (psutil ì—†ì´)
            if torch.cuda.is_available():
                # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
                gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB
                print(f"    GPU Memory: {gpu_memory:.1f} MB")
            else:
                # CPU ë©”ëª¨ë¦¬ëŠ” ê°„ë‹¨í•œ ì¶”ì •
                print(f"    Memory monitoring: CPU mode")
        except:
            print(f"    Memory monitoring: Not available")
    
    def _cleanup_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    def _check_colmap_reconstruction(self, output_dir):
        """COLMAP reconstruction ì¡´ì¬ ì—¬ë¶€ ë° ìœ íš¨ì„± í™•ì¸"""
        try:
            reconstruction_path = Path(output_dir) / "sparse" / "0"
            cameras_bin = reconstruction_path / "cameras.bin"
            images_bin = reconstruction_path / "images.bin"
            points3d_bin = reconstruction_path / "points3D.bin"
            
            if not cameras_bin.exists() or not images_bin.exists():
                print(f"    No COLMAP reconstruction found at: {reconstruction_path}")
                return False
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            if cameras_bin.stat().st_size == 0 or images_bin.stat().st_size == 0:
                print(f"    COLMAP files are empty, removing corrupted files")
                self._cleanup_corrupted_colmap_files(reconstruction_path)
                return False
            
            # íŒŒì¼ í˜•ì‹ ê²€ì¦ ì‹œë„
            try:
                from scene.colmap_loader import read_intrinsics_binary, read_extrinsics_binary
                
                # cameras.bin ê²€ì¦
                cameras = read_intrinsics_binary(str(cameras_bin))
                if len(cameras) == 0:
                    print(f"    cameras.bin is empty or corrupted")
                    self._cleanup_corrupted_colmap_files(reconstruction_path)
                    return False
                
                # images.bin ê²€ì¦
                images = read_extrinsics_binary(str(images_bin))
                if len(images) == 0:
                    print(f"    images.bin is empty or corrupted")
                    self._cleanup_corrupted_colmap_files(reconstruction_path)
                    return False
                
                print(f"    Found valid COLMAP reconstruction at: {reconstruction_path}")
                print(f"    - {len(cameras)} cameras, {len(images)} images")
                return True
                
            except Exception as e:
                print(f"    COLMAP files are corrupted: {e}")
                self._cleanup_corrupted_colmap_files(reconstruction_path)
                return False
                
        except Exception as e:
            print(f"    COLMAP reconstruction check failed: {e}")
            return False
    
    def _cleanup_corrupted_colmap_files(self, reconstruction_path):
        """ì†ìƒëœ COLMAP íŒŒì¼ë“¤ ì •ë¦¬"""
        try:
            for file_name in ["cameras.bin", "images.bin", "points3D.bin"]:
                file_path = reconstruction_path / file_name
                if file_path.exists():
                    file_path.unlink()
                    print(f"    Deleted corrupted {file_name}")
        except Exception as e:
            print(f"    Failed to cleanup corrupted files: {e}")
    
    def _process_with_colmap_hybrid(self, image_paths, output_dir):
        """COLMAP reconstructionì„ í™œìš©í•œ í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬"""
        print("  ğŸ”„ Using COLMAP + SuperGlue hybrid approach")
        
        try:
            # 1. COLMAP reconstruction ë¡œë“œ
            colmap_data = self._load_colmap_reconstruction(output_dir)
            if colmap_data is None:
                print("    Failed to load COLMAP reconstruction, falling back to SuperGlue")
                return self._process_superglue_only(image_paths, output_dir)
            
            cameras, images, points3d = colmap_data
            print(f"    Loaded {len(cameras)} cameras, {len(images)} images, {len(points3d)} points from COLMAP")
            
            # 2. COLMAP ë°ì´í„°ë¥¼ SuperGlue í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            self._convert_colmap_to_superglue_format(cameras, images, points3d, image_paths)
            
            # 3. SuperGlue íŠ¹ì§•ì  ì¶”ì¶œ (COLMAP í¬ì¦ˆ ê°œì„ ìš©)
            self._extract_all_features(image_paths)
            
            # 4. COLMAP í¬ì¦ˆë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ê°œì„ ëœ ë§¤ì¹­
            self._intelligent_matching_with_colmap_poses()
            
            # 5. Bundle Adjustment (COLMAP ì´ˆê¸°ê°’ ì‚¬ìš©)
            self._bundle_adjustment_with_colmap_initialization()
            
            # 6. 3DGS SceneInfo ìƒì„±
            scene_info = self._create_3dgs_scene_info(image_paths)
            
            # 7. 3DGS í˜•ì‹ìœ¼ë¡œ ì €ì¥ (ì•ˆì „í•œ ë°©ë²•)
            try:
                self._save_3dgs_format(scene_info, output_dir)
            except Exception as save_error:
                print(f"    Warning: Failed to save 3DGS format: {save_error}")
                import traceback
                traceback.print_exc()
                # ê¸°ë³¸ í…ìŠ¤íŠ¸ íŒŒì¼ë§Œ ì €ì¥
                self._save_basic_format(scene_info, output_dir)
            
            return scene_info
            
        except Exception as e:
            print(f"    Hybrid processing failed: {e}")
            import traceback
            traceback.print_exc()
            print("    Falling back to SuperGlue only")
            return self._process_superglue_only(image_paths, output_dir)
    
    def _save_basic_format(self, scene_info, output_dir):
        """ê¸°ë³¸ í…ìŠ¤íŠ¸ íŒŒì¼ë§Œ ì €ì¥ (fallback)"""
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True, parents=True)
        
        sparse_dir = output_dir / "sparse" / "0"
        sparse_dir.mkdir(exist_ok=True, parents=True)
        
        images_dir = output_dir / "images"
        images_dir.mkdir(exist_ok=True)
        
        # í…ìŠ¤íŠ¸ íŒŒì¼ë§Œ ì €ì¥
        self._write_cameras_txt(scene_info.train_cameras + scene_info.test_cameras, 
                               sparse_dir / "cameras.txt")
        self._write_images_txt(scene_info.train_cameras + scene_info.test_cameras, 
                              sparse_dir / "images.txt")
        self._write_points3d_ply(scene_info.point_cloud, sparse_dir / "points3D.ply")
        
        print(f"    Saved basic format to {output_dir}")
    
    def _load_colmap_reconstruction(self, output_dir):
        """COLMAP reconstruction ë¡œë“œ"""
        try:
            reconstruction_path = Path(output_dir) / "sparse" / "0"
            print(f"    Checking reconstruction path: {reconstruction_path}")
            
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            cameras_bin = reconstruction_path / "cameras.bin"
            images_bin = reconstruction_path / "images.bin"
            points3d_bin = reconstruction_path / "points3D.bin"
            
            print(f"    cameras.bin exists: {cameras_bin.exists()}")
            print(f"    images.bin exists: {images_bin.exists()}")
            print(f"    points3D.bin exists: {points3d_bin.exists()}")
            
            if not cameras_bin.exists() or not images_bin.exists() or not points3d_bin.exists():
                print("    Missing required COLMAP files")
                return None
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            print(f"    cameras.bin size: {cameras_bin.stat().st_size} bytes")
            print(f"    images.bin size: {images_bin.stat().st_size} bytes")
            print(f"    points3D.bin size: {points3d_bin.stat().st_size} bytes")
            
            # COLMAP ëª¨ë“ˆ import
            try:
                from scene.colmap_loader import read_intrinsics_binary, read_extrinsics_binary, read_points3D_binary
                print("    COLMAP loader imported successfully")
            except ImportError as e:
                print(f"    COLMAP loader import failed: {e}")
                return None
            
            # ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„° ë¡œë“œ
            print("    Loading cameras.bin...")
            try:
                cameras = read_intrinsics_binary(str(cameras_bin))
                print(f"    Loaded {len(cameras)} cameras")
            except Exception as e:
                print(f"    Failed to load cameras.bin: {e}")
                return None
            
            # ì¹´ë©”ë¼ ì™¸ë¶€ íŒŒë¼ë¯¸í„° ë¡œë“œ
            print("    Loading images.bin...")
            try:
                images = read_extrinsics_binary(str(images_bin))
                print(f"    Loaded {len(images)} images")
            except Exception as e:
                print(f"    Failed to load images.bin: {e}")
                return None
            
            # 3D í¬ì¸íŠ¸ ë¡œë“œ
            print("    Loading points3D.bin...")
            try:
                points3d = read_points3D_binary(str(points3d_bin))
                print(f"    Loaded {len(points3d)} 3D points")
            except Exception as e:
                print(f"    Failed to load points3D.bin: {e}")
                return None
            
            return cameras, images, points3d
            
        except Exception as e:
            print(f"    Failed to load COLMAP reconstruction: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _convert_colmap_to_superglue_format(self, cameras, images, points3d, image_paths):
        """COLMAP ë°ì´í„°ë¥¼ SuperGlue í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        print("    Converting COLMAP data to SuperGlue format...")
        
        # ì¹´ë©”ë¼ í¬ì¦ˆ ì„¤ì •
        for image_id, image_data in images.items():
            if image_id < len(image_paths):
                # COLMAP í¬ì¦ˆë¥¼ SuperGlue í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                R = image_data.qvec2rotmat()  # ì¿¼í„°ë‹ˆì–¸ì„ íšŒì „ í–‰ë ¬ë¡œ
                T = image_data.tvec  # ì´ë™ ë²¡í„°
                
                # ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„°
                camera_id = image_data.camera_id
                if camera_id in cameras:
                    camera = cameras[camera_id]
                    K = self._colmap_camera_to_intrinsics(camera)
                else:
                    K = self._estimate_intrinsics(image_id)
                
                self.cameras[image_id] = {
                    'R': R.astype(np.float32),
                    'T': T.astype(np.float32),
                    'K': K,
                    'image_path': str(image_paths[image_id])
                }
        
        print(f"    Converted {len(self.cameras)} camera poses from COLMAP")
    
    def _colmap_camera_to_intrinsics(self, camera):
        """COLMAP ì¹´ë©”ë¼ë¥¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„°ë¡œ ë³€í™˜"""
        if camera.model == "PINHOLE":
            fx, fy, cx, cy = camera.params
            K = np.array([
                [fx, 0, cx],
                [0, fy, cy],
                [0, 0, 1]
            ], dtype=np.float32)
        else:
            # ê¸°ë³¸ PINHOLE ëª¨ë¸ ì‚¬ìš©
            width, height = camera.width, camera.height
            focal = max(width, height) * 0.9
            cx, cy = width / 2, height / 2
            K = np.array([
                [focal, 0, cx],
                [0, focal, cy],
                [0, 0, 1]
            ], dtype=np.float32)
        
        return K
    
    def _intelligent_matching_with_colmap_poses(self):
        """COLMAP í¬ì¦ˆë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ê°œì„ ëœ ë§¤ì¹­"""
        print("    Using COLMAP poses for improved matching...")
        
        # COLMAP í¬ì¦ˆê°€ ìˆìœ¼ë©´ ë” ì ê·¹ì ì¸ ë§¤ì¹­
        n_images = len(self.image_features)
        
        # ì „ì—­ descriptors ê³„ì‚°
        self._compute_global_descriptors()
        
        # COLMAP í¬ì¦ˆ ê¸°ë°˜ ë§¤ì¹­ (ë” ì ê·¹ì )
        for i in range(n_images):
            for j in range(i+1, n_images):
                if i in self.cameras and j in self.cameras:
                    # COLMAP í¬ì¦ˆ ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°
                    pose_similarity = self._compute_pose_similarity(i, j)
                    
                    if pose_similarity > 0.1:  # í¬ì¦ˆê°€ ìœ ì‚¬í•œ ê²½ìš°
                        matches = self._match_pair_superglue(i, j)
                        if len(matches) > 1:
                            self.matches[(i, j)] = matches
                            self.camera_graph[i].append(j)
                            self.camera_graph[j].append(i)
    
    def _compute_pose_similarity(self, cam_i, cam_j):
        """ë‘ ì¹´ë©”ë¼ í¬ì¦ˆ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            R_i, T_i = self.cameras[cam_i]['R'], self.cameras[cam_i]['T']
            R_j, T_j = self.cameras[cam_j]['R'], self.cameras[cam_j]['T']
            
            # íšŒì „ ì°¨ì´
            R_diff = R_i @ R_j.T
            rotation_error = np.arccos(np.clip((np.trace(R_diff) - 1) / 2, -1, 1))
            
            # ì´ë™ ì°¨ì´
            translation_error = np.linalg.norm(T_i - T_j)
            
            # ì¢…í•© ìœ ì‚¬ë„ (ì‘ì„ìˆ˜ë¡ ìœ ì‚¬)
            similarity = 1.0 / (1.0 + rotation_error + translation_error * 0.1)
            
            return similarity
            
        except Exception:
            return 0.0
    
    def _bundle_adjustment_with_colmap_initialization(self):
        """COLMAP ì´ˆê¸°ê°’ì„ ì‚¬ìš©í•œ Bundle Adjustment"""
        print("    Using COLMAP poses as initialization for Bundle Adjustment...")
        
        # COLMAP í¬ì¦ˆê°€ ì´ë¯¸ ìˆìœ¼ë¯€ë¡œ ë” ë³´ìˆ˜ì ì¸ BA
        self._bundle_adjustment_robust(max_iterations=30)  # ë°˜ë³µ íšŸìˆ˜ ì¤„ì„
    
    def _process_superglue_only(self, image_paths, output_dir):
        """SuperGlueë§Œ ì‚¬ìš©í•œ ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹)"""
        print("    Using SuperGlue-only processing...")
        
        # íŠ¹ì§•ì  ì¶”ì¶œ
        self._extract_all_features(image_paths)
        
        # ë§¤ì¹­
        self._intelligent_matching()
        
        # ì¹´ë©”ë¼ í¬ì¦ˆ ì¶”ì •
        self._estimate_camera_poses_robust()
        
        # ì‚¼ê°ì¸¡ëŸ‰
        n_points = self._triangulate_all_points_robust()
        
        # Bundle Adjustment
        self._bundle_adjustment_robust()
        
        # í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
        self._compute_quality_metrics()
        
        # 3DGS SceneInfo ìƒì„±
        scene_info = self._create_3dgs_scene_info(image_paths)
        
        # 3DGS í˜•ì‹ìœ¼ë¡œ ì €ì¥
        self._save_3dgs_format(scene_info, output_dir)
        
        return scene_info
    
    def _compute_quality_metrics(self):
        """í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        try:
            # í¬ì¦ˆ ì¶”ì • ì„±ê³µë¥ 
            total_cameras = len(self.image_features)
            estimated_cameras = len([cam for cam in self.cameras.values() if 'R' in cam])
            self.quality_metrics['pose_estimation_success_rate'] = estimated_cameras / total_cameras
            
            # í‰ê·  ë§¤ì¹­ ìˆ˜
            if self.matches:
                avg_matches = np.mean([len(matches) for matches in self.matches.values()])
                self.quality_metrics['average_matches_per_pair'] = avg_matches
            
            # ì²˜ë¦¬ ì‹œê°„
            self.quality_metrics['total_processing_time'] = time.time() - self.start_time
            
            print(f"\n=== Quality Metrics ===")
            print(f"Pose estimation success rate: {self.quality_metrics['pose_estimation_success_rate']:.2%}")
            print(f"Average matches per pair: {self.quality_metrics['average_matches_per_pair']:.1f}")
            print(f"Total processing time: {self.quality_metrics['total_processing_time']:.1f}s")
            
            # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ê°€ëŠ¥í•œ ê²½ìš°)
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024
                print(f"GPU Memory usage: {gpu_memory:.1f} MB")
            
        except Exception as e:
            print(f"Quality metrics calculation failed: {e}")
    
    def _create_fallback_scene_info(self, image_paths):
        """ê°œì„ ëœ fallback scene ìƒì„±"""
        try:
            # Lazy import 3DGS modules
            CameraInfo, SceneInfo, BasicPointCloud = get_3dgs_imports()
            if CameraInfo is None:
                raise ImportError("3DGS modules not available")
            
            print(f"ğŸ“¸ Creating fallback scene for {len(image_paths)} images")
            
            # ì¹´ë©”ë¼ ì •ë³´ ìƒì„±
            cam_infos = []
            for i, image_path in enumerate(image_paths):
                try:
                    # ì´ë¯¸ì§€ í¬ê¸° í™•ì¸
                    image = Image.open(image_path)
                    width, height = image.size
                    
                    # ì›í˜• ë°°ì¹˜ë¡œ ì¹´ë©”ë¼ ë°°ì¹˜
                    angle = i * (2 * np.pi / len(image_paths))
                    radius = 3.0
                    
                    # ì¹´ë©”ë¼ í¬ì¦ˆ (ì›ì„ ë°”ë¼ë³´ë„ë¡)
                    camera_pos = np.array([
                        radius * np.cos(angle),
                        0.0,  # ë†’ì´ ê³ ì •
                        radius * np.sin(angle)
                    ])
                    
                    # ì›ì ì„ í–¥í•˜ëŠ” ë°©í–¥
                    look_at = np.array([0.0, 0.0, 0.0])
                    up = np.array([0.0, 1.0, 0.0])
                    
                    # ì¹´ë©”ë¼ íšŒì „ í–‰ë ¬ ê³„ì‚°
                    forward = look_at - camera_pos
                    forward = forward / np.linalg.norm(forward)
                    right = np.cross(forward, up)
                    right = right / np.linalg.norm(right)
                    up = np.cross(right, forward)
                    
                    R = np.array([right, up, -forward]).T  # OpenCV ì»¨ë²¤ì…˜
                    T = camera_pos
                    
                    # FOV ê³„ì‚° (ë” ì•ˆì „í•œ ê°’ë“¤)
                    focal_length = max(width, height) * 0.8
                    FovX = 2 * np.arctan(width / (2 * focal_length))
                    FovY = 2 * np.arctan(height / (2 * focal_length))
                    
                    # í…ŒìŠ¤íŠ¸ ì¹´ë©”ë¼ ì„ íƒ (ë” ê· ë“±í•˜ê²Œ ë¶„ì‚°)
                    is_test = (i % 8 == 0)  # 8ê°œë§ˆë‹¤ 1ê°œì”© í…ŒìŠ¤íŠ¸
                    
                    cam_info = CameraInfo(
                        uid=i,
                        R=R.astype(np.float32),
                        T=T.astype(np.float32),
                        FovY=float(FovY),
                        FovX=float(FovX),
                        image_path=str(image_path),
                        image_name=Path(image_path).name,
                        width=int(width),
                        height=int(height),
                        depth_params=None,
                        depth_path="",
                        is_test=is_test
                    )
                    cam_infos.append(cam_info)
                    
                except Exception as e:
                    print(f"    Warning: Failed to process {image_path}: {e}")
                    continue
            
            if not cam_infos:
                raise ValueError("No valid cameras created")
            
            # ê°œì„ ëœ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ìƒì„±
            n_points = 12000  # 8000 â†’ 12000ë¡œ ì¦ê°€
            
            # ë” í˜„ì‹¤ì ì¸ 3D í¬ì¸íŠ¸ ë¶„í¬
            # êµ¬í˜• ë¶„í¬ + ì¼ë¶€ í‰ë©´ êµ¬ì¡°
            points_sphere = np.random.randn(n_points // 2, 3).astype(np.float32)
            points_sphere = points_sphere / np.linalg.norm(points_sphere, axis=1, keepdims=True) * 3.0  # 2.0 â†’ 3.0
            
            # í‰ë©´ êµ¬ì¡° ì¶”ê°€ (ë°”ë‹¥ë©´)
            points_plane = np.random.randn(n_points // 2, 3).astype(np.float32)
            points_plane[:, 1] = np.abs(points_plane[:, 1]) * 0.2 - 1.0  # ë°”ë‹¥ ê·¼ì²˜ (0.1 â†’ 0.2, -0.5 â†’ -1.0)
            points_plane[:, [0, 2]] *= 2.0  # 1.5 â†’ 2.0
            
            points = np.vstack([points_sphere, points_plane])
            
            # ë” í˜„ì‹¤ì ì¸ ìƒ‰ìƒ (íšŒìƒ‰ì¡° + ì•½ê°„ì˜ ìƒ‰ìƒ)
            colors = np.random.rand(n_points, 3).astype(np.float32)
            colors = colors * 0.5 + 0.3  # 0.3-0.8 ë²”ìœ„
            
            # ë²•ì„  ë²¡í„° (ë¬´ì‘ìœ„ì§€ë§Œ ì •ê·œí™”ë¨)
            normals = np.random.randn(n_points, 3).astype(np.float32)
            normals = normals / (np.linalg.norm(normals, axis=1, keepdims=True) + 1e-10)
            
            # BasicPointCloud ìƒì„± ì‹œ ì°¨ì› í™•ì¸
            assert points.shape == (n_points, 3), f"Points shape error: {points.shape}"
            assert colors.shape == (n_points, 3), f"Colors shape error: {colors.shape}"
            assert normals.shape == (n_points, 3), f"Normals shape error: {normals.shape}"
            
            # BasicPointCloudê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ fallback
            try:
                pcd = BasicPointCloud(
                    points=points,
                    colors=colors,
                    normals=normals
                )
            except NameError:
                # BasicPointCloudê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ê°„ë‹¨í•œ í´ë˜ìŠ¤ ìƒì„±
                class BasicPointCloud:
                    def __init__(self, points, colors, normals):
                        self.points = points
                        self.colors = colors
                        self.normals = normals
                
                pcd = BasicPointCloud(
                    points=points,
                    colors=colors,
                    normals=normals
                )
            
            # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
            train_cams = [c for c in cam_infos if not c.is_test]
            test_cams = [c for c in cam_infos if c.is_test]
            
            # NeRF ì •ê·œí™” (ê°œì„ ëœ ë²„ì „)
            if train_cams:
                camera_centers = []
                for cam in train_cams:
                    # ì¹´ë©”ë¼ ì¤‘ì‹¬ ê³„ì‚°
                    center = -cam.R.T @ cam.T
                    camera_centers.append(center)
                
                camera_centers = np.array(camera_centers)
                scene_center = np.mean(camera_centers, axis=0)
                distances = np.linalg.norm(camera_centers - scene_center, axis=1)
                scene_radius = np.max(distances) * 1.2
                
                # ìµœì†Œ/ìµœëŒ€ ì œí•œ
                scene_radius = max(scene_radius, 1.0)
                scene_radius = min(scene_radius, 10.0)
            else:
                scene_center = np.zeros(3)
                scene_radius = 3.0
            
            nerf_normalization = {
                "translate": -scene_center.astype(np.float32),
                "radius": float(scene_radius)
            }
            
            scene_info = SceneInfo(
                point_cloud=pcd,
                train_cameras=train_cams,
                test_cameras=test_cams,
                nerf_normalization=nerf_normalization,
                ply_path="",
                is_nerf_synthetic=False
            )
            
            print(f"âœ“ Fallback scene created:")
            print(f"  - {len(train_cams)} training cameras")
            print(f"  - {len(test_cams)} test cameras")
            print(f"  - {n_points} 3D points")
            print(f"  - Scene radius: {scene_radius:.2f}")
            
            return scene_info
            
        except Exception as e:
            print(f"Failed to create fallback scene: {e}")
            raise
    
    def _collect_images(self, image_dir, max_images):
        """ì´ë¯¸ì§€ ìˆ˜ì§‘ ë° ì •ë ¬"""
        image_dir = Path(image_dir)
        image_paths = []
        
        # ì§€ì›í•˜ëŠ” í™•ì¥ì
        extensions = ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']
        for ext in extensions:
            image_paths.extend(list(image_dir.glob(ext)))
        
        # ì •ë ¬ ë° ì œí•œ
        image_paths.sort()
        return image_paths[:max_images]
    
    def _extract_all_features(self, image_paths):
        """ëª¨ë“  ì´ë¯¸ì§€ì—ì„œ SuperPoint íŠ¹ì§•ì  ì¶”ì¶œ (ìˆ˜ì •ëœ ë²„ì „)"""
        if not self.superglue_available:
            print("  Using fallback feature extraction (SuperGlue not available)")
            return self._extract_features_fallback(image_paths)
        
        for i, image_path in enumerate(image_paths):
            print(f"  {i+1:3d}/{len(image_paths)}: {image_path.name}")
            
            # ì´ë¯¸ì§€ ë¡œë“œ
            image = self._load_image(image_path)
            if image is None:
                continue
            
            # SuperPoint íŠ¹ì§•ì  ì¶”ì¶œ
            inp = frame2tensor(image, self.device)
            with torch.no_grad():
                pred = self.matching.superpoint({'image': inp})
            
            # ê²°ê³¼ ì €ì¥ - ëª¨ë“  í•„ìš”í•œ í‚¤ í¬í•¨
            self.image_features[i] = {
                'keypoints': pred['keypoints'][0].cpu().numpy(),
                'descriptors': pred['descriptors'][0].cpu().numpy(), 
                'scores': pred['scores'][0].cpu().numpy(),
                'image_path': str(image_path),
                'image_size': image.shape[:2]  # (H, W)
            }
            
            print(f"    Keypoints: {self.image_features[i]['keypoints'].shape[0]}")
            
        print(f"  Extracted features from {len(self.image_features)} images")
    
    def _extract_features_fallback(self, image_paths):
        """SuperGlueê°€ ì—†ì„ ë•Œ ì‚¬ìš©í•˜ëŠ” fallback íŠ¹ì§•ì  ì¶”ì¶œ"""
        print("  Using OpenCV SIFT for feature extraction")
        
        try:
            import cv2
            sift = cv2.SIFT_create()
        except ImportError:
            print("  OpenCV not available, using random features")
            return self._extract_random_features(image_paths)
        
        for i, image_path in enumerate(image_paths):
            print(f"  {i+1:3d}/{len(image_paths)}: {image_path.name}")
            
            try:
                # ì´ë¯¸ì§€ ë¡œë“œ
                image = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)
                if image is None:
                    continue
                
                # SIFT íŠ¹ì§•ì  ì¶”ì¶œ
                keypoints, descriptors = sift.detectAndCompute(image, None)
                
                if keypoints is None or descriptors is None:
                    continue
                
                # ê²°ê³¼ë¥¼ SuperPoint í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                kpts = np.array([[kp.pt[0], kp.pt[1]] for kp in keypoints])
                scores = np.array([kp.response for kp in keypoints])
                
                # descriptorë¥¼ float32ë¡œ ë³€í™˜
                desc = descriptors.astype(np.float32)
                
                self.image_features[i] = {
                    'keypoints': kpts,
                    'descriptors': desc.T,  # SuperPoint í˜•ì‹ì— ë§ì¶¤
                    'scores': scores,
                    'image_path': str(image_path),
                    'image_size': image.shape[:2]
                }
                
                print(f"    Keypoints: {len(keypoints)}")
                
            except Exception as e:
                print(f"    Error processing {image_path.name}: {e}")
                continue
        
        print(f"  Extracted features from {len(self.image_features)} images (fallback)")
    
    def _extract_random_features(self, image_paths):
        """ëª¨ë“  ë°©ë²•ì´ ì‹¤íŒ¨í–ˆì„ ë•Œ ì‚¬ìš©í•˜ëŠ” ëœë¤ íŠ¹ì§•ì """
        print("  Using random features (no feature extraction available)")
        
        for i, image_path in enumerate(image_paths):
            print(f"  {i+1:3d}/{len(image_paths)}: {image_path.name}")
            
            try:
                # ì´ë¯¸ì§€ í¬ê¸° í™•ì¸
                from PIL import Image
                img = Image.open(image_path)
                width, height = img.size
                
                # ëœë¤ íŠ¹ì§•ì  ìƒì„±
                n_keypoints = 1000
                kpts = np.random.rand(n_keypoints, 2)
                kpts[:, 0] *= width
                kpts[:, 1] *= height
                
                # ëœë¤ descriptor (128ì°¨ì›)
                desc = np.random.randn(128, n_keypoints).astype(np.float32)
                
                # ëœë¤ scores
                scores = np.random.rand(n_keypoints).astype(np.float32)
                
                self.image_features[i] = {
                    'keypoints': kpts,
                    'descriptors': desc,
                    'scores': scores,
                    'image_path': str(image_path),
                    'image_size': (height, width)
                }
                
                print(f"    Random keypoints: {n_keypoints}")
                
            except Exception as e:
                print(f"    Error processing {image_path.name}: {e}")
                continue
        
        print(f"  Generated random features for {len(self.image_features)} images")
    
    def _intelligent_matching(self, max_pairs=3000):
        """ì§€ëŠ¥ì  ì´ë¯¸ì§€ ë§¤ì¹­ (ê·¹ë„ë¡œ ì™„í™”ëœ ë²„ì „)"""
        n_images = len(self.image_features)
        
        # ì „ì—­ descriptors ê³„ì‚°
        self._compute_global_descriptors()
        
        print(f"  Starting intelligent matching for {n_images} images...")
        
        # 1. ìˆœì°¨ì  ë§¤ì¹­ - ëª¨ë“  ì´ë¯¸ì§€ë¥¼ í•œ ë°”í€´ ëŒê¸°
        sequential_count = 0
        for i in range(n_images):
            # ë‹¤ìŒ ì´ë¯¸ì§€ (ë§ˆì§€ë§‰ ì´ë¯¸ì§€ëŠ” ì²« ë²ˆì§¸ì™€ ì—°ê²°)
            next_i = (i + 1) % n_images
            
            matches = self._match_pair_superglue(i, next_i)
            if len(matches) > 1:  # 2 â†’ 1ë¡œ ê·¹ë„ë¡œ ì™„í™”
                self.matches[(i, next_i)] = matches
                self.camera_graph[i].append(next_i)
                self.camera_graph[next_i].append(i)
                sequential_count += 1
        
        print(f"    Sequential pairs: {sequential_count}")
        
        # 2. ìœ ì‚¬ë„ ê¸°ë°˜ ë§¤ì¹­ (ë” ì ê·¹ì ìœ¼ë¡œ)
        similarity_count = self._similarity_based_matching_very_relaxed(max_pairs)
        print(f"    Similarity pairs: {similarity_count}")
        
        # 3. Loop closure ë§¤ì¹­ (ë” ì ê·¹ì ìœ¼ë¡œ)
        loop_count = self._loop_closure_matching_very_relaxed()
        print(f"    Loop closure pairs: {loop_count}")
        
        # 4. ğŸ”§ NEW: ê·¸ë¦¬ë“œ ê¸°ë°˜ ë§¤ì¹­ (ì—°ì†ëœ ì´ë¯¸ì§€ë“¤ ê°„ì˜ ì—°ê²°)
        grid_count = self._grid_based_matching_very_relaxed()
        print(f"    Grid-based pairs: {grid_count}")
        
        # 5. ğŸ”§ NEW: ëœë¤ ìƒ˜í”Œë§ ë§¤ì¹­ (ì—°ê²°ë˜ì§€ ì•Šì€ ì¹´ë©”ë¼ë“¤ì„ ìœ„í•œ fallback)
        random_count = self._random_sampling_matching_very_relaxed(max_pairs)
        print(f"    Random sampling pairs: {random_count}")
        
        print(f"  Total matching pairs: {len(self.matches)}")
        
        # ğŸ”§ NEW: ì—°ê²°ì„± ë¶„ì„ ë° ê°œì„ 
        self._analyze_and_improve_connectivity_very_relaxed()

    def _similarity_based_matching_very_relaxed(self, max_pairs):
        """ê·¹ë„ë¡œ ì™„í™”ëœ ìœ ì‚¬ë„ ê¸°ë°˜ ë§¤ì¹­"""
        # ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
        n_images = len(self.global_descriptors)
        similarity_matrix = np.zeros((n_images, n_images))
        
        for i in range(n_images):
            for j in range(i+1, n_images):
                if i in self.global_descriptors and j in self.global_descriptors:
                    sim = np.dot(self.global_descriptors[i], self.global_descriptors[j])
                    similarity_matrix[i, j] = sim
                    similarity_matrix[j, i] = sim
        
        # ìœ ì‚¬í•œ ì´ë¯¸ì§€ë“¤ ë§¤ì¹­ (ê·¹ë„ë¡œ ì ê·¹ì ìœ¼ë¡œ)
        similarity_count = 0
        for cam_id in range(n_images):
            # ìœ ì‚¬ë„ ë†’ì€ ìƒìœ„ 30ê°œ ì„ íƒ (20 â†’ 30ìœ¼ë¡œ ì¦ê°€)
            similarities = similarity_matrix[cam_id]
            candidates = np.argsort(similarities)[::-1]
            candidates = [c for c in candidates if c != cam_id and similarities[c] > 0.01][:30]  # 0.05 â†’ 0.01ë¡œ ê·¹ë„ë¡œ ì™„í™”
            
            for candidate in candidates:
                pair_key = (min(cam_id, candidate), max(cam_id, candidate))
                if pair_key in self.matches:
                    continue
                
                matches = self._match_pair_superglue(cam_id, candidate)
                if len(matches) > 1:  # 2 â†’ 1ë¡œ ê·¹ë„ë¡œ ì™„í™”
                    self.matches[pair_key] = matches
                    self.camera_graph[cam_id].append(candidate)
                    self.camera_graph[candidate].append(cam_id)
                    similarity_count += 1
                
                if len(self.matches) >= max_pairs:
                    return similarity_count
        
        return similarity_count

    def _loop_closure_matching_very_relaxed(self):
        """ê·¹ë„ë¡œ ì™„í™”ëœ Loop closure ë§¤ì¹­"""
        n_images = len(self.image_features)
        loop_count = 0
        
        # ë” ë„“ì€ ë²”ìœ„ì—ì„œ loop closure ì‹œë„
        for i in range(min(20, n_images//3)):  # 15 â†’ 20ìœ¼ë¡œ ì¦ê°€
            for j in range(max(n_images-20, 2*n_images//3), n_images):  # 15 â†’ 20ìœ¼ë¡œ ì¦ê°€
                if i >= j:
                    continue
                
                pair_key = (i, j)
                if pair_key in self.matches:
                    continue
                
                # ì „ì—­ ìœ ì‚¬ë„ ì²´í¬ (ê·¹ë„ë¡œ ì™„í™”ëœ ì¡°ê±´)
                if hasattr(self, 'global_descriptors') and i in self.global_descriptors and j in self.global_descriptors:
                    sim = np.dot(self.global_descriptors[i], self.global_descriptors[j])
                    if sim > 0.05:  # 0.1 â†’ 0.05ë¡œ ê·¹ë„ë¡œ ì™„í™”
                        matches = self._match_pair_superglue(i, j)
                        if len(matches) > 1:  # 2 â†’ 1ë¡œ ê·¹ë„ë¡œ ì™„í™”
                            self.matches[pair_key] = matches
                            self.camera_graph[i].append(j)
                            self.camera_graph[j].append(i)
                            loop_count += 1
        
        return loop_count

    def _grid_based_matching_very_relaxed(self):
        """ê·¹ë„ë¡œ ì™„í™”ëœ ê·¸ë¦¬ë“œ ê¸°ë°˜ ë§¤ì¹­"""
        n_images = len(self.image_features)
        grid_count = 0
        
        # ì—°ì†ëœ ì´ë¯¸ì§€ë“¤ ê°„ì˜ ì¶”ê°€ ì—°ê²°
        for i in range(n_images - 1):
            # ì¸ì ‘í•œ ì´ë¯¸ì§€ë“¤
            for offset in [1, 2, 3, 4, 5]:  # 1, 2, 3 â†’ 1, 2, 3, 4, 5ë¡œ ì¦ê°€
                j = i + offset
                if j >= n_images:
                    continue
                
                pair_key = (i, j)
                if pair_key in self.matches:
                    continue
                
                matches = self._match_pair_superglue(i, j)
                if len(matches) > 1:  # 1 â†’ 1ë¡œ ìœ ì§€ (ì´ë¯¸ ìµœì†Œê°’)
                    self.matches[pair_key] = matches
                    self.camera_graph[i].append(j)
                    self.camera_graph[j].append(i)
                    grid_count += 1
        
        return grid_count

    def _random_sampling_matching_very_relaxed(self, max_pairs):
        """ê·¹ë„ë¡œ ì™„í™”ëœ ëœë¤ ìƒ˜í”Œë§ ë§¤ì¹­"""
        n_images = len(self.image_features)
        random_count = 0
        
        # ì—°ê²°ë˜ì§€ ì•Šì€ ì¹´ë©”ë¼ë“¤ì„ ì°¾ê¸°
        unconnected_cameras = []
        for cam_id in range(n_images):
            if len(self.camera_graph[cam_id]) == 0:
                unconnected_cameras.append(cam_id)
        
        print(f"    Found {len(unconnected_cameras)} unconnected cameras")
        
        # ì—°ê²°ë˜ì§€ ì•Šì€ ì¹´ë©”ë¼ë“¤ì— ëŒ€í•´ ëœë¤ ë§¤ì¹­ ì‹œë„
        for cam_id in unconnected_cameras:
            # ë‹¤ë¥¸ ëª¨ë“  ì¹´ë©”ë¼ì™€ ë§¤ì¹­ ì‹œë„
            for other_cam in range(n_images):
                if cam_id == other_cam:
                    continue
                
                pair_key = (min(cam_id, other_cam), max(cam_id, other_cam))
                if pair_key in self.matches:
                    continue
                
                matches = self._match_pair_superglue(cam_id, other_cam)
                if len(matches) > 1:  # 1 â†’ 1ë¡œ ìœ ì§€ (ì´ë¯¸ ìµœì†Œê°’)
                    self.matches[pair_key] = matches
                    self.camera_graph[cam_id].append(other_cam)
                    self.camera_graph[other_cam].append(cam_id)
                    random_count += 1
                    break  # í•˜ë‚˜ë¼ë„ ì—°ê²°ë˜ë©´ ë‹¤ìŒ ì¹´ë©”ë¼ë¡œ
        
        return random_count

    def _analyze_and_improve_connectivity_very_relaxed(self):
        """ê·¹ë„ë¡œ ì™„í™”ëœ ì—°ê²°ì„± ë¶„ì„ ë° ê°œì„ """
        n_images = len(self.image_features)
        
        # ì—°ê²°ì„± ë¶„ì„
        connected_cameras = []
        isolated_cameras = []
        
        for cam_id in range(n_images):
            if len(self.camera_graph[cam_id]) > 0:
                connected_cameras.append(cam_id)
            else:
                isolated_cameras.append(cam_id)
        
        print(f"    Connectivity analysis:")
        print(f"      Connected cameras: {len(connected_cameras)}")
        print(f"      Isolated cameras: {len(isolated_cameras)}")
        
        # ì—°ê²°ë˜ì§€ ì•Šì€ ì¹´ë©”ë¼ë“¤ì„ ì—°ê²°ëœ ì¹´ë©”ë¼ì™€ ì—°ê²° ì‹œë„
        if len(connected_cameras) > 0 and len(isolated_cameras) > 0:
            print(f"    Attempting to connect {len(isolated_cameras)} isolated cameras...")
            
            for isolated_cam in isolated_cameras:
                # ê°€ì¥ ê°€ê¹Œìš´ ì—°ê²°ëœ ì¹´ë©”ë¼ ì°¾ê¸°
                best_connection = None
                best_similarity = -1
                
                for connected_cam in connected_cameras:
                    if hasattr(self, 'global_descriptors'):
                        if isolated_cam in self.global_descriptors and connected_cam in self.global_descriptors:
                            sim = np.dot(self.global_descriptors[isolated_cam], self.global_descriptors[connected_cam])
                            if sim > best_similarity:
                                best_similarity = sim
                                best_connection = connected_cam
                
                if best_connection is not None:
                    # ë§¤ì¹­ ì‹œë„
                    matches = self._match_pair_superglue(isolated_cam, best_connection)
                    if len(matches) > 1:  # 1 â†’ 1ë¡œ ìœ ì§€ (ì´ë¯¸ ìµœì†Œê°’)
                        pair_key = (min(isolated_cam, best_connection), max(isolated_cam, best_connection))
                        self.matches[pair_key] = matches
                        self.camera_graph[isolated_cam].append(best_connection)
                        self.camera_graph[best_connection].append(isolated_cam)
                        print(f"      Connected camera {isolated_cam} to {best_connection}")

    def _compute_global_descriptors(self):
        """ì „ì—­ ì´ë¯¸ì§€ descriptor ê³„ì‚° (NEW METHOD)"""
        self.global_descriptors = {}
        
        for cam_id, features in self.image_features.items():
            descriptors = features['descriptors']  # (256, N)
            scores = features['scores']
            
            if len(scores) > 0:
                # Scoreë¡œ ê°€ì¤‘í‰ê· í•˜ì—¬ ì „ì—­ descriptor ê³„ì‚°
                weights = scores / (scores.sum() + 1e-10)
                global_desc = np.average(descriptors.T, weights=weights, axis=0)
                global_desc = global_desc / (np.linalg.norm(global_desc) + 1e-10)
                self.global_descriptors[cam_id] = global_desc
            else:
                self.global_descriptors[cam_id] = np.zeros(256)
                
    def _similarity_based_matching(self, max_pairs):
        """ìœ ì‚¬ë„ ê¸°ë°˜ ë§¤ì¹­ (NEW METHOD)"""
        # ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
        n_images = len(self.global_descriptors)
        similarity_matrix = np.zeros((n_images, n_images))
        
        for i in range(n_images):
            for j in range(i+1, n_images):
                if i in self.global_descriptors and j in self.global_descriptors:
                    sim = np.dot(self.global_descriptors[i], self.global_descriptors[j])
                    similarity_matrix[i, j] = sim
                    similarity_matrix[j, i] = sim
        
        # ìœ ì‚¬í•œ ì´ë¯¸ì§€ë“¤ ë§¤ì¹­
        similarity_count = 0
        for cam_id in range(n_images):
            # ìœ ì‚¬ë„ ë†’ì€ ìƒìœ„ 12ê°œ ì„ íƒ (8 â†’ 12ë¡œ ì¦ê°€)
            similarities = similarity_matrix[cam_id]
            candidates = np.argsort(similarities)[::-1]
            candidates = [c for c in candidates if c != cam_id and similarities[c] > 0.2][:12]  # 0.3 â†’ 0.2, 8 â†’ 12
            
            for candidate in candidates:
                pair_key = (min(cam_id, candidate), max(cam_id, candidate))
                if pair_key in self.matches:
                    continue
                
                matches = self._match_pair_superglue(cam_id, candidate)
                if len(matches) > 10:  # 15 â†’ 10ìœ¼ë¡œ ì™„í™”
                    self.matches[pair_key] = matches
                    self.camera_graph[cam_id].append(candidate)
                    self.camera_graph[candidate].append(cam_id)
                    similarity_count += 1
                
                if len(self.matches) >= max_pairs:
                    return similarity_count
        
        return similarity_count

    def _loop_closure_matching(self):
        """Loop closure ë§¤ì¹­ (NEW METHOD)"""
        n_images = len(self.image_features)
        loop_count = 0
        
        # ì²« ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ ëª‡ ê°œ ì´ë¯¸ì§€ ê°„ ë§¤ì¹­
        for i in range(min(8, n_images//3)):  # 5 â†’ 8ë¡œ ì¦ê°€
            for j in range(max(n_images-8, 2*n_images//3), n_images):  # 5 â†’ 8ë¡œ ì¦ê°€
                if i >= j:
                    continue
                
                pair_key = (i, j)
                if pair_key in self.matches:
                    continue
                
                # ì „ì—­ ìœ ì‚¬ë„ ì²´í¬
                if hasattr(self, 'global_descriptors') and i in self.global_descriptors and j in self.global_descriptors:
                    sim = np.dot(self.global_descriptors[i], self.global_descriptors[j])
                    if sim > 0.3:  # 0.4 â†’ 0.3ìœ¼ë¡œ ì™„í™”
                        matches = self._match_pair_superglue(i, j)
                        if len(matches) > 15:  # 20 â†’ 15ë¡œ ì™„í™”
                            self.matches[pair_key] = matches
                            self.camera_graph[i].append(j)
                            self.camera_graph[j].append(i)
                            loop_count += 1
        
        return loop_count
    
    def _filter_low_quality_matches_very_relaxed(self):
        """ë§¤ìš° ì™„í™”ëœ ë‚®ì€ í’ˆì§ˆì˜ ë§¤ì¹­ í•„í„°ë§"""
        pairs_to_remove = []
        
        for (cam_i, cam_j), matches in self.matches.items():
            if len(matches) < 2:  # ë” ë‚®ì€ ì„ê³„ê°’ (3 â†’ 2)
                pairs_to_remove.append((cam_i, cam_j))
                continue
            
            # ë§¤ì¹­ í’ˆì§ˆ ë¶„ì„
            confidences = [conf for _, _, conf in matches]
            avg_confidence = np.mean(confidences)
            
            if avg_confidence < 0.001:  # ë” ë‚®ì€ ì„ê³„ê°’ (0.1 â†’ 0.001)
                pairs_to_remove.append((cam_i, cam_j))
                continue
            
            # ë§¤ì¹­ ë¶„í¬ ë¶„ì„ (ë” ì™„í™”ëœ ì¡°ê±´)
            if self._has_poor_matching_distribution_very_relaxed(cam_i, cam_j, matches):
                pairs_to_remove.append((cam_i, cam_j))
        
        # í•„í„°ë§ëœ ë§¤ì¹­ ì œê±°
        for pair in pairs_to_remove:
            cam_i, cam_j = pair
            del self.matches[pair]
            
            # ê·¸ë˜í”„ì—ì„œë„ ì œê±°
            if cam_j in self.camera_graph[cam_i]:
                self.camera_graph[cam_i].remove(cam_j)
            if cam_i in self.camera_graph[cam_j]:
                self.camera_graph[cam_j].remove(cam_i)
        
        print(f"  Filtered out {len(pairs_to_remove)} low-quality matches (very relaxed)")
    
    def _has_poor_matching_distribution_very_relaxed(self, cam_i, cam_j, matches):
        """ë§¤ìš° ì™„í™”ëœ ë§¤ì¹­ ë¶„í¬ ê²€ì‚¬"""
        kpts_i = self.image_features[cam_i]['keypoints']
        kpts_j = self.image_features[cam_j]['keypoints']
        
        # ì¸ë±ìŠ¤ ë²”ìœ„ ê²€ì¦
        valid_matches = []
        for idx_i, idx_j, conf in matches:
            if (idx_i < len(kpts_i) and idx_j < len(kpts_j) and 
                idx_i >= 0 and idx_j >= 0):
                valid_matches.append((idx_i, idx_j, conf))
        
        if len(valid_matches) < 1:  # ë” ë‚®ì€ ì„ê³„ê°’ (2 â†’ 1)
            return True
        
        # ë§¤ì¹­ëœ ì ë“¤ì˜ ìœ„ì¹˜ ë¶„ì„
        matched_i = np.array([kpts_i[idx_i] for idx_i, _, _ in valid_matches])
        matched_j = np.array([kpts_j[idx_j] for _, idx_j, _ in valid_matches])
        
        # ì´ë¯¸ì§€ í¬ê¸°
        h_i, w_i = self.image_features[cam_i]['image_size']
        h_j, w_j = self.image_features[cam_j]['image_size']
        
        # ê²½ê³„ ê·¼ì²˜ì˜ ë§¤ì¹­ì´ ë„ˆë¬´ ë§ì€ì§€ í™•ì¸ (ë” ì™„í™”ëœ ì¡°ê±´)
        border_threshold = 10  # ë” ì‘ì€ ê²½ê³„ (20 â†’ 10)
        
        border_matches_i = np.sum((matched_i[:, 0] < border_threshold) | 
                                  (matched_i[:, 0] > w_i - border_threshold) |
                                  (matched_i[:, 1] < border_threshold) | 
                                  (matched_i[:, 1] > h_i - border_threshold))
        
        border_matches_j = np.sum((matched_j[:, 0] < border_threshold) | 
                                  (matched_j[:, 0] > w_j - border_threshold) |
                                  (matched_j[:, 1] < border_threshold) | 
                                  (matched_j[:, 1] > h_j - border_threshold))
        
        # ê²½ê³„ ë§¤ì¹­ì´ ì „ì²´ì˜ 98% ì´ìƒì´ë©´ ë‚˜ìœ ë¶„í¬ (95% â†’ 98%)
        if border_matches_i > len(valid_matches) * 0.98 or border_matches_j > len(valid_matches) * 0.98:
            return True
        
        return False
    
    def _match_pair_superglue(self, cam_i, cam_j):
        """SuperGlue í˜ì–´ ë§¤ì¹­ (ë” ì™„í™”ëœ ë²„ì „)"""
        if not self.superglue_available:
            return self._match_pair_fallback(cam_i, cam_j)
        
        try:
            feat_i = self.image_features[cam_i]
            feat_j = self.image_features[cam_j]
            
            # ì…ë ¥ ë°ì´í„° ì¤€ë¹„
            data = {
                'image0': torch.zeros((1, 1, 480, 640)).to(self.device),
                'image1': torch.zeros((1, 1, 480, 640)).to(self.device),
                'keypoints0': torch.from_numpy(feat_i['keypoints']).unsqueeze(0).to(self.device),
                'keypoints1': torch.from_numpy(feat_j['keypoints']).unsqueeze(0).to(self.device),
                'descriptors0': torch.from_numpy(feat_i['descriptors']).unsqueeze(0).to(self.device),
                'descriptors1': torch.from_numpy(feat_j['descriptors']).unsqueeze(0).to(self.device),
                'scores0': torch.from_numpy(feat_i['scores']).unsqueeze(0).to(self.device),
                'scores1': torch.from_numpy(feat_j['scores']).unsqueeze(0).to(self.device),
            }
            
            # SuperGlue ë§¤ì¹­
            with torch.no_grad():
                result = self.matching.superglue(data)
            
            # ê²°ê³¼ ì¶”ì¶œ
            indices0 = result['indices0'][0].cpu().numpy()
            indices1 = result['indices1'][0].cpu().numpy()
            mscores0 = result['matching_scores0'][0].cpu().numpy()
            
            # ğŸ”§ ë” ì™„í™”ëœ ë§¤ì¹­ í•„í„°ë§
            valid_matches = []
            threshold = 0.00001  # 0.0001 â†’ 0.00001ë¡œ ëŒ€í­ ì™„í™”
            
            for i, j in enumerate(indices0):
                if j >= 0 and mscores0[i] > threshold:
                    # ìƒí˜¸ ë§¤ì¹­ í™•ì¸
                    if j < len(indices1) and indices1[j] == i:
                        # ì¸ë±ìŠ¤ ë²”ìœ„ ê²€ì¦
                        if i < len(feat_i['keypoints']) and j < len(feat_j['keypoints']):
                            valid_matches.append((i, j, mscores0[i]))
            
            # ğŸ”§ ë” ì™„í™”ëœ ê¸°í•˜í•™ì  í•„í„°ë§
            if len(valid_matches) >= 1:  # 1ê°œ ì´ìƒì´ë©´ í•„í„°ë§ ì‹œë„
                valid_matches = self._geometric_filtering_relaxed(valid_matches, feat_i['keypoints'], feat_j['keypoints'])
            
            return valid_matches
            
        except Exception as e:
            print(f"    SuperGlue matching failed for pair {cam_i}-{cam_j}: {e}")
            return self._match_pair_fallback(cam_i, cam_j)
    
    def _match_pair_fallback(self, cam_i, cam_j):
        """SuperGlueê°€ ì—†ì„ ë•Œ ì‚¬ìš©í•˜ëŠ” ì •êµí•œ fallback ë§¤ì¹­"""
        try:
            feat_i = self.image_features[cam_i]
            feat_j = self.image_features[cam_j]
            
            # 1ë‹¨ê³„: ë‹¤ì¤‘ descriptor ë§¤ì¹­
            matches_stage1 = self._multi_descriptor_matching(feat_i, feat_j)
            
            # 2ë‹¨ê³„: ê¸°í•˜í•™ì  í•„í„°ë§
            matches_stage2 = self._advanced_geometric_filtering(matches_stage1, feat_i, feat_j)
            
            # 3ë‹¨ê³„: ì‹ ë¢°ë„ ê¸°ë°˜ ì¬ìˆœìœ„
            matches_stage3 = self._confidence_reranking(matches_stage2, feat_i, feat_j)
            
            # 4ë‹¨ê³„: ìƒí˜¸ ì¼ê´€ì„± ê²€ì¦
            final_matches = self._mutual_consistency_check(matches_stage3, feat_i, feat_j)
            
            return final_matches
            
        except Exception as e:
            print(f"    Advanced fallback matching failed for pair {cam_i}-{cam_j}: {e}")
            return self._basic_fallback_matching(cam_i, cam_j)
    
    def _multi_descriptor_matching(self, feat_i, feat_j):
        """ë‹¤ì¤‘ descriptor ë§¤ì¹­"""
        desc_i = feat_i['descriptors'].T  # (N, D)
        desc_j = feat_j['descriptors'].T  # (M, D)
        
        # ë°©ë²• 1: ì½”ì‚¬ì¸ ìœ ì‚¬ë„
        desc_i_norm = desc_i / (np.linalg.norm(desc_i, axis=1, keepdims=True) + 1e-10)
        desc_j_norm = desc_j / (np.linalg.norm(desc_j, axis=1, keepdims=True) + 1e-10)
        cosine_sim = desc_i_norm @ desc_j_norm.T
        
        # ë°©ë²• 2: ìœ í´ë¦¬ë“œ ê±°ë¦¬
        euclidean_dist = np.linalg.norm(desc_i[:, np.newaxis] - desc_j[np.newaxis, :], axis=2)
        euclidean_sim = 1.0 / (1.0 + euclidean_dist)
        
        # ë°©ë²• 3: í•´ë° ê±°ë¦¬ (ì´ì§„ íŠ¹ì§•ì˜ ê²½ìš°)
        # SIFT descriptorë¥¼ ì´ì§„í™”í•˜ì—¬ ì‚¬ìš©
        binary_i = (desc_i > np.median(desc_i, axis=1, keepdims=True)).astype(np.uint8)
        binary_j = (desc_j > np.median(desc_j, axis=1, keepdims=True)).astype(np.uint8)
        hamming_dist = np.sum(binary_i[:, np.newaxis] != binary_j[np.newaxis, :], axis=2)
        hamming_sim = 1.0 - hamming_dist / desc_i.shape[1]
        
        # ê°€ì¤‘ ì¡°í•©
        combined_sim = (0.5 * cosine_sim + 0.3 * euclidean_sim + 0.2 * hamming_sim)
        
        # ì´ˆê¸° ë§¤ì¹­ í›„ë³´ ì„ íƒ
        matches = []
        threshold = 0.6  # ë” ì—„ê²©í•œ ì´ˆê¸° ì„ê³„ê°’
        
        for i in range(len(desc_i)):
            # ìƒìœ„ 3ê°œ í›„ë³´ ì„ íƒ
            top_indices = np.argsort(combined_sim[i])[-3:][::-1]
            for j in top_indices:
                if combined_sim[i, j] > threshold:
                    # Lowe's ratio test
                    if len(top_indices) > 1:
                        ratio = combined_sim[i, top_indices[0]] / (combined_sim[i, top_indices[1]] + 1e-10)
                        if ratio > 0.8:  # ë” ì—„ê²©í•œ ratio test
                            continue
                    
                    matches.append((i, j, combined_sim[i, j]))
        
        return matches
    
    def _advanced_geometric_filtering(self, matches, feat_i, feat_j):
        """ê³ ê¸‰ ê¸°í•˜í•™ì  í•„í„°ë§"""
        if len(matches) < 8:
            return matches
        
        kpts_i = feat_i['keypoints']
        kpts_j = feat_j['keypoints']
        
        # ë§¤ì¹­ì  ì¢Œí‘œ ì¶”ì¶œ
        pts_i = np.array([kpts_i[m[0]] for m in matches])
        pts_j = np.array([kpts_j[m[1]] for m in matches])
        
        # 1. í˜¸ëª¨ê·¸ë˜í”¼ ê¸°ë°˜ í•„í„°ë§
        filtered_matches = []
        try:
            H, mask = cv2.findHomography(pts_i, pts_j, cv2.RANSAC, 3.0)
            if H is not None:
                for i, is_inlier in enumerate(mask.flatten()):
                    if is_inlier:
                        filtered_matches.append(matches[i])
        except:
            filtered_matches = matches
        
        # 2. ì—í”¼í´ë¼ ì œì•½ í•„í„°ë§ (ì¶”ê°€)
        if len(filtered_matches) >= 8:
            try:
                pts_i_filtered = np.array([kpts_i[m[0]] for m in filtered_matches])
                pts_j_filtered = np.array([kpts_j[m[1]] for m in filtered_matches])
                
                F, mask = cv2.findFundamentalMat(pts_i_filtered, pts_j_filtered, cv2.FM_RANSAC)
                if F is not None:
                    epi_filtered = []
                    for i, is_inlier in enumerate(mask.flatten()):
                        if is_inlier:
                            epi_filtered.append(filtered_matches[i])
                    
                    if len(epi_filtered) >= 8:
                        filtered_matches = epi_filtered
            except:
                pass
        
        # 3. ì§€ì—­ì  ì¼ê´€ì„± ê²€ì‚¬
        if len(filtered_matches) >= 10:
            filtered_matches = self._local_consistency_check(filtered_matches, kpts_i, kpts_j)
        
        return filtered_matches
    
    def _local_consistency_check(self, matches, kpts_i, kpts_j):
        """ì§€ì—­ì  ì¼ê´€ì„± ê²€ì‚¬"""
        if len(matches) < 10:
            return matches
        
        # ê° ë§¤ì¹­ì— ëŒ€í•´ ì§€ì—­ì  ì´ì›ƒ ê²€ì‚¬
        consistent_matches = []
        
        for idx, (i, j, conf) in enumerate(matches):
            pt_i = kpts_i[i]
            pt_j = kpts_j[j]
            
            # ì£¼ë³€ ë§¤ì¹­ë“¤ ì°¾ê¸°
            neighbor_votes = 0
            total_neighbors = 0
            
            for other_idx, (other_i, other_j, _) in enumerate(matches):
                if idx == other_idx:
                    continue
                
                other_pt_i = kpts_i[other_i]
                other_pt_j = kpts_j[other_j]
                
                # ê±°ë¦¬ ê³„ì‚°
                dist_i = np.linalg.norm(pt_i - other_pt_i)
                dist_j = np.linalg.norm(pt_j - other_pt_j)
                
                if dist_i < 50 and dist_j < 50:  # ì§€ì—­ì  ì´ì›ƒ
                    total_neighbors += 1
                    
                    # ê±°ë¦¬ ë¹„ìœ¨ ì¼ê´€ì„± ê²€ì‚¬
                    if dist_i > 0 and dist_j > 0:
                        ratio = dist_j / dist_i
                        if 0.5 < ratio < 2.0:  # í•©ë¦¬ì ì¸ ìŠ¤ì¼€ì¼ ë¹„ìœ¨
                            neighbor_votes += 1
            
            # ì§€ì—­ì  ì¼ê´€ì„± ì ìˆ˜
            if total_neighbors > 0:
                consistency_score = neighbor_votes / total_neighbors
                if consistency_score > 0.3:  # 30% ì´ìƒ ì¼ê´€ì„±
                    consistent_matches.append((i, j, conf * consistency_score))
        
        return consistent_matches
    
    def _confidence_reranking(self, matches, feat_i, feat_j):
        """ì‹ ë¢°ë„ ê¸°ë°˜ ì¬ìˆœìœ„"""
        if len(matches) < 5:
            return matches
        
        enhanced_matches = []
        
        for i, j, base_conf in matches:
            # ë‹¤ì–‘í•œ ì‹ ë¢°ë„ ì§€í‘œ ê³„ì‚°
            
            # 1. íŠ¹ì§•ì  ê°•ë„ (response)
            score_i = feat_i['scores'][i] if i < len(feat_i['scores']) else 0.5
            score_j = feat_j['scores'][j] if j < len(feat_j['scores']) else 0.5
            response_conf = np.sqrt(score_i * score_j)
            
            # 2. ì´ë¯¸ì§€ ì¤‘ì‹¬ìœ¼ë¡œë¶€í„° ê±°ë¦¬ (ì¤‘ì‹¬ë¶€ íŠ¹ì§•ì  ì„ í˜¸)
            h_i, w_i = feat_i['image_size']
            h_j, w_j = feat_j['image_size']
            
            center_i = np.array([w_i/2, h_i/2])
            center_j = np.array([w_j/2, h_j/2])
            
            dist_i = np.linalg.norm(feat_i['keypoints'][i] - center_i)
            dist_j = np.linalg.norm(feat_j['keypoints'][j] - center_j)
            
            max_dist_i = np.sqrt(w_i**2 + h_i**2) / 2
            max_dist_j = np.sqrt(w_j**2 + h_j**2) / 2
            
            center_conf = (1.0 - dist_i/max_dist_i) * (1.0 - dist_j/max_dist_j)
            
            # 3. ê²½ê³„ ê·¼ì²˜ í˜ë„í‹°
            border_penalty = 1.0
            border_size = 10
            
            if (feat_i['keypoints'][i][0] < border_size or 
                feat_i['keypoints'][i][0] > w_i - border_size or
                feat_i['keypoints'][i][1] < border_size or 
                feat_i['keypoints'][i][1] > h_i - border_size):
                border_penalty *= 0.8
            
            if (feat_j['keypoints'][j][0] < border_size or 
                feat_j['keypoints'][j][0] > w_j - border_size or
                feat_j['keypoints'][j][1] < border_size or 
                feat_j['keypoints'][j][1] > h_j - border_size):
                border_penalty *= 0.8
            
            # ì¢…í•© ì‹ ë¢°ë„ ê³„ì‚°
            final_conf = (0.4 * base_conf + 
                         0.3 * response_conf + 
                         0.2 * center_conf + 
                         0.1) * border_penalty
            
            enhanced_matches.append((i, j, final_conf))
        
        # ì‹ ë¢°ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        enhanced_matches.sort(key=lambda x: x[2], reverse=True)
        
        # ìƒìœ„ Nê°œë§Œ ì„ íƒ (í’ˆì§ˆ ë³´ì¥)
        max_matches = min(len(enhanced_matches), 200)
        return enhanced_matches[:max_matches]
    
    def _mutual_consistency_check(self, matches, feat_i, feat_j):
        """ìƒí˜¸ ì¼ê´€ì„± ê²€ì¦"""
        if len(matches) < 10:
            return matches
        
        # ì—­ë°©í–¥ ë§¤ì¹­ ìˆ˜í–‰
        desc_i = feat_i['descriptors'].T
        desc_j = feat_j['descriptors'].T
        
        # ì •ê·œí™”
        desc_i_norm = desc_i / (np.linalg.norm(desc_i, axis=1, keepdims=True) + 1e-10)
        desc_j_norm = desc_j / (np.linalg.norm(desc_j, axis=1, keepdims=True) + 1e-10)
        
        # ìœ ì‚¬ë„ í–‰ë ¬
        similarity = desc_i_norm @ desc_j_norm.T
        
        # ìƒí˜¸ ì¼ê´€ì„± ê²€ì¦
        consistent_matches = []
        
        for i, j, conf in matches:
            # ìˆœë°©í–¥ ìµœì  ë§¤ì¹­
            best_j_for_i = np.argmax(similarity[i])
            
            # ì—­ë°©í–¥ ìµœì  ë§¤ì¹­
            best_i_for_j = np.argmax(similarity[:, j])
            
            # ìƒí˜¸ ì¼ê´€ì„± ê²€ì‚¬
            if best_j_for_i == j and best_i_for_j == i:
                # ì¶”ê°€ ê²€ì¦: 2ì°¨ í›„ë³´ì™€ì˜ ê²©ì°¨
                sorted_j = np.argsort(similarity[i])[::-1]
                sorted_i = np.argsort(similarity[:, j])[::-1]
                
                if len(sorted_j) > 1 and len(sorted_i) > 1:
                    gap_j = similarity[i, sorted_j[0]] - similarity[i, sorted_j[1]]
                    gap_i = similarity[sorted_i[0], j] - similarity[sorted_i[1], j]
                    
                    if gap_j > 0.1 and gap_i > 0.1:  # ì¶©ë¶„í•œ ê²©ì°¨
                        consistent_matches.append((i, j, conf * (gap_j + gap_i)))
        
        return consistent_matches
    
    def _basic_fallback_matching(self, cam_i, cam_j):
        """ê¸°ë³¸ fallback ë§¤ì¹­ (ìµœí›„ì˜ ìˆ˜ë‹¨)"""
        try:
            feat_i = self.image_features[cam_i]
            feat_j = self.image_features[cam_j]
            
            desc_i = feat_i['descriptors'].T
            desc_j = feat_j['descriptors'].T
            
            # ë‹¨ìˆœ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë§¤ì¹­
            desc_i_norm = desc_i / (np.linalg.norm(desc_i, axis=1, keepdims=True) + 1e-10)
            desc_j_norm = desc_j / (np.linalg.norm(desc_j, axis=1, keepdims=True) + 1e-10)
            
            similarity = desc_i_norm @ desc_j_norm.T
            
            matches = []
            threshold = 0.5
            
            for i in range(len(desc_i)):
                best_j = np.argmax(similarity[i])
                if similarity[i, best_j] > threshold:
                    if np.argmax(similarity[:, best_j]) == i:
                        matches.append((i, best_j, similarity[i, best_j]))
            
            return matches
            
        except Exception as e:
            print(f"    Basic fallback matching failed: {e}")
            return []
    
    def _geometric_filtering_relaxed(self, matches, kpts_i, kpts_j):
        """ì™„í™”ëœ ê¸°í•˜í•™ì  í•„í„°ë§ (NEW METHOD)"""
        try:
            pts_i = np.array([kpts_i[m[0]] for m in matches])
            pts_j = np.array([kpts_j[m[1]] for m in matches])
            
            # í˜¸ëª¨ê·¸ë˜í”¼ ê¸°ë°˜ outlier ì œê±° (ë” ì™„í™”ëœ ì¡°ê±´)
            H, mask = cv2.findHomography(pts_i, pts_j, cv2.RANSAC, 100.0)  # 50.0 â†’ 100.0ìœ¼ë¡œ ë” ì™„í™”
            
            if H is not None and mask is not None:
                inlier_matches = [matches[i] for i, is_inlier in enumerate(mask.flatten()) if is_inlier]
                if len(inlier_matches) >= 1:  # 1ê°œ ì´ìƒì´ë©´ í†µê³¼
                    return inlier_matches
        except:
            pass
        
        return matches
    
    def _estimate_camera_poses_robust(self):
        """ê°œì„ ëœ ì¹´ë©”ë¼ í¬ì¦ˆ ì¶”ì • - ë” ê°•ë ¥í•œ ì—°ê²°ì„±"""
        
        # ğŸ”§ DEBUG: í¬ì¦ˆ ì¶”ì • ì‹œì‘ ìƒíƒœ í™•ì¸
        print(f"    DEBUG: Starting pose estimation with {len(self.matches)} matches")
        print(f"    DEBUG: Camera graph has {len(self.camera_graph)} nodes")
        print(f"    DEBUG: Image features for {len(self.image_features)} images")
        
        # ì²« ë²ˆì§¸ ì¹´ë©”ë¼ë¥¼ ì›ì ìœ¼ë¡œ ì„¤ì •
        self.cameras[0] = {
            'R': np.eye(3, dtype=np.float32),
            'T': np.zeros(3, dtype=np.float32),
            'K': self._estimate_intrinsics(0)
        }
        
        print(f"  Camera 0: Origin (reference)")
        
        # ğŸ”§ ê°œì„ ëœ í¬ì¦ˆ ì¶”ì • ì „ëµ
        estimated_cameras = {0}
        queue = [0]
        
        # 1ë‹¨ê³„: ì—°ê²°ëœ ì¹´ë©”ë¼ë“¤ë§Œ í¬ì¦ˆ ì¶”ì •
        while queue:
            current_cam = queue.pop(0)
            
            # í˜„ì¬ ì¹´ë©”ë¼ì™€ ì—°ê²°ëœ ì¹´ë©”ë¼ë“¤ í™•ì¸
            for neighbor_cam in self.camera_graph[current_cam]:
                if neighbor_cam in estimated_cameras:
                    continue
                
                # ë§¤ì¹­ ë°ì´í„° ì°¾ê¸°
                pair_key = (current_cam, neighbor_cam) if current_cam < neighbor_cam else (neighbor_cam, current_cam)
                if pair_key not in self.matches:
                    continue
                
                # Essential Matrix ê¸°ë°˜ í¬ì¦ˆ ì¶”ì •
                R_rel, T_rel = self._estimate_relative_pose_robust(current_cam, neighbor_cam, pair_key)
                
                if R_rel is not None and T_rel is not None:
                    # ì›”ë“œ ì¢Œí‘œê³„ì—ì„œì˜ ì ˆëŒ€ í¬ì¦ˆ ê³„ì‚°
                    R_ref, T_ref = self.cameras[current_cam]['R'], self.cameras[current_cam]['T']
                    
                    # ìƒëŒ€ í¬ì¦ˆë¥¼ ì ˆëŒ€ í¬ì¦ˆë¡œ ë³€í™˜
                    R_world = R_rel @ R_ref
                    T_world = R_rel @ T_ref + T_rel
                    
                    # í¬ì¦ˆ ìœ íš¨ì„± ê²€ì‚¬
                    if self._is_valid_rotation_matrix(R_world):
                        self.cameras[neighbor_cam] = {
                            'R': R_world.astype(np.float32),
                            'T': T_world.astype(np.float32),
                            'K': self._estimate_intrinsics(neighbor_cam)
                        }
                        
                        print(f"  Camera {neighbor_cam}: Estimated from camera {current_cam}")
                        estimated_cameras.add(neighbor_cam)
                        queue.append(neighbor_cam)
                    else:
                        print(f"  Camera {neighbor_cam}: Invalid pose, skipping...")
                else:
                    print(f"  Camera {neighbor_cam}: Pose estimation failed, will use default pose")
        
        # 2ë‹¨ê³„: ì—°ê²°ë˜ì§€ ì•Šì€ ì¹´ë©”ë¼ë“¤ì— ëŒ€í•œ ê°œì„ ëœ ê¸°ë³¸ í¬ì¦ˆ ì„¤ì •
        unconnected_count = 0
        for cam_id in range(len(self.image_features)):
            if cam_id not in estimated_cameras:
                unconnected_count += 1
                print(f"  Camera {cam_id}: Using improved default pose (not connected)")
                
                # ğŸ”§ ê°œì„ ëœ ê¸°ë³¸ í¬ì¦ˆ ì„¤ì •
                if len(estimated_cameras) > 0:
                    # ì—°ê²°ëœ ì¹´ë©”ë¼ë“¤ì˜ í‰ê·  ìœ„ì¹˜ ê³„ì‚°
                    connected_positions = []
                    for est_cam in estimated_cameras:
                        R, T = self.cameras[est_cam]['R'], self.cameras[est_cam]['T']
                        # ì¹´ë©”ë¼ ì¤‘ì‹¬ ê³„ì‚°
                        center = -R.T @ T
                        connected_positions.append(center)
                    
                    if connected_positions:
                        # ì—°ê²°ëœ ì¹´ë©”ë¼ë“¤ì˜ ì¤‘ì‹¬ ì£¼ë³€ì— ë°°ì¹˜
                        avg_position = np.mean(connected_positions, axis=0)
                        position_std = np.std(connected_positions, axis=0)
                        
                        # ì¹´ë©”ë¼ IDì— ë”°ë¥¸ ìœ„ì¹˜ ë³€í™”
                        angle = cam_id * (2 * np.pi / len(self.image_features))
                        radius = 2.0 + np.random.normal(0, 0.5)  # ì•½ê°„ì˜ ëœë¤ì„±
                        
                        # ì›í˜• ë°°ì¹˜ + ì¤‘ì‹¬ìœ¼ë¡œë¶€í„°ì˜ ì˜¤í”„ì…‹
                        camera_pos = avg_position + np.array([
                            radius * np.cos(angle),
                            0.5 * np.sin(angle),  # ë†’ì´ ë³€í™”
                            radius * np.sin(angle)
                        ])
                        
                        # ì¤‘ì‹¬ì„ í–¥í•˜ëŠ” ë°©í–¥
                        look_at = avg_position
                        up = np.array([0.0, 1.0, 0.0])
                        
                        # ì¹´ë©”ë¼ íšŒì „ í–‰ë ¬ ê³„ì‚°
                        forward = look_at - camera_pos
                        forward = forward / (np.linalg.norm(forward) + 1e-10)
                        right = np.cross(forward, up)
                        right = right / (np.linalg.norm(right) + 1e-10)
                        up = np.cross(right, forward)
                        
                        R = np.array([right, up, -forward]).T
                        T = camera_pos
                    else:
                        # ê¸°ë³¸ ì›í˜• ë°°ì¹˜
                        angle = cam_id * (2 * np.pi / len(self.image_features))
                        radius = 3.0
                        
                        R = np.array([[np.cos(angle), 0, np.sin(angle)],
                                     [0, 1, 0],
                                     [-np.sin(angle), 0, np.cos(angle)]], dtype=np.float32)
                        T = np.array([radius * np.sin(angle), 0, radius * (1 - np.cos(angle))], dtype=np.float32)
                else:
                    # ê¸°ë³¸ ì›í˜• ë°°ì¹˜
                    angle = cam_id * (2 * np.pi / len(self.image_features))
                    radius = 3.0
                    
                    R = np.array([[np.cos(angle), 0, np.sin(angle)],
                                 [0, 1, 0],
                                 [-np.sin(angle), 0, np.cos(angle)]], dtype=np.float32)
                    T = np.array([radius * np.sin(angle), 0, radius * (1 - np.cos(angle))], dtype=np.float32)
                
                self.cameras[cam_id] = {
                    'R': R.astype(np.float32),
                    'T': T.astype(np.float32),
                    'K': self._estimate_intrinsics(cam_id)
                }
        
        print(f"  Estimated poses for {len(estimated_cameras)} cameras")
        print(f"  Used default poses for {unconnected_count} cameras")
        print(f"  Total cameras with poses: {len(self.cameras)}")
        
        # ğŸ”§ DEBUG: í¬ì¦ˆ ì¶”ì • ì™„ë£Œ ìƒíƒœ í™•ì¸
        print(f"    DEBUG: Final cameras dictionary has {len(self.cameras)} entries")
        if len(self.cameras) > 0:
            sample_cam = list(self.cameras.keys())[0]
            print(f"    DEBUG: Sample camera {sample_cam} has keys: {list(self.cameras[sample_cam].keys())}")
            print(f"    DEBUG: Sample camera R shape: {self.cameras[sample_cam]['R'].shape}")
            print(f"    DEBUG: Sample camera T shape: {self.cameras[sample_cam]['T'].shape}")
            print(f"    DEBUG: Sample camera K shape: {self.cameras[sample_cam]['K'].shape}")
        else:
            print(f"    DEBUG: âš ï¸ No cameras in dictionary after pose estimation!")
    
    def _estimate_relative_pose_robust(self, cam_i, cam_j, pair_key):
        """ê°œì„ ëœ ë‘ ì¹´ë©”ë¼ ê°„ ìƒëŒ€ í¬ì¦ˆ ì¶”ì • - ê·¹ë„ë¡œ ì™„í™”ëœ ë²„ì „"""
        matches = self.matches[pair_key]
        
        if len(matches) < 4:  # 6 â†’ 4ë¡œ ë” ì™„í™”
            print(f"    Pair {cam_i}-{cam_j}: Insufficient matches ({len(matches)} < 4)")
            return None, None
        
        # ë§¤ì¹­ì ë“¤ ì¶”ì¶œ
        kpts_i = self.image_features[cam_i]['keypoints']
        kpts_j = self.image_features[cam_j]['keypoints']
        
        print(f"    Pair {cam_i}-{cam_j}: kpts_i shape: {kpts_i.shape}, kpts_j shape: {kpts_j.shape}")
        
        # ğŸ”§ ê·¹ë„ë¡œ ì™„í™”ëœ ì‹ ë¢°ë„ ì„ê³„ê°’
        high_conf_matches = [(idx_i, idx_j, conf) for idx_i, idx_j, conf in matches if conf > 0.00001]  # 0.0001 â†’ 0.00001ë¡œ ëŒ€í­ ì™„í™”
        
        if len(high_conf_matches) < 4:  # 6 â†’ 4ë¡œ ë” ì™„í™”
            high_conf_matches = [(idx_i, idx_j, conf) for idx_i, idx_j, conf in matches if conf > 0.000001]  # 0.00001 â†’ 0.000001ë¡œ ëŒ€í­ ì™„í™”
        
        if len(high_conf_matches) < 4:  # 6 â†’ 4ë¡œ ë” ì™„í™”
            # ëª¨ë“  ë§¤ì¹­ì„ ì‚¬ìš©
            high_conf_matches = [(idx_i, idx_j, conf) for idx_i, idx_j, conf in matches]
        
        if len(high_conf_matches) < 4:  # 6 â†’ 4ë¡œ ë” ì™„í™”
            print(f"    Pair {cam_i}-{cam_j}: Insufficient high-confidence matches ({len(high_conf_matches)} < 4)")
            return None, None
        
        # ğŸ”§ ì¸ë±ìŠ¤ ë²”ìœ„ ê²€ì¦ ê°•í™”
        valid_matches = []
        for idx_i, idx_j, conf in high_conf_matches:
            if (isinstance(idx_i, (int, np.integer)) and isinstance(idx_j, (int, np.integer)) and
                idx_i >= 0 and idx_j >= 0 and 
                idx_i < len(kpts_i) and idx_j < len(kpts_j)):
                valid_matches.append((idx_i, idx_j, conf))
        
        if len(valid_matches) < 4:  # 6 â†’ 4ë¡œ ë” ì™„í™”
            print(f"    Pair {cam_i}-{cam_j}: Insufficient valid matches after index validation ({len(valid_matches)} < 4)")
            return None, None
        
        print(f"    Pair {cam_i}-{cam_j}: Using {len(valid_matches)} validated matches")
        
        # ğŸ”§ ê°œì„ ëœ í¬ì¸íŠ¸ ì¶”ì¶œ
        try:
            pts_i = np.array([kpts_i[idx_i] for idx_i, _, _ in valid_matches])
            pts_j = np.array([kpts_j[idx_j] for _, idx_j, _ in valid_matches])
        except IndexError as e:
            print(f"    IndexError during point extraction: {e}")
            return None, None
        
        # ğŸ”§ ê¸°í•˜í•™ì  ì¼ê´€ì„± ì‚¬ì „ ê²€ì¦ (ë” ê´€ëŒ€í•˜ê²Œ)
        if not self._check_geometric_consistency_very_relaxed(pts_i, pts_j):
            print(f"    Pair {cam_i}-{cam_j}: Failed geometric consistency check")
            return None, None
        
        # ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„°
        K_i = self.cameras.get(cam_i, {}).get('K', self._estimate_intrinsics(cam_i))
        K_j = self._estimate_intrinsics(cam_j)
        
        # ğŸ”§ ê·¹ë„ë¡œ ê´€ëŒ€í•œ Essential Matrix ì¶”ì • ë°©ë²•ë“¤
        methods = [
            (cv2.RANSAC, 0.5, 0.99),    # ê·¹ë„ë¡œ ê´€ëŒ€í•œ ì„ê³„ê°’
            (cv2.RANSAC, 1.0, 0.95),
            (cv2.RANSAC, 2.0, 0.90),
            (cv2.LMEDS, 0.5, 0.95),
            (cv2.RANSAC, 5.0, 0.85),    # ë§¤ìš° ê´€ëŒ€í•œ ì„¤ì •
            (cv2.RANSAC, 10.0, 0.80),   # ê·¹ë„ë¡œ ê´€ëŒ€í•œ ì„¤ì •
            (cv2.RANSAC, 20.0, 0.70)    # ìµœëŒ€í•œ ê´€ëŒ€í•œ ì„¤ì •
        ]
        
        best_R, best_T = None, None
        best_inliers = 0
        best_quality = 0
        
        for method, threshold, confidence in methods:
            try:
                # Essential Matrix ì¶”ì •
                E, mask = cv2.findEssentialMat(
                    pts_i, pts_j, K_i,
                    method=method,
                    prob=confidence,
                    threshold=threshold,
                    maxIters=500  # ë°˜ë³µ íšŸìˆ˜ ì¤„ì„
                )
                
                if E is None or E.shape != (3, 3):
                    continue
                
                # í¬ì¦ˆ ë³µì›
                _, R, T, mask = cv2.recoverPose(E, pts_i, pts_j, K_i, mask=mask)
                
                if R is None or T is None:
                    continue
                
                inliers = np.sum(mask)
                
                if inliers >= 2:  # 4 â†’ 2ë¡œ ê·¹ë„ë¡œ ì™„í™”
                    # ğŸ”§ ë” ê´€ëŒ€í•œ í¬ì¦ˆ í’ˆì§ˆ ê²€ì¦
                    quality_score = self._evaluate_pose_quality_very_relaxed(pts_i, pts_j, R, T.flatten(), K_i, K_j, mask)
                    
                    if quality_score > best_quality:
                        best_R, best_T = R, T.flatten()
                        best_inliers = inliers
                        best_quality = quality_score
                        
            except Exception as e:
                print(f"      Method {method} failed: {e}")
                continue
        
        if best_R is not None:
            print(f"    Pair {cam_i}-{cam_j}: Successfully estimated pose with {best_inliers} inliers, quality: {best_quality:.3f}")
        else:
            print(f"    Pair {cam_i}-{cam_j}: Failed to estimate pose")
        
        return best_R, best_T

    def _check_geometric_consistency_very_relaxed(self, pts_i, pts_j):
        """ê·¹ë„ë¡œ ê´€ëŒ€í•œ ê¸°í•˜í•™ì  ì¼ê´€ì„± ì‚¬ì „ ê²€ì¦"""
        try:
            # 1. í˜¸ëª¨ê·¸ë˜í”¼ ê¸°ë°˜ ì¼ê´€ì„± ê²€ì‚¬ (ê·¹ë„ë¡œ ê´€ëŒ€í•˜ê²Œ)
            H, mask = cv2.findHomography(pts_i, pts_j, cv2.RANSAC, 20.0)  # 10.0 â†’ 20.0ìœ¼ë¡œ ë” ì™„í™”
            if H is not None:
                homography_inliers = np.sum(mask)
                if homography_inliers < len(pts_i) * 0.01:  # 5% â†’ 1%ë¡œ ê·¹ë„ë¡œ ì™„í™”
                    return False
            
            # 2. í¬ì¸íŠ¸ ë¶„í¬ ê²€ì‚¬ (ê·¹ë„ë¡œ ê´€ëŒ€í•˜ê²Œ)
            if len(pts_i) > 2:  # 3 â†’ 2ë¡œ ë” ì™„í™”
                # í¬ì¸íŠ¸ë“¤ì˜ ë¶„ì‚° ê³„ì‚°
                var_i = np.var(pts_i, axis=0)
                var_j = np.var(pts_j, axis=0)
                
                # ë¶„ì‚°ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ ë‚˜ìœ ë¶„í¬ (ê·¹ë„ë¡œ ê´€ëŒ€í•˜ê²Œ)
                if np.min(var_i) < 0.1 or np.min(var_j) < 0.1:  # 1 â†’ 0.1ë¡œ ëŒ€í­ ì™„í™”
                    return False
            
            # 3. í¬ì¸íŠ¸ ê°„ ê±°ë¦¬ ê²€ì‚¬ (ê·¹ë„ë¡œ ê´€ëŒ€í•˜ê²Œ)
            if len(pts_i) > 1:  # 2 â†’ 1ë¡œ ë” ì™„í™”
                distances_i = cdist(pts_i, pts_i)
                distances_j = cdist(pts_j, pts_j)
                
                # ëŒ€ê°ì„  ì œê±°
                np.fill_diagonal(distances_i, np.inf)
                np.fill_diagonal(distances_j, np.inf)
                
                min_dist_i = np.min(distances_i)
                min_dist_j = np.min(distances_j)
                
                # ìµœì†Œ ê±°ë¦¬ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ë‚˜ìœ ë¶„í¬ (ê·¹ë„ë¡œ ê´€ëŒ€í•˜ê²Œ)
                if min_dist_i < 0.01 or min_dist_j < 0.01:  # 0.1 â†’ 0.01ë¡œ ëŒ€í­ ì™„í™”
                    return False
            
            return True
            
        except Exception as e:
            print(f"      Geometric consistency check failed: {e}")
            return True  # ì˜¤ë¥˜ì‹œ í†µê³¼

    def _evaluate_pose_quality_very_relaxed(self, pts_i, pts_j, R, T, K_i, K_j, mask):
        """ê·¹ë„ë¡œ ê´€ëŒ€í•œ í¬ì¦ˆ í’ˆì§ˆ í‰ê°€"""
        try:
            # 1. íšŒì „ í–‰ë ¬ ìœ íš¨ì„± í™•ì¸ (ê·¹ë„ë¡œ ê´€ëŒ€í•˜ê²Œ)
            det = np.linalg.det(R)
            if abs(det - 1.0) > 0.5:  # 0.3 â†’ 0.5ë¡œ ë” ì™„í™”
                return 0.0
            
            # 2. ì‚¼ê°ì¸¡ëŸ‰ í’ˆì§ˆ ê²€ì‚¬ (ê·¹ë„ë¡œ ê´€ëŒ€í•˜ê²Œ)
            P_i = K_i @ np.hstack([np.eye(3), np.zeros((3, 1))])
            P_j = K_j @ np.hstack([R, T.reshape(-1, 1)])
            
            # inlier í¬ì¸íŠ¸ë“¤ë§Œ ì‚¬ìš©
            inlier_pts_i = pts_i[mask.flatten()]
            inlier_pts_j = pts_j[mask.flatten()]
            
            if len(inlier_pts_i) < 2:  # 4 â†’ 2ë¡œ ê·¹ë„ë¡œ ì™„í™”
                return 0.0
            
            # ì‚¼ê°ì¸¡ëŸ‰ í…ŒìŠ¤íŠ¸ (ê·¹ë„ë¡œ ê´€ëŒ€í•˜ê²Œ)
            valid_points = 0
            total_error = 0.0
            
            for pt_i, pt_j in zip(inlier_pts_i, inlier_pts_j):
                try:
                    # ì‚¼ê°ì¸¡ëŸ‰
                    pt_4d = cv2.triangulatePoints(P_i, P_j, pt_i.reshape(2, 1), pt_j.reshape(2, 1))
                    
                    if abs(pt_4d[3, 0]) > 1e-10:
                        pt_3d = (pt_4d[:3] / pt_4d[3]).flatten()
                        
                        # ê±°ë¦¬ ì²´í¬ (ê·¹ë„ë¡œ ê´€ëŒ€í•˜ê²Œ)
                        if 0.0001 < np.linalg.norm(pt_3d) < 100000:  # 0.001~10000 â†’ 0.0001~100000ìœ¼ë¡œ ì™„í™”
                            # ì¬íˆ¬ì˜ ì˜¤ì°¨ ê³„ì‚°
                            proj_i = P_i @ np.append(pt_3d, 1)
                            proj_j = P_j @ np.append(pt_3d, 1)
                            
                            if proj_i[2] > 0 and proj_j[2] > 0:
                                proj_i_2d = proj_i[:2] / proj_i[2]
                                proj_j_2d = proj_j[:2] / proj_j[2]
                                
                                error_i = np.linalg.norm(proj_i_2d - pt_i)
                                error_j = np.linalg.norm(proj_j_2d - pt_j)
                                
                                total_error += max(error_i, error_j)
                                valid_points += 1
                                
                except:
                    continue
            
            if valid_points < 2:  # 4 â†’ 2ë¡œ ê·¹ë„ë¡œ ì™„í™”
                return 0.0
            
            # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (ê·¹ë„ë¡œ ê´€ëŒ€í•˜ê²Œ)
            avg_error = total_error / valid_points
            inlier_ratio = len(inlier_pts_i) / len(pts_i)
            
            # ì˜¤ì°¨ê°€ ì‘ê³  inlier ë¹„ìœ¨ì´ ë†’ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜ (ê·¹ë„ë¡œ ê´€ëŒ€í•˜ê²Œ)
            quality_score = inlier_ratio * (1.0 / (1.0 + avg_error * 0.001))  # 0.01 â†’ 0.001ë¡œ ì™„í™”
            
            return quality_score
            
        except Exception as e:
            print(f"      Pose quality evaluation failed: {e}")
            return 0.0
    
    def _estimate_pose_fallback(self, pts_i, pts_j, K_i, K_j):
        """OpenCV ì‹¤íŒ¨ì‹œ ì‚¬ìš©í•  fallback í¬ì¦ˆ ì¶”ì •"""
        try:
            # ê°„ë‹¨í•œ í˜¸ëª¨ê·¸ë˜í”¼ ê¸°ë°˜ ë°©ë²•
            H, mask = cv2.findHomography(pts_i, pts_j, cv2.RANSAC, 5.0)
            
            if H is None:
                return None, None
            
            # í˜¸ëª¨ê·¸ë˜í”¼ì—ì„œ íšŒì „ê³¼ ì´ë™ ì¶”ì¶œ (ê·¼ì‚¬)
            # ì´ëŠ” ì •í™•í•˜ì§€ ì•Šì§€ë§Œ ê¸°ë³¸ì ì¸ í¬ì¦ˆë¥¼ ì œê³µ
            K_inv = np.linalg.inv(K_i)
            R_approx = K_inv @ H @ K_i
            
            # SVDë¥¼ ì‚¬ìš©í•˜ì—¬ íšŒì „ í–‰ë ¬ë¡œ ì •ê·œí™”
            U, _, Vt = np.linalg.svd(R_approx)
            R = U @ Vt
            
            # íšŒì „ í–‰ë ¬ ìœ íš¨ì„± ê²€ì‚¬
            if not self._is_valid_rotation_matrix(R):
                # ê¸°ë³¸ íšŒì „ í–‰ë ¬ ì‚¬ìš©
                R = np.eye(3)
            
            # ì´ë™ ë²¡í„° ì¶”ì • (ê°„ë‹¨í•œ ê·¼ì‚¬)
            T = np.array([0.1, 0.0, 0.0])  # ê¸°ë³¸ ì´ë™
            
            return R, T
            
        except Exception as e:
            print(f"      Fallback pose estimation failed: {e}")
            return None, None
    
    def _is_valid_rotation_matrix(self, R):
        """íšŒì „ í–‰ë ¬ì´ ìœ íš¨í•œì§€ í™•ì¸"""
        try:
            # í–‰ë ¬ì‹ì´ 1ì— ê°€ê¹Œìš´ì§€ í™•ì¸
            det = np.linalg.det(R)
            if abs(det - 1.0) > 0.1:
                return False
            
            # R * R^T = Iì¸ì§€ í™•ì¸
            I = np.eye(3)
            RRt = R @ R.T
            if np.max(np.abs(RRt - I)) > 0.1:
                return False
            
            return True
        except:
            return False
    
    def _verify_pose_quality_very_relaxed(self, pts_i, pts_j, R, T, K_i, K_j):
        """ë§¤ìš° ì™„í™”ëœ í¬ì¦ˆ í’ˆì§ˆ ê²€ì¦"""
        # ì¬íˆ¬ì˜ ì˜¤ì°¨ ê³„ì‚°
        P_i = K_i @ np.hstack([np.eye(3), np.zeros((3, 1))])
        P_j = K_j @ np.hstack([R, T.reshape(-1, 1)])
        
        errors = []
        depths_i = []
        depths_j = []
        
        for pt_i, pt_j in zip(pts_i, pts_j):
            # ì‚¼ê°ì¸¡ëŸ‰
            point_4d = cv2.triangulatePoints(P_i, P_j, pt_i.reshape(2, 1), pt_j.reshape(2, 1))
            if abs(point_4d[3, 0]) > 1e-10:
                point_3d = (point_4d[:3] / point_4d[3]).flatten()
                
                # ì¬íˆ¬ì˜ (3D ì¢Œí‘œê³„ì—ì„œ)
                proj_i_3d = P_i @ np.append(point_3d, 1)
                proj_j_3d = P_j @ np.append(point_3d, 1)
                
                # 2D ì¢Œí‘œë¡œ ë³€í™˜
                proj_i_2d = proj_i_3d[:2] / proj_i_3d[2]
                proj_j_2d = proj_j_3d[:2] / proj_j_3d[2]
                
                error_i = np.linalg.norm(proj_i_2d - pt_i)
                error_j = np.linalg.norm(proj_j_2d - pt_j)
                errors.append(max(error_i, error_j))
                
                # ê¹Šì´ ì •ë³´ ì €ì¥ (3D ì¢Œí‘œê³„ì—ì„œ)
                depths_i.append(proj_i_3d[2])
                depths_j.append(proj_j_3d[2])
        
        if len(errors) < 2:  # ë” ë‚®ì€ ì„ê³„ê°’ (3 â†’ 2)
            return False
        
        # ì˜¤ì°¨ í†µê³„
        median_error = np.median(errors)
        mean_error = np.mean(errors)
        max_error = np.max(errors)
        
        # ê¹Šì´ ê²€ì¦ (ë” ì™„í™”ëœ ì¡°ê±´)
        if depths_i and depths_j:
            depths_i = np.array(depths_i)
            depths_j = np.array(depths_j)
            
            # ê¹Šì´ê°€ ì–‘ìˆ˜ì¸ì§€ í™•ì¸
            if np.any(depths_i <= 0) or np.any(depths_j <= 0):
                return False
            
            # ê¹Šì´ ë¹„ìœ¨ì´ í•©ë¦¬ì ì¸ì§€ í™•ì¸ (ë” ì™„í™”ëœ ì¡°ê±´)
            depth_ratios = depths_j / depths_i
            if np.median(depth_ratios) < 0.01 or np.median(depth_ratios) > 50:  # 0.05~20 â†’ 0.01~50
                return False
        
        # ì˜¤ì°¨ ì„ê³„ê°’ ê²€ì¦ (ë” ì™„í™”ëœ ì¡°ê±´)
        pose_quality = (median_error < 15.0 and   # 8.0 â†’ 15.0
                mean_error < 20.0 and    # 10.0 â†’ 20.0
                max_error < 50.0)        # 20.0 â†’ 50.0
        
        if not pose_quality:
            print(f"      Pose quality check failed: median={median_error:.2f}, mean={mean_error:.2f}, max={max_error:.2f}")
        
        return pose_quality
    
    def _estimate_intrinsics(self, cam_id):
        """ê°œì„ ëœ ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„° ì¶”ì • (COLMAP ìš°ì„ )"""
        h, w = self.image_features[cam_id]['image_size']
        
        # COLMAP reconstructionì´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©
        try:
            colmap_cameras = self._get_colmap_intrinsics()
            if colmap_cameras and cam_id in colmap_cameras:
                camera = colmap_cameras[cam_id]
                width, height = camera.width, camera.height
                
                # PINHOLE ëª¨ë¸ ê°€ì • (fx, fy, cx, cy)
                if len(camera.params) == 4:
                    fx, fy, cx, cy = camera.params
                    # COLMAPì—ì„œ ì¶”ì •í•œ ì •í™•í•œ focal length ì‚¬ìš©
                    K = np.array([
                        [fx, 0, cx],
                        [0, fy, cy],
                        [0, 0, 1]
                    ], dtype=np.float32)
                    print(f"    Camera {cam_id}: Using COLMAP focal length (fx={fx:.1f}, fy={fy:.1f})")
                    return K
        except Exception as e:
            print(f"    Camera {cam_id}: COLMAP intrinsics failed, using default: {e}")
            # ì†ìƒëœ COLMAP íŒŒì¼ë“¤ì„ ì •ë¦¬
            try:
                output_dir = getattr(self, 'output_dir', None)
                if output_dir:
                    reconstruction_path = Path(output_dir) / "sparse" / "0"
                    for file_name in ["cameras.bin", "images.bin", "points3D.bin"]:
                        file_path = reconstruction_path / file_name
                        if file_path.exists():
                            file_path.unlink()
                            print(f"    Deleted corrupted {file_name}")
            except Exception as cleanup_error:
                print(f"    Failed to cleanup corrupted files: {cleanup_error}")
        
        # COLMAPì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì¶”ì • ì‚¬ìš©
        focal = max(w, h) * 0.9  # ì•½ê°„ ë³´ìˆ˜ì ì¸ ì¶”ì •
        cx, cy = w / 2, h / 2
        
        K = np.array([
            [focal, 0, cx],
            [0, focal, cy],
            [0, 0, 1]
        ], dtype=np.float32)
        
        print(f"    Camera {cam_id}: Using default focal length ({focal:.1f})")
        return K
    
    def _get_colmap_intrinsics(self):
        """COLMAP reconstructionì—ì„œ ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„° ì½ê¸°"""
        try:
            # COLMAP reconstruction ê²½ë¡œ í™•ì¸
            output_dir = getattr(self, 'output_dir', None)
            if output_dir is None:
                return None
            
            reconstruction_path = Path(output_dir) / "sparse" / "0"
            cameras_bin = reconstruction_path / "cameras.bin"
            
            if not cameras_bin.exists():
                print(f"    cameras.bin not found at {cameras_bin}")
                return None
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            file_size = cameras_bin.stat().st_size
            if file_size == 0:
                print(f"    cameras.bin is empty")
                return None
            
            print(f"    Reading COLMAP intrinsics from {cameras_bin} ({file_size} bytes)")
            
            # COLMAP reconstruction íŒŒì‹±
            try:
                from scene.colmap_loader import read_intrinsics_binary
                cameras = read_intrinsics_binary(str(cameras_bin))
                print(f"    Successfully loaded {len(cameras)} cameras from COLMAP")
                
                # ì´ë¯¸ì§€ IDì™€ ì¹´ë©”ë¼ ID ë§¤í•‘
                images_bin = reconstruction_path / "images.bin"
                if images_bin.exists():
                    try:
                        from scene.colmap_loader import read_extrinsics_binary
                        images = read_extrinsics_binary(str(images_bin))
                        
                        # ì´ë¯¸ì§€ ID -> ì¹´ë©”ë¼ ID ë§¤í•‘
                        image_to_camera = {}
                        for image_id, image in images.items():
                            image_to_camera[image_id] = image.camera_id
                        
                        return image_to_camera, cameras
                    except Exception as e:
                        print(f"    Failed to read images.bin: {e}")
                        return None
                
                return None
                
            except Exception as e:
                print(f"    Failed to read cameras.bin: {e}")
                return None
            
        except Exception as e:
            print(f"    COLMAP intrinsics ì½ê¸° ì‹¤íŒ¨: {e}")
            # íŒŒì¼ì´ ì†ìƒë˜ì—ˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì‚­ì œ ì‹œë„
            try:
                if cameras_bin.exists():
                    cameras_bin.unlink()
                    print(f"    Deleted corrupted cameras.bin")
                if images_bin.exists():
                    images_bin.unlink()
                    print(f"    Deleted corrupted images.bin")
                if points3d_bin.exists():
                    points3d_bin.unlink()
                    print(f"    Deleted corrupted points3D.bin")
            except Exception as del_error:
                print(f"    Failed to delete corrupted files: {del_error}")
            return None
    
    def _triangulate_all_points_robust(self):
        """ê°œì„ ëœ ì‚¼ê°ì¸¡ëŸ‰ - ë” ë§ì€ í¬ì¸íŠ¸ ìƒì„±"""
        point_id = 0
        total_matches_processed = 0
        total_valid_matches = 0
        total_triangulated = 0
        total_validated = 0
        
        print(f"  Processing {len(self.matches)} image pairs for triangulation...")
        
        for (cam_i, cam_j), matches in self.matches.items():
            if cam_i not in self.cameras or cam_j not in self.cameras:
                continue
            
            try:
                # íˆ¬ì˜ í–‰ë ¬ ìƒì„±
                P_i = self._get_projection_matrix(cam_i)
                P_j = self._get_projection_matrix(cam_j)
                
                kpts_i = self.image_features[cam_i]['keypoints']
                kpts_j = self.image_features[cam_j]['keypoints']
                
                # ğŸ”§ ë” ì™„í™”ëœ ì‹ ë¢°ë„ ì„ê³„ê°’
                high_conf_matches = [(idx_i, idx_j, conf) for idx_i, idx_j, conf in matches if conf > 0.05]  # 0.2 â†’ 0.05ë¡œ ì™„í™”
                total_matches_processed += len(matches)
                
                # ì¸ë±ìŠ¤ ë²”ìœ„ ê²€ì¦
                valid_matches = []
                for idx_i, idx_j, conf in high_conf_matches:
                    if (idx_i < len(kpts_i) and idx_j < len(kpts_j) and 
                        idx_i >= 0 and idx_j >= 0):
                        valid_matches.append((idx_i, idx_j, conf))
                
                total_valid_matches += len(valid_matches)
                
                # ğŸ”§ ë” ì ê·¹ì ì¸ ì‚¼ê°ì¸¡ëŸ‰
                if len(valid_matches) > 5:  # 10 â†’ 5ë¡œ ì™„í™”
                    batch_size = min(100, len(valid_matches))  # ë°°ì¹˜ í¬ê¸° ì¦ê°€
                    for batch_start in range(0, len(valid_matches), batch_size):
                        batch_end = min(batch_start + batch_size, len(valid_matches))
                        batch_matches = valid_matches[batch_start:batch_end]
                        
                        # ë°°ì¹˜ ì‚¼ê°ì¸¡ëŸ‰
                        pts_i_batch = np.array([kpts_i[idx_i] for idx_i, _, _ in batch_matches])
                        pts_j_batch = np.array([kpts_j[idx_j] for _, idx_j, _ in batch_matches])
                        
                        try:
                            # OpenCV ë°°ì¹˜ ì‚¼ê°ì¸¡ëŸ‰
                            points_4d = cv2.triangulatePoints(P_i, P_j, pts_i_batch.T, pts_j_batch.T)
                            
                            # 4Dì—ì„œ 3Dë¡œ ë³€í™˜ (ë” ì™„í™”ëœ ê²€ì¦)
                            for i in range(points_4d.shape[1]):
                                point_4d = points_4d[:, i]
                                
                                if abs(point_4d[3]) < 1e-10:
                                    continue
                                
                                point_3d = (point_4d[:3] / point_4d[3]).flatten()
                                total_triangulated += 1
                                
                                # ğŸ”§ ë” ì™„í™”ëœ ìœ íš¨ì„± ê²€ì‚¬
                                if self._is_point_valid_relaxed(point_3d, cam_i, cam_j, pts_i_batch[i], pts_j_batch[i]):
                                    # ìƒ‰ìƒ ì¶”ì •
                                    color = self._estimate_point_color_robust(point_3d, cam_i, batch_matches[i][0])
                                    
                                    # 3D í¬ì¸íŠ¸ ì €ì¥
                                    self.points_3d[point_id] = {
                                        'xyz': point_3d.astype(np.float32),
                                        'color': color,
                                        'observations': [(cam_i, pts_i_batch[i], batch_matches[i][2]), 
                                                        (cam_j, pts_j_batch[i], batch_matches[i][2])]
                                    }
                                    
                                    # ê´€ì°° ë°ì´í„° ì¶”ê°€
                                    self.point_observations[point_id].append((cam_i, pts_i_batch[i], batch_matches[i][2]))
                                    self.point_observations[point_id].append((cam_j, pts_j_batch[i], batch_matches[i][2]))
                                    
                                    point_id += 1
                                    total_validated += 1
                                    
                        except Exception as e:
                            print(f"    Batch triangulation failed for pair {cam_i}-{cam_j}: {e}")
                            continue
                else:
                    # ê°œë³„ ì‚¼ê°ì¸¡ëŸ‰ (ê¸°ì¡´ ë°©ì‹)
                    for idx_i, idx_j, conf in valid_matches:
                        try:
                            # ì‚¼ê°ì¸¡ëŸ‰
                            pt_i = kpts_i[idx_i].astype(np.float32)
                            pt_j = kpts_j[idx_j].astype(np.float32)
                            
                            point_4d = cv2.triangulatePoints(P_i, P_j, pt_i.reshape(2, 1), pt_j.reshape(2, 1))
                            
                            if abs(point_4d[3, 0]) < 1e-10:
                                continue
                                
                            point_3d = (point_4d[:3] / point_4d[3]).flatten()
                            total_triangulated += 1
                            
                            # ğŸ”§ ë” ì™„í™”ëœ ìœ íš¨ì„± ê²€ì‚¬
                            if self._is_point_valid_relaxed(point_3d, cam_i, cam_j, pt_i, pt_j):
                                # ìƒ‰ìƒ ì¶”ì •
                                color = self._estimate_point_color_robust(point_3d, cam_i, idx_i)
                                
                                # 3D í¬ì¸íŠ¸ ì €ì¥
                                self.points_3d[point_id] = {
                                    'xyz': point_3d.astype(np.float32),
                                    'color': color,
                                    'observations': [(cam_i, pt_i, conf), (cam_j, pt_j, conf)]
                                }
                                
                                # ê´€ì°° ë°ì´í„° ì¶”ê°€
                                self.point_observations[point_id].append((cam_i, pt_i, conf))
                                self.point_observations[point_id].append((cam_j, pt_j, conf))
                                
                                point_id += 1
                                total_validated += 1
                                
                        except Exception as e:
                            continue
                        
            except Exception as e:
                print(f"    Error processing pair {cam_i}-{cam_j}: {e}")
                continue
        
        print(f"  Triangulation statistics:")
        print(f"    Total matches processed: {total_matches_processed}")
        print(f"    Valid matches: {total_valid_matches}")
        print(f"    Successfully triangulated: {total_triangulated}")
        print(f"    Passed validation: {total_validated}")
        print(f"    Final 3D points: {len(self.points_3d)}")
        
        return len(self.points_3d)

    def _is_point_valid_relaxed(self, point_3d, cam_i, cam_j, pt_i, pt_j):
        """ì™„í™”ëœ 3D í¬ì¸íŠ¸ ìœ íš¨ì„± ê²€ì‚¬"""
        
        # 1. ê¸°ë³¸ NaN/Inf ì²´í¬
        if np.any(np.isnan(point_3d)) or np.any(np.isinf(point_3d)):
            return False
        
        # 2. ê±°ë¦¬ ì œí•œ (ë” ê´€ëŒ€í•œ ë²”ìœ„)
        distance = np.linalg.norm(point_3d)
        if distance > 500 or distance < 0.001:  # 100 â†’ 500, 0.01 â†’ 0.001ë¡œ ì™„í™”
            return False
        
        # 3. ì™„í™”ëœ ì¬íˆ¬ì˜ ì˜¤ì°¨ ì²´í¬
        try:
            max_reprojection_error = 0.0
            
            for cam_id, pt_observed in [(cam_i, pt_i), (cam_j, pt_j)]:
                if cam_id not in self.cameras:
                    continue
                
                cam = self.cameras[cam_id]
                K, R, T = cam['K'], cam['R'], cam['T']
                
                # ì¹´ë©”ë¼ ì¢Œí‘œê³„ë¡œ ë³€í™˜
                point_cam = R @ (point_3d - T)
                
                # ê¹Šì´ ì²´í¬ (ë” ì™„í™”ëœ ì¡°ê±´)
                if point_cam[2] <= 0.001:  # 0.01 â†’ 0.001ë¡œ ì™„í™”
                    return False
                
                # ì¬íˆ¬ì˜
                point_2d_proj = K @ point_cam
                
                if abs(point_2d_proj[2]) < 1e-10:
                    return False
                    
                point_2d_proj = point_2d_proj[:2] / point_2d_proj[2]
                
                # ì¬íˆ¬ì˜ ì˜¤ì°¨ ê³„ì‚°
                error = np.linalg.norm(point_2d_proj - pt_observed)
                max_reprojection_error = max(max_reprojection_error, error)
            
            # ì¬íˆ¬ì˜ ì˜¤ì°¨ ì„ê³„ê°’ (ë” ê´€ëŒ€í•˜ê²Œ)
            if max_reprojection_error > 50.0:  # 10 â†’ 50ìœ¼ë¡œ ì™„í™”
                return False
            
            return True
            
        except Exception as e:
            return False

    def _estimate_point_color_robust(self, point_3d, cam_id, kpt_idx):
        """ê°œì„ ëœ 3D í¬ì¸íŠ¸ ìƒ‰ìƒ ì¶”ì •"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì´ë¯¸ì§€ì—ì„œ ìƒ‰ìƒì„ ìƒ˜í”Œë§
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ëœë¤ ìƒ‰ìƒ ì‚¬ìš©
        return np.random.rand(3).astype(np.float32)
    
    def _bundle_adjustment_robust(self, max_iterations=50):
        """ê°œì„ ëœ Bundle Adjustment - ë” ì™„í™”ëœ ì¡°ê±´"""
        
        n_cameras = len(self.cameras)
        n_points = len(self.points_3d)
        
        if n_cameras < 2 or n_points < 5:  # 20 â†’ 5ë¡œ ëŒ€í­ ì™„í™”
            print("  Insufficient data for bundle adjustment")
            return
        
        # ê´€ì°° ë°ì´í„° ìˆ˜ ê³„ì‚°
        total_observations = sum(len(obs) for obs in self.point_observations.values())
        n_residuals = total_observations * 2  # ê° ê´€ì°°ë‹¹ 2ê°œ ì”ì°¨ (x, y)
        n_variables = n_cameras * 6 + n_points * 3  # ì¹´ë©”ë¼ 6DOF + í¬ì¸íŠ¸ 3DOF
        
        print(f"  BA Statistics:")
        print(f"    Cameras: {n_cameras}, Points: {n_points}")
        print(f"    Observations: {total_observations}")
        print(f"    Residuals: {n_residuals}, Variables: {n_variables}")
        
        # ğŸ”§ ë” ì™„í™”ëœ ë°©ë²• ì„ íƒ
        if n_residuals < n_variables:  # 2ë°° ì¡°ê±´ ì œê±°
            print(f"  âš ï¸  Under-constrained problem: {n_residuals} residuals < {n_variables} variables")
            print("  Using 'trf' method with very conservative settings")
            method = 'trf'
        else:
            print("  Using 'lm' method")
            method = 'lm'
        
        try:
            params = self._pack_parameters()
        except Exception as e:
            print(f"  Parameter packing failed: {e}")
            return
        
        try:
            # ğŸ”§ ë” ì™„í™”ëœ BA ì„¤ì •
            if method == 'trf':
                result = least_squares(
                    self._compute_residuals_improved,
                    params,
                    method='trf',
                    max_nfev=max_iterations,  # ë°˜ë³µ íšŸìˆ˜ ì¤„ì„
                    verbose=1,
                    ftol=1e-3,  # ë” ì™„í™”ëœ ìˆ˜ë ´ ì¡°ê±´
                    xtol=1e-3,
                    bounds=(-np.inf, np.inf)
                )
            else:
                result = least_squares(
                    self._compute_residuals_improved,
                    params,
                    method='lm',
                    max_nfev=max_iterations * 2,  # ë°˜ë³µ íšŸìˆ˜ ì¤„ì„
                    verbose=1,
                    ftol=1e-4,  # ë” ì™„í™”ëœ ìˆ˜ë ´ ì¡°ê±´
                    xtol=1e-4
                )
            
            # ê²°ê³¼ ì–¸íŒ¨í‚¹
            self._unpack_parameters(result.x)
            
            print(f"  Bundle adjustment completed. Final cost: {result.cost:.6f}")
            print(f"  Method: {method}, Iterations: {result.nfev}")
            
            # ğŸ”§ ë” ì™„í™”ëœ cost í‰ê°€
            if result.cost > 1000:
                print(f"  âš ï¸  ë†’ì€ BA cost: {result.cost:.2f}")
                print("  í¬ì¸íŠ¸ í´ë¼ìš°ë“œ í’ˆì§ˆì´ ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
            elif result.cost > 100:
                print(f"  âš ï¸  ì¤‘ê°„ BA cost: {result.cost:.2f}")
            else:
                print(f"  âœ… ì¢‹ì€ BA cost: {result.cost:.2f}")
            
        except Exception as e:
            print(f"  Bundle adjustment failed: {e}")
            print("  Continuing without bundle adjustment...")

    def _compute_residuals_improved(self, params):
        """ê°œì„ ëœ Bundle Adjustment ì”ì°¨ ê³„ì‚° (ë” ì™„í™”ëœ ë²„ì „)"""
        residuals = []
        
        # íŒŒë¼ë¯¸í„° ì–¸íŒ¨í‚¹
        try:
            self._unpack_parameters(params)
        except Exception as e:
            print(f"    Warning: Parameter unpacking failed: {e}")
            return np.ones(100) * 1e6
        
        # ê° ê´€ì°°ì— ëŒ€í•œ ì¬íˆ¬ì˜ ì˜¤ì°¨ ê³„ì‚°
        for point_id, observations in self.point_observations.items():
            if point_id not in self.points_3d:
                continue
                
            point_3d = self.points_3d[point_id]['xyz']
            
            for cam_id, observed_pt, conf in observations:
                if cam_id not in self.cameras:
                    continue
                
                try:
                    cam = self.cameras[cam_id]
                    K = cam['K']
                    R = cam['R']
                    T = cam['T']
                    
                    # ì¹´ë©”ë¼ ì¢Œí‘œê³„ë¡œ ë³€í™˜
                    point_cam = R @ (point_3d - T)
                    
                    # ê¹Šì´ ì²´í¬ (ë” ì™„í™”ëœ ì¡°ê±´)
                    if point_cam[2] <= 0:
                        residuals.extend([10.0, 10.0])  # ë” ì‘ì€ í˜ë„í‹°
                        continue
                    
                    # ì¬íˆ¬ì˜
                    point_2d_proj = K @ point_cam
                    if abs(point_2d_proj[2]) < 1e-10:
                        residuals.extend([10.0, 10.0])
                        continue
                    point_2d_proj = point_2d_proj[:2] / point_2d_proj[2]
                    
                    # ğŸ”§ ë” ì™„í™”ëœ ì”ì°¨ ê³„ì‚°
                    residual = point_2d_proj - observed_pt
                    
                    # ğŸ”§ ë” ì™„í™”ëœ Huber loss
                    residual = self._apply_huber_loss_improved(residual, delta=10.0)  # 3.0 â†’ 10.0ìœ¼ë¡œ ì™„í™”
                    
                    # ğŸ”§ ë” ì™„í™”ëœ ì‹ ë¢°ë„ ê°€ì¤‘ì¹˜
                    weight = np.clip(conf, 0.05, 1.0)  # 0.1 â†’ 0.05ë¡œ ì™„í™”
                    
                    # ğŸ”§ ë” ì™„í™”ëœ ìŠ¤ì¼€ì¼ë§
                    residual = residual * weight * 0.01  # 0.05 â†’ 0.01ë¡œ ì™„í™”
                    
                    residuals.extend(residual)
                    
                except Exception as e:
                    residuals.extend([2.0, 2.0])  # ë” ì‘ì€ ê¸°ë³¸ ì˜¤ì°¨
        
        if len(residuals) == 0:
            return np.ones(100) * 1e6
        
        residuals = np.array(residuals)
        
        # NaNì´ë‚˜ ë¬´í•œëŒ€ ê°’ ì²´í¬
        if np.any(np.isnan(residuals)) or np.any(np.isinf(residuals)):
            return np.ones(len(residuals)) * 1e6
        
        return residuals

    def _apply_huber_loss_improved(self, residual, delta=3.0):
        """ê°œì„ ëœ Huber loss ì ìš©"""
        abs_residual = np.abs(residual)
        mask = abs_residual <= delta
        
        result = np.zeros_like(residual)
        result[mask] = residual[mask]
        result[~mask] = delta * np.sign(residual[~mask]) * (2 * np.sqrt(abs_residual[~mask] / delta) - 1)
        
        return result
    
    def _expand_point_observations(self):
        """í¬ì¸íŠ¸ ê´€ì°° ë°ì´í„° í™•ì¥ìœ¼ë¡œ ì”ì°¨ ìˆ˜ ì¦ê°€"""
        
        print("  Expanding point observations...")
        
        original_obs = sum(len(obs) for obs in self.point_observations.values())
        
        # ê° 3D í¬ì¸íŠ¸ì— ëŒ€í•´ ë‹¤ë¥¸ ì¹´ë©”ë¼ì—ì„œì˜ ì¬íˆ¬ì˜ í™•ì¸
        for point_id, point_data in self.points_3d.items():
            point_3d = point_data['xyz']
            current_cams = set([obs[0] for obs in self.point_observations[point_id]])
            
            # ë‹¤ë¥¸ ì¹´ë©”ë¼ë“¤ì—ì„œë„ ì´ í¬ì¸íŠ¸ê°€ ë³´ì´ëŠ”ì§€ í™•ì¸
            for cam_id in self.cameras:
                if cam_id in current_cams:
                    continue
                
                try:
                    # ì¬íˆ¬ì˜ ê³„ì‚°
                    cam = self.cameras[cam_id]
                    K, R, T = cam['K'], cam['R'], cam['T']
                    
                    point_cam = R @ (point_3d - T)
                    if point_cam[2] <= 0:  # ì¹´ë©”ë¼ ë’¤ìª½
                        continue
                    
                    point_2d_proj = K @ point_cam
                    point_2d_proj = point_2d_proj[:2] / point_2d_proj[2]
                    
                    # ì´ë¯¸ì§€ ê²½ê³„ í™•ì¸
                    h, w = self.image_features[cam_id]['image_size']
                    if (0 <= point_2d_proj[0] < w and 0 <= point_2d_proj[1] < h):
                        
                        # í•´ë‹¹ ì¹´ë©”ë¼ì˜ í‚¤í¬ì¸íŠ¸ì™€ ê°€ê¹Œìš´ì§€ í™•ì¸
                        kpts = self.image_features[cam_id]['keypoints']
                        distances = np.linalg.norm(kpts - point_2d_proj, axis=1)
                        min_idx = np.argmin(distances)
                        
                        if distances[min_idx] < 30.0:  # 30 í”½ì…€ ë‚´
                            # ê´€ì°° ì¶”ê°€
                            confidence = 0.1  # ë‚®ì€ ì‹ ë¢°ë„
                            self.point_observations[point_id].append((cam_id, point_2d_proj, confidence))
                            
                except Exception:
                    continue
        
        expanded_obs = sum(len(obs) for obs in self.point_observations.values())
        print(f"    Expanded observations: {original_obs} â†’ {expanded_obs}")
    
    def _rotation_matrix_to_angle_axis(self, R):
        """íšŒì „ í–‰ë ¬ì„ ë¡œë“œë¦¬ê²ŒìŠ¤ ë²¡í„°ë¡œ ë³€í™˜ (ì •êµí•œ êµ¬í˜„)"""
        # ìˆ˜ì¹˜ì ìœ¼ë¡œ ì•ˆì •í•œ ë³€í™˜
        trace = np.trace(R)
        
        # ë‹¨ìœ„ í–‰ë ¬ì¸ ê²½ìš° (íšŒì „ ì—†ìŒ)
        if trace > 3.0 - 1e-6:
            return np.zeros(3, dtype=np.float32)
        
        # 180ë„ íšŒì „ì¸ ê²½ìš° (íŠ¹ìˆ˜ ì²˜ë¦¬)
        if trace < -1.0 + 1e-6:
            # ê°€ì¥ í° ëŒ€ê°ì„  ì›ì†Œ ì°¾ê¸°
            k = np.argmax(np.diag(R))
            
            # íšŒì „ì¶• ê³„ì‚°
            axis = np.zeros(3)
            axis[k] = np.sqrt((R[k, k] + 1.0) / 2.0)
            
            for i in range(3):
                if i != k:
                    axis[i] = R[k, i] / (2.0 * axis[k])
            
            return axis * np.pi
        
        # ì¼ë°˜ì ì¸ ê²½ìš°
        angle = np.arccos(np.clip((trace - 1.0) / 2.0, -1.0, 1.0))
        
        # íšŒì „ì¶• ê³„ì‚° (ì•ˆì •ì ì¸ ë°©ë²•)
        if angle < 1e-6:
            return np.zeros(3, dtype=np.float32)
        
        # ë°˜ëŒ€ì¹­ í–‰ë ¬ì—ì„œ ì¶• ì¶”ì¶œ
        axis = np.array([
            R[2, 1] - R[1, 2],
            R[0, 2] - R[2, 0],
            R[1, 0] - R[0, 1]
        ])
        
        # ì¶• ì •ê·œí™”
        axis_norm = np.linalg.norm(axis)
        if axis_norm < 1e-6:
            return np.zeros(3, dtype=np.float32)
        
        axis = axis / axis_norm
        
        # ê°ë„ ìŠ¤ì¼€ì¼ë§
        sin_angle = axis_norm / 2.0
        
        # ì•ˆì •ì ì¸ ê°ë„ ê³„ì‚°
        if sin_angle > 1e-6:
            angle = np.arcsin(np.clip(sin_angle, -1.0, 1.0))
            if trace < 1.0:
                angle = np.pi - angle
        
        return (axis * angle).astype(np.float32)
    
    def _angle_axis_to_rotation_matrix(self, angle_axis):
        """ë¡œë“œë¦¬ê²ŒìŠ¤ ë²¡í„°ë¥¼ íšŒì „ í–‰ë ¬ë¡œ ë³€í™˜ (ì •êµí•œ êµ¬í˜„)"""
        angle = np.linalg.norm(angle_axis)
        
        # íšŒì „ì´ ì—†ëŠ” ê²½ìš°
        if angle < 1e-8:
            return np.eye(3, dtype=np.float32)
        
        # íšŒì „ì¶• ì •ê·œí™”
        axis = angle_axis / angle
        
        # ë¡œë“œë¦¬ê²ŒìŠ¤ ê³µì‹ ì‚¬ìš©
        cos_angle = np.cos(angle)
        sin_angle = np.sin(angle)
        one_minus_cos = 1.0 - cos_angle
        
        # ì™¸ì  í–‰ë ¬
        outer = np.outer(axis, axis)
        
        # ë°˜ëŒ€ì¹­ í–‰ë ¬ (êµì°¨ê³± í–‰ë ¬)
        cross = np.array([
            [0, -axis[2], axis[1]],
            [axis[2], 0, -axis[0]],
            [-axis[1], axis[0], 0]
        ])
        
        # ë¡œë“œë¦¬ê²ŒìŠ¤ ê³µì‹: R = I + sin(Î¸)[K] + (1-cos(Î¸))[K]Â²
        # ì—¬ê¸°ì„œ [K]ëŠ” ë°˜ëŒ€ì¹­ í–‰ë ¬, [K]Â² = axis*axis^T - I
        R = (cos_angle * np.eye(3) + 
             sin_angle * cross + 
             one_minus_cos * outer)
        
        return R.astype(np.float32)
    
    def _refine_point_cloud(self):
        """í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ì •ì œ (ë” ì™„í™”ëœ ë²„ì „)"""
        print(f"  Refining point cloud...")
        
        # 1. ì¤‘ë³µ í¬ì¸íŠ¸ ì œê±° (ë” ì™„í™”ëœ ì¡°ê±´)
        points_to_remove = set()
        points_list = list(self.points_3d.items())
        
        for i, (id1, point1) in enumerate(points_list):
            for j, (id2, point2) in enumerate(points_list[i+1:], i+1):
                if id1 in points_to_remove or id2 in points_to_remove:
                    continue
                
                dist = np.linalg.norm(point1['xyz'] - point2['xyz'])
                if dist < 0.0001:  # 0.001 â†’ 0.0001ë¡œ ë” ì—„ê²©í•˜ê²Œ
                    points_to_remove.add(id2)
        
        # ì¤‘ë³µ í¬ì¸íŠ¸ ì œê±°
        for point_id in points_to_remove:
            del self.points_3d[point_id]
            if point_id in self.point_observations:
                del self.point_observations[point_id]
        
        print(f"  Removed {len(points_to_remove)} duplicate points")
        print(f"  Final point cloud: {len(self.points_3d)} points")
    
    def _get_projection_matrix(self, cam_id):
        """ì¹´ë©”ë¼ íˆ¬ì˜ í–‰ë ¬ ìƒì„± (ìˆ˜ì •ëœ ë²„ì „)"""
        cam = self.cameras[cam_id]
        K, R, T = cam['K'], cam['R'], cam['T']
        
        # Tê°€ ì›”ë“œ ì¢Œí‘œê³„ì˜ ì¹´ë©”ë¼ ì¤‘ì‹¬ì´ë¼ê³  ê°€ì •
        # P = K[R|t] where t = -R * T (ì¹´ë©”ë¼ ì¤‘ì‹¬ì„ ì¹´ë©”ë¼ ì¢Œí‘œê³„ë¡œ ë³€í™˜)
        t = -R @ T  # ì¹´ë©”ë¼ ì¤‘ì‹¬ì„ ì¹´ë©”ë¼ ì¢Œí‘œê³„ë¡œ ë³€í™˜
        RT = np.hstack([R, t.reshape(-1, 1)])
        P = K @ RT
        
        return P
    
    def _create_3dgs_scene_info(self, image_paths):
        """3DGSìš© SceneInfo ìƒì„± (ê°œì„ ëœ ë²„ì „)"""
        
        # Lazy import 3DGS modules
        CameraInfo, SceneInfo, BasicPointCloud = get_3dgs_imports()
        if CameraInfo is None:
            raise ImportError("3DGS modules not available")
        
        # CameraInfo ë¦¬ìŠ¤íŠ¸ ìƒì„±
        cam_infos = []
        
        # ğŸ”§ FALLBACK: self.camerasê°€ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ ì¹´ë©”ë¼ ìƒì„±
        if not self.cameras:
            print("    âš ï¸  No cameras estimated, creating fallback cameras...")
            for i, image_path in enumerate(image_paths):
                try:
                    # ì´ë¯¸ì§€ í¬ê¸° í™•ì¸
                    from PIL import Image
                    image = Image.open(image_path)
                    width, height = image.size
                    
                    # ì›í˜• ë°°ì¹˜ë¡œ ì¹´ë©”ë¼ ë°°ì¹˜
                    angle = i * (2 * np.pi / len(image_paths))
                    radius = 3.0
                    
                    # ì¹´ë©”ë¼ í¬ì¦ˆ (ì›ì„ ë°”ë¼ë³´ë„ë¡)
                    camera_pos = np.array([
                        radius * np.cos(angle),
                        0.0,  # ë†’ì´ ê³ ì •
                        radius * np.sin(angle)
                    ])
                    
                    # ì›ì ì„ í–¥í•˜ëŠ” ë°©í–¥
                    look_at = np.array([0.0, 0.0, 0.0])
                    up = np.array([0.0, 1.0, 0.0])
                    
                    # ì¹´ë©”ë¼ íšŒì „ í–‰ë ¬ ê³„ì‚°
                    forward = look_at - camera_pos
                    forward = forward / np.linalg.norm(forward)
                    right = np.cross(forward, up)
                    right = right / np.linalg.norm(right)
                    up = np.cross(right, forward)
                    
                    R = np.array([right, up, -forward]).T  # OpenCV ì»¨ë²¤ì…˜
                    T = camera_pos
                    
                    # FOV ê³„ì‚° (ë” ì•ˆì „í•œ ê°’ë“¤)
                    focal_length = max(width, height) * 0.8
                    FovX = 2 * np.arctan(width / (2 * focal_length))
                    FovY = 2 * np.arctan(height / (2 * focal_length))
                    
                    # í…ŒìŠ¤íŠ¸ ì¹´ë©”ë¼ ì„ íƒ (ë” ê· ë“±í•˜ê²Œ ë¶„ì‚°)
                    is_test = (i % 8 == 0)  # 8ê°œë§ˆë‹¤ 1ê°œì”© í…ŒìŠ¤íŠ¸
                    
                    cam_info = CameraInfo(
                        uid=i,
                        R=R.astype(np.float32),
                        T=T.astype(np.float32),
                        FovY=float(FovY),
                        FovX=float(FovX),
                        image_path=str(image_path),
                        image_name=Path(image_path).name,
                        width=int(width),
                        height=int(height),
                        depth_params=None,
                        depth_path="",
                        is_test=is_test
                    )
                    cam_infos.append(cam_info)
                    
                except Exception as e:
                    print(f"    Warning: Failed to process {image_path}: {e}")
                    continue
        else:
            # ê¸°ì¡´ ì¹´ë©”ë¼ ì •ë³´ ì‚¬ìš©
            for cam_id in sorted(self.cameras.keys()):
                cam = self.cameras[cam_id]
                image_path = self.image_features[cam_id]['image_path']
                h, w = self.image_features[cam_id]['image_size']
                
                # FoV ê³„ì‚°
                K = cam['K']
                focal_x, focal_y = K[0, 0], K[1, 1]
                FovX = 2 * np.arctan(w / (2 * focal_x))
                FovY = 2 * np.arctan(h / (2 * focal_y))
                
                # ë” ë‚˜ì€ í…ŒìŠ¤íŠ¸ ë¶„í•  (ì—°ê²°ì„± ê¸°ë°˜)
                is_test = self._should_be_test_camera(cam_id)
                
                cam_info = CameraInfo(
                    uid=cam_id,
                    R=cam['R'],
                    T=cam['T'],
                    FovY=float(FovY),
                    FovX=float(FovX),
                    image_path=image_path,
                    image_name=Path(image_path).name,
                    width=w,
                    height=h,
                    depth_params=None,
                    depth_path="",
                    is_test=is_test
                )
                cam_infos.append(cam_info)
        
        # í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ìƒì„±
        if self.points_3d and len(self.points_3d) > 0:
            points = np.array([pt['xyz'] for pt in self.points_3d.values()])
            colors = np.array([pt['color'] for pt in self.points_3d.values()])
            
            # ë²•ì„  ë²¡í„° (ê°œì„ ëœ ê³„ì‚°)
            normals = self._compute_point_normals(points)
            
            pcd = BasicPointCloud(points=points, colors=colors, normals=normals)
        else:
            # ğŸ”§ FALLBACK: ë” í˜„ì‹¤ì ì¸ ê¸°ë³¸ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ
            print("    âš ï¸  No 3D points available, creating fallback point cloud...")
            n_points = 25000  # 15000 â†’ 25000ë¡œ ì¦ê°€
            
            # ë” í˜„ì‹¤ì ì¸ 3D í¬ì¸íŠ¸ ë¶„í¬
            # êµ¬í˜• ë¶„í¬ + ì¼ë¶€ í‰ë©´ êµ¬ì¡°
            points_sphere = np.random.randn(n_points // 2, 3).astype(np.float32)
            points_sphere = points_sphere / np.linalg.norm(points_sphere, axis=1, keepdims=True) * 3.0  # 2.0 â†’ 3.0
            
            # í‰ë©´ êµ¬ì¡° ì¶”ê°€ (ë°”ë‹¥ë©´)
            points_plane = np.random.randn(n_points // 2, 3).astype(np.float32)
            points_plane[:, 1] = np.abs(points_plane[:, 1]) * 0.2 - 1.0  # ë°”ë‹¥ ê·¼ì²˜ (0.1 â†’ 0.2, -0.5 â†’ -1.0)
            points_plane[:, [0, 2]] *= 2.0  # 1.5 â†’ 2.0
            
            points = np.vstack([points_sphere, points_plane])
            
            # ë” í˜„ì‹¤ì ì¸ ìƒ‰ìƒ (íšŒìƒ‰ì¡° + ì•½ê°„ì˜ ìƒ‰ìƒ)
            colors = np.random.rand(n_points, 3).astype(np.float32)
            colors = colors * 0.5 + 0.3  # 0.3-0.8 ë²”ìœ„
            
            # ë²•ì„  ë²¡í„° (ë¬´ì‘ìœ„ì§€ë§Œ ì •ê·œí™”ë¨)
            normals = np.random.randn(n_points, 3).astype(np.float32)
            normals = normals / (np.linalg.norm(normals, axis=1, keepdims=True) + 1e-10)
            
            # BasicPointCloudê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ fallback
            try:
                pcd = BasicPointCloud(points=points, colors=colors, normals=normals)
            except NameError:
                # BasicPointCloudê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ê°„ë‹¨í•œ í´ë˜ìŠ¤ ìƒì„±
                class BasicPointCloud:
                    def __init__(self, points, colors, normals):
                        self.points = points
                        self.colors = colors
                        self.normals = normals
                
                pcd = BasicPointCloud(points=points, colors=colors, normals=normals)
        
        # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
        train_cams = [c for c in cam_infos if not c.is_test]
        test_cams = [c for c in cam_infos if c.is_test]
        
        # NeRF ì •ê·œí™”
        nerf_norm = self._compute_nerf_normalization(train_cams)
        
        return SceneInfo(
            point_cloud=pcd,
            train_cameras=train_cams,
            test_cameras=test_cams,
            nerf_normalization=nerf_norm,
            ply_path="",
            is_nerf_synthetic=False
        )
    
    def _should_be_test_camera(self, cam_id):
        """ë” ë‚˜ì€ í…ŒìŠ¤íŠ¸ ì¹´ë©”ë¼ ì„ íƒ"""
        # ì—°ê²°ì„±ì´ ë‚®ì€ ì¹´ë©”ë¼ë¥¼ í…ŒìŠ¤íŠ¸ë¡œ ì„ íƒ
        connectivity = len(self.camera_graph.get(cam_id, []))
        
        # ì—°ê²°ì„±ì´ 1 ì´í•˜ì´ê±°ë‚˜, íŠ¹ì • ê°„ê²©ìœ¼ë¡œ ì„ íƒ
        if connectivity <= 1:
            return True
        
        # 10ê°œë§ˆë‹¤ 1ê°œì”© í…ŒìŠ¤íŠ¸ë¡œ ì„ íƒ (ì—°ê²°ì„±ì´ ë†’ì€ ì¹´ë©”ë¼ë“¤ ì¤‘ì—ì„œ)
        if cam_id % 10 == 0 and connectivity >= 2:
            return True
        
        return False
    
    def _compute_point_normals(self, points):
        """í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ë²•ì„  ë²¡í„° ê³„ì‚°"""
        if len(points) < 3:
            return np.random.randn(len(points), 3).astype(np.float32)
        
        # ê°„ë‹¨í•œ ë²•ì„  ê³„ì‚° (sklearn ì˜ì¡´ì„± ì œê±°)
        try:
            normals = np.zeros_like(points)
            
            for i in range(len(points)):
                # í˜„ì¬ í¬ì¸íŠ¸
                current_point = points[i]
                
                # ë‹¤ë¥¸ ëª¨ë“  í¬ì¸íŠ¸ì™€ì˜ ê±°ë¦¬ ê³„ì‚°
                distances = np.linalg.norm(points - current_point, axis=1)
                
                # ê°€ì¥ ê°€ê¹Œìš´ 10ê°œ í¬ì¸íŠ¸ ì„ íƒ (ìê¸° ìì‹  ì œì™¸)
                nearest_indices = np.argsort(distances)[1:11]  # ìê¸° ìì‹  ì œì™¸
                
                if len(nearest_indices) < 3:
                    normals[i] = np.random.randn(3)
                    continue
                
                # ì´ì›ƒ í¬ì¸íŠ¸ë“¤ì˜ ì¤‘ì‹¬ ê³„ì‚°
                neighbors = points[nearest_indices]
                centroid = np.mean(neighbors, axis=0)
                
                # ê³µë¶„ì‚° í–‰ë ¬ ê³„ì‚°
                centered = neighbors - centroid
                cov_matrix = centered.T @ centered
                
                # ê°€ì¥ ì‘ì€ ê³ ìœ ê°’ì— í•´ë‹¹í•˜ëŠ” ê³ ìœ ë²¡í„°ê°€ ë²•ì„ 
                eigenvals, eigenvecs = np.linalg.eigh(cov_matrix)
                normal = eigenvecs[:, 0]  # ê°€ì¥ ì‘ì€ ê³ ìœ ê°’
                
                # ë°©í–¥ ì¼ê´€ì„± í™•ì¸
                if normal[2] < 0:
                    normal = -normal
                
                normals[i] = normal
            
            # ì •ê·œí™”
            norms = np.linalg.norm(normals, axis=1, keepdims=True)
            norms[norms == 0] = 1
            normals = normals / norms
            
        except Exception as e:
            print(f"    Warning: Normal computation failed: {e}")
            # ì‹¤íŒ¨ì‹œ ëœë¤ ë²•ì„ 
            normals = np.random.randn(len(points), 3).astype(np.float32)
            normals = normals / np.linalg.norm(normals, axis=1, keepdims=True)
        
        return normals.astype(np.float32)
    
    def _create_default_pointcloud(self):
        """ê¸°ë³¸ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ìƒì„± (ê°œì„ ëœ ë²„ì „)"""
        # Lazy import 3DGS modules
        _, _, BasicPointCloud = get_3dgs_imports()
        if BasicPointCloud is None:
            # Fallback: ê°„ë‹¨í•œ í´ë˜ìŠ¤ ì •ì˜
            class BasicPointCloud:
                def __init__(self, points, colors, normals):
                    self.points = points
                    self.colors = colors
                    self.normals = normals
        
        # ì¹´ë©”ë¼ ìœ„ì¹˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ë” í˜„ì‹¤ì ì¸ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ
        if len(self.cameras) > 0:
            # ì¹´ë©”ë¼ ì¤‘ì‹¬ ê³„ì‚°
            camera_centers = []
            for cam_id in self.cameras:
                R, T = self.cameras[cam_id]['R'], self.cameras[cam_id]['T']
                center = -R.T @ T
                camera_centers.append(center)
            
            if camera_centers:
                camera_centers = np.array(camera_centers)
                center_mean = np.mean(camera_centers, axis=0)
                center_std = np.std(camera_centers, axis=0)
                
                # ì‹¤ì œ í¬ì¸íŠ¸ê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±
                if self.points_3d:
                    actual_points = np.array([pt['xyz'] for pt in self.points_3d.values()])
                    if len(actual_points) > 0:
                        # ì‹¤ì œ í¬ì¸íŠ¸ ì£¼ë³€ì— ì¶”ê°€ í¬ì¸íŠ¸ ìƒì„±
                        n_additional = 15000  # ë” ë§ì€ ì¶”ê°€ í¬ì¸íŠ¸ (5000 â†’ 15000)
                        points = np.random.randn(n_additional, 3).astype(np.float32)
                        points = points * np.std(actual_points, axis=0) * 0.8 + np.mean(actual_points, axis=0)
                        
                        # ì‹¤ì œ í¬ì¸íŠ¸ì™€ í•©ì¹˜ê¸°
                        points = np.vstack([actual_points, points])
                        colors = np.random.rand(len(points), 3).astype(np.float32)
                        normals = self._compute_point_normals(points)
                    else:
                        # ì¹´ë©”ë¼ ë¶„í¬ë¥¼ ê³ ë ¤í•œ í¬ì¸íŠ¸ ìƒì„±
                        n_points = 20000  # ë” ë§ì€ ìˆ˜ (10000 â†’ 20000)
                        points = np.random.randn(n_points, 3).astype(np.float32)
                        points = points * center_std * 0.8 + center_mean
                        colors = np.random.rand(n_points, 3).astype(np.float32)
                        normals = self._compute_point_normals(points)
                else:
                    # ì¹´ë©”ë¼ ë¶„í¬ë¥¼ ê³ ë ¤í•œ í¬ì¸íŠ¸ ìƒì„±
                    n_points = 20000  # ë” ë§ì€ ìˆ˜ (10000 â†’ 20000)
                    points = np.random.randn(n_points, 3).astype(np.float32)
                    points = points * center_std * 0.8 + center_mean
                    colors = np.random.rand(n_points, 3).astype(np.float32)
                    normals = self._compute_point_normals(points)
            else:
                # ê¸°ë³¸ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ (ë” ì ì€ ìˆ˜)
                points = np.random.randn(10000, 3).astype(np.float32) * 3
                colors = np.random.rand(10000, 3).astype(np.float32)
                normals = np.random.randn(10000, 3).astype(np.float32)
                normals = normals / np.linalg.norm(normals, axis=1, keepdims=True)
        else:
            # ê¸°ë³¸ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ (ë” ì ì€ ìˆ˜)
            points = np.random.randn(10000, 3).astype(np.float32) * 3
            colors = np.random.rand(10000, 3).astype(np.float32)
            normals = np.random.randn(10000, 3).astype(np.float32)
            normals = normals / np.linalg.norm(normals, axis=1, keepdims=True)
        
        return BasicPointCloud(points=points, colors=colors, normals=normals)
    
    def _compute_nerf_normalization(self, cam_infos):
        """NeRF ì •ê·œí™” íŒŒë¼ë¯¸í„° ê³„ì‚° (ê°œì„ ëœ ë²„ì „)"""
        # Lazy import 3DGS modules
        try:
            from utils.graphics_utils import getWorld2View2
        except ImportError:
            # Fallback: ê°„ë‹¨í•œ í•¨ìˆ˜ ì •ì˜
            def getWorld2View2(R, t):
                Rt = np.zeros((4, 4))
                Rt[:3, :3] = R
                Rt[:3, 3] = t
                Rt[3, 3] = 1.0
                return Rt
        
        if not cam_infos:
            return {"translate": np.zeros(3), "radius": 1.0}
        
        # ì¹´ë©”ë¼ ì¤‘ì‹¬ ê³„ì‚°
        cam_centers = []
        for cam in cam_infos:
            W2C = getWorld2View2(cam.R, cam.T)
            C2W = np.linalg.inv(W2C)
            cam_centers.append(C2W[:3, 3:4])
        
        cam_centers = np.hstack(cam_centers)
        
        # ë” ì•ˆì •ì ì¸ ì¤‘ì‹¬ ê³„ì‚° (ì¤‘ê°„ê°’ ì‚¬ìš©)
        center = np.median(cam_centers, axis=1, keepdims=True).flatten()
        
        # ê±°ë¦¬ ê³„ì‚°
        distances = np.linalg.norm(cam_centers - center.reshape(-1, 1), axis=0)
        
        # ë” ë³´ìˆ˜ì ì¸ ë°˜ì§€ë¦„ ê³„ì‚° (95 í¼ì„¼íƒ€ì¼ ì‚¬ìš©)
        radius = np.percentile(distances, 95) * 1.2
        
        # ìµœì†Œ ë°˜ì§€ë¦„ ë³´ì¥
        radius = max(radius, 1.0)
        
        return {"translate": -center, "radius": radius}
    
    def _save_3dgs_format(self, scene_info, output_dir):
        """3DGS í•™ìŠµì„ ìœ„í•œ íŒŒì¼ êµ¬ì¡° ìƒì„±"""
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True, parents=True)
        
        # COLMAP í˜¸í™˜ ë””ë ‰í† ë¦¬ êµ¬ì¡°
        sparse_dir = output_dir / "sparse" / "0"
        sparse_dir.mkdir(exist_ok=True, parents=True)
        
        images_dir = output_dir / "images"
        images_dir.mkdir(exist_ok=True)
        
        # 1. ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„° ì €ì¥ (cameras.txt + cameras.bin)
        self._write_cameras_txt(scene_info.train_cameras + scene_info.test_cameras, 
                               sparse_dir / "cameras.txt")
        self._write_cameras_bin(scene_info.train_cameras + scene_info.test_cameras, 
                               sparse_dir / "cameras.bin")
        
        # 2. ì¹´ë©”ë¼ í¬ì¦ˆ ì €ì¥ (images.txt + images.bin)
        self._write_images_txt(scene_info.train_cameras + scene_info.test_cameras, 
                              sparse_dir / "images.txt")
        self._write_images_bin(scene_info.train_cameras + scene_info.test_cameras, 
                              sparse_dir / "images.bin")
        
        # 3. 3D í¬ì¸íŠ¸ ì €ì¥ (points3D.ply + points3D.bin)
        self._write_points3d_ply(scene_info.point_cloud, sparse_dir / "points3D.ply")
        self._write_points3d_bin(scene_info.point_cloud, sparse_dir / "points3D.bin")
        
        # 4. ì´ë¯¸ì§€ ë³µì‚¬ ë˜ëŠ” ì‹¬ë³¼ë¦­ ë§í¬
        self._setup_images_directory(scene_info.train_cameras + scene_info.test_cameras, 
                                    images_dir)
        
        print(f"  3DGS-compatible files saved to {output_dir}")
        print(f"  Use: python train.py -s {output_dir} -m {output_dir}/3dgs_output")
    
    def _write_cameras_txt(self, cam_infos, output_path):
        """COLMAP í˜•ì‹ cameras.txt ìƒì„±"""
        with open(output_path, 'w') as f:
            f.write("# Camera list with one line of data per camera:\n")
            f.write("# CAMERA_ID, MODEL, WIDTH, HEIGHT, PARAMS[]\n")
            f.write(f"# Number of cameras: {len(cam_infos)}\n")
            
            for cam in cam_infos:
                # PINHOLE ëª¨ë¸ ì‚¬ìš©
                focal_x = cam.width / (2 * np.tan(cam.FovX / 2))
                focal_y = cam.height / (2 * np.tan(cam.FovY / 2))
                cx, cy = cam.width / 2, cam.height / 2
                
                f.write(f"{cam.uid} PINHOLE {cam.width} {cam.height} "
                       f"{focal_x:.6f} {focal_y:.6f} {cx:.6f} {cy:.6f}\n")
    
    def _write_cameras_bin(self, cam_infos, output_path):
        """COLMAP í˜•ì‹ cameras.bin ìƒì„±"""
        try:
            from scene.colmap_loader import write_intrinsics_binary
            cameras = {}
            
            for cam in cam_infos:
                # PINHOLE ëª¨ë¸ ì‚¬ìš©
                focal_x = cam.width / (2 * np.tan(cam.FovX / 2))
                focal_y = cam.height / (2 * np.tan(cam.FovY / 2))
                cx, cy = cam.width / 2, cam.height / 2
                
                # COLMAP Camera ê°ì²´ ìƒì„±
                from scene.colmap_loader import Camera
                camera = Camera(
                    id=cam.uid,
                    model="PINHOLE",
                    width=cam.width,
                    height=cam.height,
                    params=[focal_x, focal_y, cx, cy]
                )
                cameras[cam.uid] = camera
            
            write_intrinsics_binary(cameras, str(output_path))
            print(f"    Created cameras.bin with {len(cameras)} cameras")
            
        except ImportError:
            print(f"    Warning: COLMAP loader not available, creating simple cameras.bin")
            self._write_cameras_bin_simple(cam_infos, output_path)
        except Exception as e:
            print(f"    Warning: Failed to create cameras.bin: {e}")
            self._write_cameras_bin_simple(cam_infos, output_path)
    
    def _write_cameras_bin_simple(self, cam_infos, output_path):
        """ê°„ë‹¨í•œ cameras.bin ìƒì„± (COLMAP loader ì—†ì´)"""
        try:
            import struct
            
            with open(output_path, 'wb') as f:
                # ì¹´ë©”ë¼ ìˆ˜
                f.write(struct.pack('<Q', len(cam_infos)))
                
                for cam in cam_infos:
                    # PINHOLE ëª¨ë¸ ì‚¬ìš©
                    focal_x = cam.width / (2 * np.tan(cam.FovX / 2))
                    focal_y = cam.height / (2 * np.tan(cam.FovY / 2))
                    cx, cy = cam.width / 2, cam.height / 2
                    
                    # ì¹´ë©”ë¼ ID (int)
                    f.write(struct.pack('<i', cam.uid))
                    
                    # ëª¨ë¸ ID (PINHOLE = 1)
                    model_id = 1
                    f.write(struct.pack('<i', model_id))
                    
                    # ë„ˆë¹„, ë†’ì´ (unsigned long long)
                    f.write(struct.pack('<Q', cam.width))
                    f.write(struct.pack('<Q', cam.height))
                    
                    # íŒŒë¼ë¯¸í„°ë“¤ (double)
                    params = [focal_x, focal_y, cx, cy]
                    for param in params:
                        f.write(struct.pack('<d', param))
            
            print(f"    Created simple cameras.bin with {len(cam_infos)} cameras")
            
        except Exception as e:
            print(f"    Error creating simple cameras.bin: {e}")
            import traceback
            traceback.print_exc()
    
    def _write_images_bin(self, cam_infos, output_path):
        """COLMAP í˜•ì‹ images.bin ìƒì„±"""
        try:
            from scene.colmap_loader import write_extrinsics_binary
            images = {}
            
            for cam in cam_infos:
                # íšŒì „ í–‰ë ¬ì„ ì¿¼í„°ë‹ˆì–¸ìœ¼ë¡œ ë³€í™˜
                R = cam.R
                trace = np.trace(R)
                
                if trace > 0:
                    s = np.sqrt(trace + 1.0) * 2
                    qw = 0.25 * s
                    qx = (R[2, 1] - R[1, 2]) / s
                    qy = (R[0, 2] - R[2, 0]) / s
                    qz = (R[1, 0] - R[0, 1]) / s
                else:
                    qw = np.sqrt(1 + R[0,0] + R[1,1] + R[2,2]) / 2
                    qx = (R[2,1] - R[1,2]) / (4 * qw) if qw != 0 else 0
                    qy = (R[0,2] - R[2,0]) / (4 * qw) if qw != 0 else 0
                    qz = (R[1,0] - R[0,1]) / (4 * qw) if qw != 0 else 0
                
                # ì •ê·œí™”
                q_norm = np.sqrt(qw*qw + qx*qx + qy*qy + qz*qz)
                if q_norm > 0:
                    qw, qx, qy, qz = qw/q_norm, qx/q_norm, qy/q_norm, qz/q_norm
                
                # COLMAP Image ê°ì²´ ìƒì„±
                from scene.colmap_loader import Image
                image = Image(
                    id=cam.uid,
                    qvec=np.array([qw, qx, qy, qz]),
                    tvec=cam.T,
                    camera_id=cam.uid,
                    name=cam.image_name,
                    xys=np.array([]),  # ë¹ˆ íŠ¹ì§•ì  ë°°ì—´
                    point3D_ids=np.array([])  # ë¹ˆ 3D í¬ì¸íŠ¸ ID ë°°ì—´
                )
                images[cam.uid] = image
            
            write_extrinsics_binary(images, str(output_path))
            print(f"    Created images.bin with {len(images)} images")
            
        except ImportError:
            print(f"    Warning: COLMAP loader not available, creating simple images.bin")
            self._write_images_bin_simple(cam_infos, output_path)
        except Exception as e:
            print(f"    Warning: Failed to create images.bin: {e}")
            self._write_images_bin_simple(cam_infos, output_path)
    
    def _write_images_bin_simple(self, cam_infos, output_path):
        """ê°„ë‹¨í•œ images.bin ìƒì„± (COLMAP loader ì—†ì´)"""
        try:
            import struct
            
            with open(output_path, 'wb') as f:
                # ì´ë¯¸ì§€ ìˆ˜
                f.write(struct.pack('<Q', len(cam_infos)))
                
                for cam in cam_infos:
                    # íšŒì „ í–‰ë ¬ì„ ì¿¼í„°ë‹ˆì–¸ìœ¼ë¡œ ë³€í™˜
                    R = cam.R
                    trace = np.trace(R)
                    
                    if trace > 0:
                        s = np.sqrt(trace + 1.0) * 2
                        qw = 0.25 * s
                        qx = (R[2, 1] - R[1, 2]) / s
                        qy = (R[0, 2] - R[2, 0]) / s
                        qz = (R[1, 0] - R[0, 1]) / s
                    else:
                        qw = np.sqrt(1 + R[0,0] + R[1,1] + R[2,2]) / 2
                        qx = (R[2,1] - R[1,2]) / (4 * qw) if qw != 0 else 0
                        qy = (R[0,2] - R[2,0]) / (4 * qw) if qw != 0 else 0
                        qz = (R[1,0] - R[0,1]) / (4 * qw) if qw != 0 else 0
                    
                    # ì •ê·œí™”
                    q_norm = np.sqrt(qw*qw + qx*qx + qy*qy + qz*qz)
                    if q_norm > 0:
                        qw, qx, qy, qz = qw/q_norm, qx/q_norm, qy/q_norm, qz/q_norm
                    
                    # ì´ë¯¸ì§€ ID
                    f.write(struct.pack('<Q', cam.uid))
                    
                    # ì¿¼í„°ë‹ˆì–¸ (qw, qx, qy, qz)
                    f.write(struct.pack('<dddd', qw, qx, qy, qz))
                    
                    # ì´ë™ ë²¡í„° (tx, ty, tz)
                    f.write(struct.pack('<ddd', cam.T[0], cam.T[1], cam.T[2]))
                    
                    # ì¹´ë©”ë¼ ID
                    f.write(struct.pack('<Q', cam.uid))
                    
                    # ì´ë¯¸ì§€ ì´ë¦„ ê¸¸ì´ì™€ ì´ë¦„
                    name_bytes = cam.image_name.encode('utf-8')
                    f.write(struct.pack('<Q', len(name_bytes)))
                    f.write(name_bytes)
                    
                    # íŠ¹ì§•ì  ìˆ˜ (0ê°œ)
                    f.write(struct.pack('<Q', 0))
                    
                    # 3D í¬ì¸íŠ¸ ID ìˆ˜ (0ê°œ)
                    f.write(struct.pack('<Q', 0))
            
            print(f"    Created simple images.bin with {len(cam_infos)} images")
            
        except Exception as e:
            print(f"    Error creating simple images.bin: {e}")
    
    def _write_points3d_bin(self, point_cloud, output_path):
        """COLMAP í˜•ì‹ points3D.bin ìƒì„±"""
        try:
            from scene.colmap_loader import write_points3D_binary
            points3d = {}
            
            points = point_cloud.points
            colors = point_cloud.colors
            
            for i in range(len(points)):
                # COLMAP Point3D ê°ì²´ ìƒì„±
                from scene.colmap_loader import Point3D
                point3d = Point3D(
                    id=i,
                    xyz=points[i],
                    rgb=colors[i] * 255,  # 0-1 ë²”ìœ„ë¥¼ 0-255ë¡œ ë³€í™˜
                    error=0.0,  # ê¸°ë³¸ ì˜¤ì°¨
                    track=[]  # ë¹ˆ íŠ¸ë™ (ê´€ì°° ì •ë³´ ì—†ìŒ)
                )
                points3d[i] = point3d
            
            write_points3D_binary(points3d, str(output_path))
            print(f"    Created points3D.bin with {len(points3d)} points")
            
        except ImportError:
            print(f"    Warning: COLMAP loader not available, creating simple points3D.bin")
            self._write_points3d_bin_simple(point_cloud, output_path)
        except Exception as e:
            print(f"    Warning: Failed to create points3D.bin: {e}")
            self._write_points3d_bin_simple(point_cloud, output_path)
    
    def _write_points3d_bin_simple(self, point_cloud, output_path):
        """ê°„ë‹¨í•œ points3D.bin ìƒì„± (COLMAP loader ì—†ì´)"""
        try:
            import struct
            
            points = point_cloud.points
            colors = point_cloud.colors
            
            with open(output_path, 'wb') as f:
                # í¬ì¸íŠ¸ ìˆ˜
                f.write(struct.pack('<Q', len(points)))
                
                for i in range(len(points)):
                    # í¬ì¸íŠ¸ ID
                    f.write(struct.pack('<Q', i))
                    
                    # 3D ì¢Œí‘œ (x, y, z)
                    f.write(struct.pack('<ddd', points[i][0], points[i][1], points[i][2]))
                    
                    # RGB ìƒ‰ìƒ (0-255 ë²”ìœ„ë¡œ ë³€í™˜)
                    rgb = (colors[i] * 255).astype(np.uint8)
                    f.write(struct.pack('<BBB', rgb[0], rgb[1], rgb[2]))
                    
                    # ì˜¤ì°¨ (0.0)
                    f.write(struct.pack('<d', 0.0))
                    
                    # íŠ¸ë™ ê¸¸ì´ (0ê°œ)
                    f.write(struct.pack('<Q', 0))
            
            print(f"    Created simple points3D.bin with {len(points)} points")
            
        except Exception as e:
            print(f"    Error creating simple points3D.bin: {e}")
    
    def _write_images_txt(self, cam_infos, output_path):
        """COLMAP í˜•ì‹ images.txt ìƒì„±"""
        with open(output_path, 'w') as f:
            f.write("# Image list with two lines of data per image:\n")
            f.write("# IMAGE_ID, QW, QX, QY, QZ, TX, TY, TZ, CAMERA_ID, NAME\n")
            f.write("# POINTS2D[] as (X, Y, POINT3D_ID)\n")
            f.write(f"# Number of images: {len(cam_infos)}\n")
            
            for cam in cam_infos:
                # íšŒì „ í–‰ë ¬ì„ ì¿¼í„°ë‹ˆì–¸ìœ¼ë¡œ ë³€í™˜
                R = cam.R
                trace = np.trace(R)
                
                if trace > 0:
                    s = np.sqrt(trace + 1.0) * 2  # s = 4 * qw
                    qw = 0.25 * s
                    qx = (R[2, 1] - R[1, 2]) / s
                    qy = (R[0, 2] - R[2, 0]) / s
                    qz = (R[1, 0] - R[0, 1]) / s
                else:
                    # ì•ˆì •ì ì¸ ì¿¼í„°ë‹ˆì–¸ ë³€í™˜
                    qw = np.sqrt(1 + R[0,0] + R[1,1] + R[2,2]) / 2
                    qx = (R[2,1] - R[1,2]) / (4 * qw) if qw != 0 else 0
                    qy = (R[0,2] - R[2,0]) / (4 * qw) if qw != 0 else 0
                    qz = (R[1,0] - R[0,1]) / (4 * qw) if qw != 0 else 0
                
                # ì •ê·œí™”
                q_norm = np.sqrt(qw*qw + qx*qx + qy*qy + qz*qz)
                if q_norm > 0:
                    qw, qx, qy, qz = qw/q_norm, qx/q_norm, qy/q_norm, qz/q_norm
                
                f.write(f"{cam.uid} {qw:.6f} {qx:.6f} {qy:.6f} {qz:.6f} "
                       f"{cam.T[0]:.6f} {cam.T[1]:.6f} {cam.T[2]:.6f} "
                       f"{cam.uid} {cam.image_name}\n")
                f.write("\n")  # ë¹ˆ íŠ¹ì§•ì  ë¼ì¸
    
    def _write_points3d_ply(self, point_cloud, output_path):
        """PLY í˜•ì‹ìœ¼ë¡œ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ì €ì¥"""
        points = point_cloud.points
        colors = (point_cloud.colors * 255).astype(np.uint8)
        
        with open(output_path, 'w') as f:
            # PLY í—¤ë”
            f.write("ply\n")
            f.write("format ascii 1.0\n")
            f.write(f"element vertex {len(points)}\n")
            f.write("property float x\n")
            f.write("property float y\n")
            f.write("property float z\n")
            f.write("property float nx\n")
            f.write("property float ny\n")
            f.write("property float nz\n")
            f.write("property uchar red\n")
            f.write("property uchar green\n")
            f.write("property uchar blue\n")
            f.write("end_header\n")
            
            # ë°ì´í„°
            for i in range(len(points)):
                x, y, z = points[i]
                nx, ny, nz = point_cloud.normals[i]
                r, g, b = colors[i]
                f.write(f"{x:.6f} {y:.6f} {z:.6f} "
                       f"{nx:.6f} {ny:.6f} {nz:.6f} "
                       f"{r} {g} {b}\n")
    
    def _setup_images_directory(self, cam_infos, images_dir):
        """ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ì„¤ì • (ì‹¬ë³¼ë¦­ ë§í¬ ë˜ëŠ” ë³µì‚¬)"""
        import shutil
        
        for cam in cam_infos:
            src_path = Path(cam.image_path)
            dst_path = images_dir / cam.image_name
            
            if not dst_path.exists():
                try:
                    # ì‹¬ë³¼ë¦­ ë§í¬ ì‹œë„
                    dst_path.symlink_to(src_path.resolve())
                except (OSError, NotImplementedError):
                    # ì‹¤íŒ¨ì‹œ ë³µì‚¬
                    shutil.copy2(src_path, dst_path)
    
    def _calculate_adaptive_resize(self, image_path, max_dim=1600):
        """SuperGlue ê¶Œì¥ í•´ìƒë„(ìµœëŒ€ max_dim)ë¡œ ë¹„ìœ¨ ìœ ì§€ ë¦¬ì‚¬ì´ì¦ˆ ê³„ì‚°"""
        try:
            img = cv2.imread(str(image_path))
            if img is None:
                return [1024, 768]  # ê¸°ë³¸ê°’
            h, w = img.shape[:2]
            largest = max(h, w)
            if largest <= max_dim:
                return None  # ì›ë³¸ í¬ê¸° ìœ ì§€
            scale = max_dim / largest
            return [int(w * scale), int(h * scale)]
        except:
            return [1024, 768]  # ê¸°ë³¸ê°’

    def _load_image(self, image_path, resize=None):
        """ì´ë¯¸ì§€ ë¡œë“œ ë° SuperGlue ê¶Œì¥ í•´ìƒë„ ì ìš©"""
        try:
            image = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)
            if image is None:
                print(f"    Warning: Failed to load {image_path}")
                return None

            # SuperGlue ê¶Œì¥ í•´ìƒë„ ì ìš© (ìµœëŒ€ 1600px)
            if resize is None:
                resize = self._calculate_adaptive_resize(image_path, max_dim=1600)
            if resize is not None:
                image = cv2.resize(image, tuple(resize))
            return image.astype(np.float32)
        except Exception as e:
            print(f"    Error loading {image_path}: {e}")
            return None
    
    def _pack_parameters(self):
        """ì¹´ë©”ë¼ í¬ì¦ˆì™€ 3D í¬ì¸íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ íŒ¨í‚¹"""
        params = []
        
        # ì¹´ë©”ë¼ í¬ì¦ˆ (íšŒì „ + ì´ë™)
        for cam_id in sorted(self.cameras.keys()):
            cam = self.cameras[cam_id]
            R = cam['R']
            T = cam['T']
            
            # íšŒì „ í–‰ë ¬ì„ ë¡œë“œë¦¬ê²ŒìŠ¤ ë²¡í„°ë¡œ ë³€í™˜
            angle_axis = self._rotation_matrix_to_angle_axis(R)
            params.extend(angle_axis)
            params.extend(T)
        
        # 3D í¬ì¸íŠ¸
        for point_id in sorted(self.points_3d.keys()):
            point = self.points_3d[point_id]['xyz']
            params.extend(point)
        
        params = np.array(params)
        
        # NaNì´ë‚˜ ë¬´í•œëŒ€ ê°’ ì²´í¬
        if np.any(np.isnan(params)) or np.any(np.isinf(params)):
            raise ValueError("Invalid parameters detected (NaN or Inf)")
        
        return params
    
    def _unpack_parameters(self, params):
        """ë²¡í„°ì—ì„œ ì¹´ë©”ë¼ í¬ì¦ˆì™€ 3D í¬ì¸íŠ¸ ì–¸íŒ¨í‚¹"""
        idx = 0
        
        # ì¹´ë©”ë¼ í¬ì¦ˆ ë³µì›
        for cam_id in sorted(self.cameras.keys()):
            # ë¡œë“œë¦¬ê²ŒìŠ¤ ë²¡í„° (3ê°œ)
            angle_axis = params[idx:idx+3]
            idx += 3
            
            # ì´ë™ ë²¡í„° (3ê°œ)
            T = params[idx:idx+3]
            idx += 3
            
            # íšŒì „ í–‰ë ¬ë¡œ ë³€í™˜
            R = self._angle_axis_to_rotation_matrix(angle_axis)
            
            self.cameras[cam_id]['R'] = R.astype(np.float32)
            self.cameras[cam_id]['T'] = T.astype(np.float32)
        
        # 3D í¬ì¸íŠ¸ ë³µì›
        for point_id in sorted(self.points_3d.keys()):
            xyz = params[idx:idx+3]
            idx += 3
            self.points_3d[point_id]['xyz'] = xyz.astype(np.float32)

    def _build_camera_graph_from_matches(self):
        """ë§¤ì¹­ ê²°ê³¼ë¡œë¶€í„° ì¹´ë©”ë¼ ê·¸ë˜í”„ êµ¬ì¶•"""
        for (cam_i, cam_j), matches in self.matches.items():
            if cam_i not in self.camera_graph:
                self.camera_graph[cam_i] = []
            if cam_j not in self.camera_graph:
                self.camera_graph[cam_j] = []
            if cam_i != cam_j:
                self.camera_graph[cam_i].append(cam_j)
                self.camera_graph[cam_j].append(cam_i)

    def _setup_point_observations_from_triangulation(self, triangulated_points):
        """Triangulation ê²°ê³¼ë¡œë¶€í„° Point observations ì„¤ì •"""
        for point_id, point_data in triangulated_points.items():
            if point_id not in self.point_observations:
                self.point_observations[point_id] = []
            for cam_id, observed_pt, confidence in point_data['observations']:
                self.point_observations[point_id].append((cam_id, observed_pt, confidence))

    def _estimate_camera_poses_improved(self):
        """ê°œì„ ëœ ì¹´ë©”ë¼ í¬ì¦ˆ ì¶”ì • - ê¸€ë¡œë²Œ ìµœì í™” ë° ë” ë‚˜ì€ ê²€ì¦"""
        
        print(f"  Starting improved camera pose estimation...")
        
        # 1. ê°€ì¥ ê°•í•œ ë§¤ì¹­ ìŒì„ ì°¾ì•„ ê¸°ì¤€ ì¹´ë©”ë¼ ê²°ì •
        best_pair = self._find_best_camera_pair()
        if best_pair is None:
            print("    No suitable camera pair found, using default poses")
            self._create_default_camera_poses()
            return
        
        cam_i, cam_j = best_pair
        print(f"    Using cameras {cam_i} and {cam_j} as reference pair")
        
        # 2. ê¸°ì¤€ ì¹´ë©”ë¼ ìŒ ì´ˆê¸°í™”
        self._initialize_reference_cameras(cam_i, cam_j)
        
        # 3. ì ì§„ì  ì¹´ë©”ë¼ ë“±ë¡ (Incremental SfM)
        self._incremental_camera_registration()
        
        # 4. ê¸€ë¡œë²Œ í¬ì¦ˆ ìµœì í™”
        self._global_pose_optimization()
        
        # 5. í¬ì¦ˆ í’ˆì§ˆ ê²€ì¦ ë° ë³´ì •
        self._validate_and_correct_poses()
        
        print(f"    Improved pose estimation completed for {len(self.cameras)} cameras")
    
    def _find_best_camera_pair(self):
        """ê°€ì¥ ê°•í•œ ë§¤ì¹­ì„ ê°€ì§„ ì¹´ë©”ë¼ ìŒ ì°¾ê¸°"""
        best_pair = None
        best_score = 0
        
        for (cam_i, cam_j), matches in self.matches.items():
            if len(matches) < 20:  # ìµœì†Œ ë§¤ì¹­ ìˆ˜
                continue
            
            # ë§¤ì¹­ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            confidences = [conf for _, _, conf in matches]
            avg_confidence = np.mean(confidences)
            match_score = len(matches) * avg_confidence
            
            if match_score > best_score:
                best_score = match_score
                best_pair = (cam_i, cam_j)
        
        return best_pair
    
    def _initialize_reference_cameras(self, cam_i, cam_j):
        """ê¸°ì¤€ ì¹´ë©”ë¼ ìŒ ì´ˆê¸°í™”"""
        # ì²« ë²ˆì§¸ ì¹´ë©”ë¼ë¥¼ ì›ì ìœ¼ë¡œ ì„¤ì •
        self.cameras[cam_i] = {
            'R': np.eye(3, dtype=np.float32),
            'T': np.zeros(3, dtype=np.float32),
            'K': self._estimate_intrinsics(cam_i)
        }
        
        # ë‘ ë²ˆì§¸ ì¹´ë©”ë¼ í¬ì¦ˆ ì¶”ì •
        pair_key = (cam_i, cam_j) if cam_i < cam_j else (cam_j, cam_i)
        R_rel, T_rel = self._estimate_relative_pose_strict(cam_i, cam_j, pair_key)
        
        if R_rel is not None and T_rel is not None:
            self.cameras[cam_j] = {
                'R': R_rel.astype(np.float32),
                'T': T_rel.astype(np.float32),
                'K': self._estimate_intrinsics(cam_j)
            }
            print(f"    Initialized reference pair: cameras {cam_i} and {cam_j}")
        else:
            print(f"    Failed to initialize reference pair")
    
    def _estimate_relative_pose_strict(self, cam_i, cam_j, pair_key):
        """ë” ì—„ê²©í•œ ìƒëŒ€ í¬ì¦ˆ ì¶”ì •"""
        matches = self.matches[pair_key]
        
        if len(matches) < 15:  # ë” ë†’ì€ ìµœì†Œ ë§¤ì¹­ ìˆ˜
            return None, None
        
        # ê³ í’ˆì§ˆ ë§¤ì¹­ë§Œ ì„ íƒ
        high_conf_matches = [(idx_i, idx_j, conf) for idx_i, idx_j, conf in matches if conf > 0.3]
        
        if len(high_conf_matches) < 10:
            return None, None
        
        kpts_i = self.image_features[cam_i]['keypoints']
        kpts_j = self.image_features[cam_j]['keypoints']
        
        # ë§¤ì¹­ì ë“¤ ì¶”ì¶œ
        pts_i = np.array([kpts_i[idx_i] for idx_i, _, _ in high_conf_matches])
        pts_j = np.array([kpts_j[idx_j] for _, idx_j, _ in high_conf_matches])
        
        # ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„°
        K_i = self.cameras.get(cam_i, {}).get('K', self._estimate_intrinsics(cam_i))
        K_j = self._estimate_intrinsics(cam_j)
        
        # ì—„ê²©í•œ Essential Matrix ì¶”ì •
        methods = [
            (cv2.RANSAC, 0.5, 0.999),
            (cv2.RANSAC, 1.0, 0.999),
            (cv2.RANSAC, 2.0, 0.99),
            (cv2.LMEDS, 0.5, 0.99)
        ]
        
        best_R, best_T = None, None
        best_inliers = 0
        best_quality = 0
        
        for method, threshold, confidence in methods:
            try:
                E, mask = cv2.findEssentialMat(
                    pts_i, pts_j, K_i,
                    method=method,
                    prob=confidence,
                    threshold=threshold,
                    maxIters=2000
                )
                
                if E is None or E.shape != (3, 3):
                    continue
                
                # í¬ì¦ˆ ë³µì›
                _, R, T, mask = cv2.recoverPose(E, pts_i, pts_j, K_i, mask=mask)
                
                if R is None or T is None:
                    continue
                
                inliers = np.sum(mask)
                
                if inliers >= 8:  # ë” ì—„ê²©í•œ ìµœì†Œ inlier ìˆ˜
                    # ì—„ê²©í•œ í¬ì¦ˆ í’ˆì§ˆ ê²€ì¦
                    quality_score = self._evaluate_pose_quality_strict(pts_i, pts_j, R, T.flatten(), K_i, K_j, mask)
                    
                    if quality_score > best_quality:
                        best_R, best_T = R, T.flatten()
                        best_inliers = inliers
                        best_quality = quality_score
            except Exception as e:
                continue
        
        if best_R is not None:
            print(f"    Strict pose estimation: {best_inliers} inliers, quality: {best_quality:.3f}")
        
        return best_R, best_T
    
    def _evaluate_pose_quality_strict(self, pts_i, pts_j, R, T, K_i, K_j, mask):
        """ì—„ê²©í•œ í¬ì¦ˆ í’ˆì§ˆ í‰ê°€"""
        try:
            # íšŒì „ í–‰ë ¬ ìœ íš¨ì„± í™•ì¸
            det = np.linalg.det(R)
            if abs(det - 1.0) > 0.1:
                return 0.0
            
            # ì‚¼ê°ì¸¡ëŸ‰ í’ˆì§ˆ ê²€ì‚¬
            P_i = K_i @ np.hstack([np.eye(3), np.zeros((3, 1))])
            P_j = K_j @ np.hstack([R, T.reshape(-1, 1)])
            
            # inlier í¬ì¸íŠ¸ë“¤ë§Œ ì‚¬ìš©
            inlier_pts_i = pts_i[mask.flatten()]
            inlier_pts_j = pts_j[mask.flatten()]
            
            if len(inlier_pts_i) < 8:
                return 0.0
            
            # ì‚¼ê°ì¸¡ëŸ‰ í…ŒìŠ¤íŠ¸
            valid_points = 0
            total_error = 0.0
            
            for pt_i, pt_j in zip(inlier_pts_i, inlier_pts_j):
                try:
                    # ì‚¼ê°ì¸¡ëŸ‰
                    pt_4d = cv2.triangulatePoints(P_i, P_j, pt_i.reshape(2, 1), pt_j.reshape(2, 1))
                    
                    if abs(pt_4d[3, 0]) > 1e-10:
                        pt_3d = (pt_4d[:3] / pt_4d[3]).flatten()
                        
                        # ê±°ë¦¬ ì²´í¬
                        if 0.1 < np.linalg.norm(pt_3d) < 50:
                            # ì¬íˆ¬ì˜ ì˜¤ì°¨ ê³„ì‚°
                            proj_i = P_i @ np.append(pt_3d, 1)
                            proj_j = P_j @ np.append(pt_3d, 1)
                            
                            if proj_i[2] > 0 and proj_j[2] > 0:
                                proj_i_2d = proj_i[:2] / proj_i[2]
                                proj_j_2d = proj_j[:2] / proj_j[2]
                                
                                error_i = np.linalg.norm(proj_i_2d - pt_i)
                                error_j = np.linalg.norm(proj_j_2d - pt_j)
                                
                                max_error = max(error_i, error_j)
                                if max_error < 3.0:  # ì—„ê²©í•œ ì¬íˆ¬ì˜ ì˜¤ì°¨ ì„ê³„ê°’
                                    total_error += max_error
                                    valid_points += 1
                except:
                    continue
            
            if valid_points < 6:
                return 0.0
            
            # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            avg_error = total_error / valid_points
            inlier_ratio = len(inlier_pts_i) / len(pts_i)
            
            # ë” ì—„ê²©í•œ ì ìˆ˜ ê³„ì‚°
            quality_score = inlier_ratio * (1.0 / (1.0 + avg_error * 0.1))
            
            return quality_score
            
        except Exception as e:
            return 0.0
    
    def _incremental_camera_registration(self):
        """ì ì§„ì  ì¹´ë©”ë¼ ë“±ë¡"""
        estimated_cameras = set(self.cameras.keys())
        
        # ì—°ê²°ëœ ì¹´ë©”ë¼ë“¤ì„ ì ì§„ì ìœ¼ë¡œ ë“±ë¡
        changed = True
        while changed:
            changed = False
            
            for cam_id in range(len(self.image_features)):
                if cam_id in estimated_cameras:
                    continue
                
                # ì´ë¯¸ ë“±ë¡ëœ ì¹´ë©”ë¼ì™€ ì—°ê²°ëœ ì¹´ë©”ë¼ ì°¾ê¸°
                for registered_cam in estimated_cameras:
                    pair_key = (min(cam_id, registered_cam), max(cam_id, registered_cam))
                    
                    if pair_key in self.matches:
                        # ìƒëŒ€ í¬ì¦ˆ ì¶”ì •
                        R_rel, T_rel = self._estimate_relative_pose_strict(registered_cam, cam_id, pair_key)
                        
                        if R_rel is not None and T_rel is not None:
                            # ì ˆëŒ€ í¬ì¦ˆ ê³„ì‚°
                            R_ref, T_ref = self.cameras[registered_cam]['R'], self.cameras[registered_cam]['T']
                            
                            R_world = R_rel @ R_ref
                            T_world = R_rel @ T_ref + T_rel
                            
                            if self._is_valid_rotation_matrix(R_world):
                                self.cameras[cam_id] = {
                                    'R': R_world.astype(np.float32),
                                    'T': T_world.astype(np.float32),
                                    'K': self._estimate_intrinsics(cam_id)
                                }
                                
                                estimated_cameras.add(cam_id)
                                changed = True
                                print(f"    Registered camera {cam_id} from {registered_cam}")
                                break
    
    def _global_pose_optimization(self):
        """ê¸€ë¡œë²Œ í¬ì¦ˆ ìµœì í™”"""
        print("    Performing global pose optimization...")
        
        # ê°„ë‹¨í•œ Bundle Adjustment ìˆ˜í–‰
        if len(self.cameras) >= 3 and len(self.matches) >= 10:
            try:
                # ì¹´ë©”ë¼ í¬ì¦ˆë§Œ ìµœì í™” (í¬ì¸íŠ¸ëŠ” ì œì™¸)
                self._optimize_camera_poses_only()
                print("    Global pose optimization completed")
            except Exception as e:
                print(f"    Global pose optimization failed: {e}")
    
    def _optimize_camera_poses_only(self):
        """ì¹´ë©”ë¼ í¬ì¦ˆë§Œ ìµœì í™”"""
        from scipy.optimize import least_squares
        
        # ì¹´ë©”ë¼ í¬ì¦ˆë¥¼ ë²¡í„°ë¡œ íŒ¨í‚¹
        initial_poses = []
        cam_ids = sorted(self.cameras.keys())
        
        for cam_id in cam_ids[1:]:  # ì²« ë²ˆì§¸ ì¹´ë©”ë¼ëŠ” ê³ ì •
            cam = self.cameras[cam_id]
            # íšŒì „ í–‰ë ¬ì„ ë¡œë“œë¦¬ê²ŒìŠ¤ ë²¡í„°ë¡œ ë³€í™˜
            rvec = self._rotation_matrix_to_angle_axis(cam['R'])
            initial_poses.extend(rvec)
            initial_poses.extend(cam['T'])
        
        if len(initial_poses) == 0:
            return
        
        # ìµœì í™” ì‹¤í–‰
        try:
            result = least_squares(
                self._pose_residuals,
                initial_poses,
                args=(cam_ids,),
                method='lm',
                max_nfev=100,
                ftol=1e-4,
                xtol=1e-4
            )
            
            # ê²°ê³¼ ì–¸íŒ¨í‚¹
            self._unpack_camera_poses(result.x, cam_ids)
            
        except Exception as e:
            print(f"    Pose optimization failed: {e}")
    
    def _pose_residuals(self, poses, cam_ids):
        """í¬ì¦ˆ ìµœì í™”ë¥¼ ìœ„í•œ ì”ì°¨ ê³„ì‚°"""
        residuals = []
        
        # í¬ì¦ˆ ì–¸íŒ¨í‚¹
        self._unpack_camera_poses(poses, cam_ids)
        
        # ê° ë§¤ì¹­ì— ëŒ€í•œ ì¬íˆ¬ì˜ ì˜¤ì°¨ ê³„ì‚°
        for (cam_i, cam_j), matches in self.matches.items():
            if cam_i not in self.cameras or cam_j not in self.cameras:
                continue
            
            # ê³ í’ˆì§ˆ ë§¤ì¹­ë§Œ ì‚¬ìš©
            high_conf_matches = [(idx_i, idx_j, conf) for idx_i, idx_j, conf in matches if conf > 0.2]
            
            if len(high_conf_matches) < 5:
                continue
            
            # ì—í”¼í´ë¼ ì˜¤ì°¨ ê³„ì‚°
            for idx_i, idx_j, conf in high_conf_matches[:10]:  # ìµœëŒ€ 10ê°œë§Œ ì‚¬ìš©
                try:
                    kpt_i = self.image_features[cam_i]['keypoints'][idx_i]
                    kpt_j = self.image_features[cam_j]['keypoints'][idx_j]
                    
                    # ì—í”¼í´ë¼ ì˜¤ì°¨ ê³„ì‚°
                    error = self._compute_epipolar_error(kpt_i, kpt_j, cam_i, cam_j)
                    residuals.append(error * conf)  # ì‹ ë¢°ë„ë¡œ ê°€ì¤‘ì¹˜
                    
                except Exception:
                    continue
        
        return np.array(residuals) if residuals else np.array([0.0])
    
    def _compute_epipolar_error(self, kpt_i, kpt_j, cam_i, cam_j):
        """ì—í”¼í´ë¼ ì˜¤ì°¨ ê³„ì‚°"""
        try:
            # ì¹´ë©”ë¼ ë§¤íŠ¸ë¦­ìŠ¤
            K_i = self.cameras[cam_i]['K']
            K_j = self.cameras[cam_j]['K']
            R_i = self.cameras[cam_i]['R']
            T_i = self.cameras[cam_i]['T']
            R_j = self.cameras[cam_j]['R']
            T_j = self.cameras[cam_j]['T']
            
            # ìƒëŒ€ í¬ì¦ˆ ê³„ì‚°
            R_rel = R_j @ R_i.T
            T_rel = T_j - R_rel @ T_i
            
            # Essential Matrix ê³„ì‚°
            T_skew = np.array([
                [0, -T_rel[2], T_rel[1]],
                [T_rel[2], 0, -T_rel[0]],
                [-T_rel[1], T_rel[0], 0]
            ])
            E = T_skew @ R_rel
            
            # Fundamental Matrix ê³„ì‚°
            F = np.linalg.inv(K_j).T @ E @ np.linalg.inv(K_i)
            
            # ì—í”¼í´ë¼ ì˜¤ì°¨ ê³„ì‚°
            kpt_i_h = np.append(kpt_i, 1)
            kpt_j_h = np.append(kpt_j, 1)
            
            error = abs(kpt_j_h @ F @ kpt_i_h)
            
            return error
            
        except Exception:
            return 1.0
    
    def _unpack_camera_poses(self, poses, cam_ids):
        """ì¹´ë©”ë¼ í¬ì¦ˆ ì–¸íŒ¨í‚¹"""
        idx = 0
        
        for cam_id in cam_ids[1:]:  # ì²« ë²ˆì§¸ ì¹´ë©”ë¼ëŠ” ê³ ì •
            # ë¡œë“œë¦¬ê²ŒìŠ¤ ë²¡í„°
            rvec = poses[idx:idx+3]
            idx += 3
            
            # ì´ë™ ë²¡í„°
            tvec = poses[idx:idx+3]
            idx += 3
            
            # íšŒì „ í–‰ë ¬ë¡œ ë³€í™˜
            R = self._angle_axis_to_rotation_matrix(rvec)
            
            self.cameras[cam_id]['R'] = R.astype(np.float32)
            self.cameras[cam_id]['T'] = tvec.astype(np.float32)
    
    def _validate_and_correct_poses(self):
        """í¬ì¦ˆ í’ˆì§ˆ ê²€ì¦ ë° ë³´ì •"""
        print("    Validating and correcting poses...")
        
        invalid_cameras = []
        
        for cam_id, cam_data in self.cameras.items():
            if not self._is_valid_camera_pose_strict(cam_data):
                invalid_cameras.append(cam_id)
        
        if invalid_cameras:
            print(f"    Found {len(invalid_cameras)} invalid poses, correcting...")
            self._correct_invalid_poses(invalid_cameras)
    
    def _is_valid_camera_pose_strict(self, cam_data):
        """ì—„ê²©í•œ ì¹´ë©”ë¼ í¬ì¦ˆ ìœ íš¨ì„± ê²€ì¦"""
        try:
            R, T = cam_data['R'], cam_data['T']
            
            # íšŒì „ í–‰ë ¬ ê²€ì¦
            if not self._is_valid_rotation_matrix(R):
                return False
            
            # ì´ë™ ë²¡í„° ê²€ì¦
            if np.any(np.isnan(T)) or np.any(np.isinf(T)):
                return False
            
            # ì´ë™ ë²¡í„° í¬ê¸° ê²€ì¦
            if np.linalg.norm(T) > 100:  # ë„ˆë¬´ ë©€ë¦¬ ìˆëŠ” ì¹´ë©”ë¼
                return False
            
            return True
            
        except Exception:
            return False
    
    def _correct_invalid_poses(self, invalid_cameras):
        """ì˜ëª»ëœ í¬ì¦ˆ ë³´ì •"""
        for cam_id in invalid_cameras:
            # ì´ì›ƒ ì¹´ë©”ë¼ë“¤ì˜ í‰ê·  í¬ì¦ˆ ì‚¬ìš©
            valid_neighbors = []
            
            for neighbor_id in self.camera_graph.get(cam_id, []):
                if neighbor_id in self.cameras and neighbor_id not in invalid_cameras:
                    valid_neighbors.append(neighbor_id)
            
            if valid_neighbors:
                # ì´ì›ƒ ì¹´ë©”ë¼ë“¤ì˜ í‰ê·  í¬ì¦ˆ ê³„ì‚°
                avg_R = np.mean([self.cameras[n]['R'] for n in valid_neighbors], axis=0)
                avg_T = np.mean([self.cameras[n]['T'] for n in valid_neighbors], axis=0)
                
                # íšŒì „ í–‰ë ¬ ì •ê·œí™”
                U, _, Vt = np.linalg.svd(avg_R)
                avg_R = U @ Vt
                
                self.cameras[cam_id]['R'] = avg_R.astype(np.float32)
                self.cameras[cam_id]['T'] = avg_T.astype(np.float32)
                
                print(f"    Corrected pose for camera {cam_id}")
    
    def _create_default_camera_poses(self):
        """ê¸°ë³¸ ì¹´ë©”ë¼ í¬ì¦ˆ ìƒì„±"""
        print("    Creating default camera poses...")
        
        for cam_id in range(len(self.image_features)):
            angle = cam_id * (2 * np.pi / len(self.image_features))
            radius = 3.0
            
            # ì¹´ë©”ë¼ í¬ì¦ˆ (ì›ì„ ë°”ë¼ë³´ë„ë¡)
            camera_pos = np.array([
                radius * np.cos(angle),
                0.0,
                radius * np.sin(angle)
            ])
            
            # ì›ì ì„ í–¥í•˜ëŠ” ë°©í–¥
            look_at = np.array([0.0, 0.0, 0.0])
            up = np.array([0.0, 1.0, 0.0])
            
            # ì¹´ë©”ë¼ íšŒì „ í–‰ë ¬ ê³„ì‚°
            forward = look_at - camera_pos
            forward = forward / np.linalg.norm(forward)
            right = np.cross(forward, up)
            right = right / np.linalg.norm(right)
            up = np.cross(right, forward)
            
            R = np.array([right, up, -forward]).T
            T = camera_pos
            
            self.cameras[cam_id] = {
                'R': R.astype(np.float32),
                'T': T.astype(np.float32),
                'K': self._estimate_intrinsics(cam_id)
            }
    
    def _estimate_relative_pose_robust(self, cam_i, cam_j, pair_key):
        """ê°œì„ ëœ ë‘ ì¹´ë©”ë¼ ê°„ ìƒëŒ€ í¬ì¦ˆ ì¶”ì • - ê·¹ë„ë¡œ ì™„í™”ëœ ë²„ì „"""
        matches = self.matches[pair_key]
        
        if len(matches) < 4:  # 6 â†’ 4ë¡œ ë” ì™„í™”
            print(f"    Pair {cam_i}-{cam_j}: Insufficient matches ({len(matches)} < 4)")
            return None, None
        
        # ë§¤ì¹­ì ë“¤ ì¶”ì¶œ
        kpts_i = self.image_features[cam_i]['keypoints']
        kpts_j = self.image_features[cam_j]['keypoints']
        
        print(f"    Pair {cam_i}-{cam_j}: kpts_i shape: {kpts_i.shape}, kpts_j shape: {kpts_j.shape}")
        
        # ğŸ”§ ê·¹ë„ë¡œ ì™„í™”ëœ ì‹ ë¢°ë„ ì„ê³„ê°’
        high_conf_matches = [(idx_i, idx_j, conf) for idx_i, idx_j, conf in matches if conf > 0.00001]  # 0.0001 â†’ 0.00001ë¡œ ëŒ€í­ ì™„í™”
        
        if len(high_conf_matches) < 4:  # 6 â†’ 4ë¡œ ë” ì™„í™”
            high_conf_matches = [(idx_i, idx_j, conf) for idx_i, idx_j, conf in matches if conf > 0.000001]  # 0.00001 â†’ 0.000001ë¡œ ëŒ€í­ ì™„í™”
        
        if len(high_conf_matches) < 4:  # 6 â†’ 4ë¡œ ë” ì™„í™”
            # ëª¨ë“  ë§¤ì¹­ì„ ì‚¬ìš©
            high_conf_matches = [(idx_i, idx_j, conf) for idx_i, idx_j, conf in matches]
        
        if len(high_conf_matches) < 4:  # 6 â†’ 4ë¡œ ë” ì™„í™”
            print(f"    Pair {cam_i}-{cam_j}: Insufficient high-confidence matches ({len(high_conf_matches)} < 4)")
            return None, None
        
        # ğŸ”§ ì¸ë±ìŠ¤ ë²”ìœ„ ê²€ì¦ ê°•í™”
        valid_matches = []
        for idx_i, idx_j, conf in high_conf_matches:
            if (isinstance(idx_i, (int, np.integer)) and isinstance(idx_j, (int, np.integer)) and
                idx_i >= 0 and idx_j >= 0 and 
                idx_i < len(kpts_i) and idx_j < len(kpts_j)):
                valid_matches.append((idx_i, idx_j, conf))
        
        if len(valid_matches) < 4:  # 6 â†’ 4ë¡œ ë” ì™„í™”
            print(f"    Pair {cam_i}-{cam_j}: Insufficient valid matches after index validation ({len(valid_matches)} < 4)")
            return None, None
        
        print(f"    Pair {cam_i}-{cam_j}: Using {len(valid_matches)} validated matches")
        
        # ğŸ”§ ê°œì„ ëœ í¬ì¸íŠ¸ ì¶”ì¶œ
        try:
            pts_i = np.array([kpts_i[idx_i] for idx_i, _, _ in valid_matches])
            pts_j = np.array([kpts_j[idx_j] for _, idx_j, _ in valid_matches])
        except IndexError as e:
            print(f"    IndexError during point extraction: {e}")
            return None, None
        
        # ğŸ”§ ê¸°í•˜í•™ì  ì¼ê´€ì„± ì‚¬ì „ ê²€ì¦ (ë” ê´€ëŒ€í•˜ê²Œ)
        if not self._check_geometric_consistency_very_relaxed(pts_i, pts_j):
            print(f"    Pair {cam_i}-{cam_j}: Failed geometric consistency check")
            return None, None
        
        # ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„°
        K_i = self.cameras.get(cam_i, {}).get('K', self._estimate_intrinsics(cam_i))
        K_j = self._estimate_intrinsics(cam_j)
        
        # ğŸ”§ ê·¹ë„ë¡œ ê´€ëŒ€í•œ Essential Matrix ì¶”ì • ë°©ë²•ë“¤
        methods = [
            (cv2.RANSAC, 0.5, 0.99),    # ê·¹ë„ë¡œ ê´€ëŒ€í•œ ì„ê³„ê°’
            (cv2.RANSAC, 1.0, 0.95),
            (cv2.RANSAC, 2.0, 0.90),
            (cv2.LMEDS, 0.5, 0.95),
            (cv2.RANSAC, 5.0, 0.85),    # ë§¤ìš° ê´€ëŒ€í•œ ì„¤ì •
            (cv2.RANSAC, 10.0, 0.80),   # ê·¹ë„ë¡œ ê´€ëŒ€í•œ ì„¤ì •
            (cv2.RANSAC, 20.0, 0.70)    # ìµœëŒ€í•œ ê´€ëŒ€í•œ ì„¤ì •
        ]
        
        best_R, best_T = None, None
        best_inliers = 0
        best_quality = 0
        
        for method, threshold, confidence in methods:
            try:
                # Essential Matrix ì¶”ì •
                E, mask = cv2.findEssentialMat(
                    pts_i, pts_j, K_i,
                    method=method,
                    prob=confidence,
                    threshold=threshold,
                    maxIters=500  # ë°˜ë³µ íšŸìˆ˜ ì¤„ì„
                )
                
                if E is None or E.shape != (3, 3):
                    continue
                
                # í¬ì¦ˆ ë³µì›
                _, R, T, mask = cv2.recoverPose(E, pts_i, pts_j, K_i, mask=mask)
                
                if R is None or T is None:
                    continue
                
                inliers = np.sum(mask)
                
                if inliers >= 2:  # 4 â†’ 2ë¡œ ê·¹ë„ë¡œ ì™„í™”
                    # ğŸ”§ ë” ê´€ëŒ€í•œ í¬ì¦ˆ í’ˆì§ˆ ê²€ì¦
                    quality_score = self._evaluate_pose_quality_very_relaxed(pts_i, pts_j, R, T.flatten(), K_i, K_j, mask)
                    
                    if quality_score > best_quality:
                        best_R, best_T = R, T.flatten()
                        best_inliers = inliers
                        best_quality = quality_score
                        
            except Exception as e:
                print(f"      Method {method} failed: {e}")
                continue
        
        if best_R is not None:
            print(f"    Pair {cam_i}-{cam_j}: Successfully estimated pose with {best_inliers} inliers, quality: {best_quality:.3f}")
        else:
            print(f"    Pair {cam_i}-{cam_j}: Failed to estimate pose")
        
        return best_R, best_T


def readSuperGlueSceneInfo(path, images, eval, train_test_exp=False, llffhold=8, 
                          superglue_config="outdoor", max_images=100):
    """SuperGlue ê¸°ë°˜ ì™„ì „í•œ SfMìœ¼ë¡œ SceneInfo ìƒì„±"""
    
    print("=== SuperGlue Complete SfM Pipeline ===")
    print(f"ğŸš€ Pipeline available: {PIPELINE_AVAILABLE}")
    
    if not PIPELINE_AVAILABLE:
        print("âŒ Pipeline not available. Using fallback scene creation...")
        # ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        images_folder = Path(path) / (images if images else "images")
        return False
    
    # ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ê²½ë¡œ
    images_folder = Path(path) / (images if images else "images")
    output_folder = Path(path) / "superglue_sfm_output"
    
    # SuperGlue ì„¤ì • (ë” ì™„í™”ëœ ì„¤ì •)
    config = {
        'matcher': 'superglue',  # ì¶”ê°€: matcher íƒ€ì…(superglue, loftr, lightglue ë“±)
        'superpoint': {
            'nms_radius': 3,  # 4 â†’ 3ìœ¼ë¡œ ì™„í™”
            'keypoint_threshold': 0.001,  # 0.005 â†’ 0.001ë¡œ ëŒ€í­ ì™„í™”
            'max_keypoints': 8192  # 4096 â†’ 8192ë¡œ ì¦ê°€
        },
        'superglue': {
            'weights': superglue_config,  # 'indoor' ë˜ëŠ” 'outdoor'
            'sinkhorn_iterations': 15,  # 20 â†’ 15ë¡œ ì™„í™”
            'match_threshold': 0.05,  # 0.1 â†’ 0.05ë¡œ ì™„í™”
        },
        'loftr': {
            # LoFTR ê´€ë ¨ íŒŒë¼ë¯¸í„° ì˜ˆì‹œ (ì‹¤ì œ ì ìš©ì€ ì´í›„ ë‹¨ê³„)
            'weights': 'outdoor',
        },
        'lightglue': {
            # LightGlue ê´€ë ¨ íŒŒë¼ë¯¸í„° ì˜ˆì‹œ (ì‹¤ì œ ì ìš©ì€ ì´í›„ ë‹¨ê³„)
            'weights': 'outdoor',
        }
    }
    
    # SuperGlue 3DGS íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    try:
        pipeline = SuperGlue3DGSPipeline(config)
        print("âœ… SuperGlue pipeline initialized successfully")
    except Exception as e:
        print(f"âŒ Failed to initialize SuperGlue pipeline: {e}")
        print("Falling back to simple camera arrangement...")
        return False
    
    try:
        scene_info = pipeline.process_images_to_3dgs(
            image_dir=images_folder,
            output_dir=output_folder,
            max_images=max_images
        )
        
        print(f"\n=== SuperGlue SfM Results ===")
        print(f"Training cameras: {len(scene_info.train_cameras)}")
        print(f"Test cameras: {len(scene_info.test_cameras)}")
        print(f"3D points: {len(scene_info.point_cloud.points)}")
        print(f"Scene radius: {scene_info.nerf_normalization['radius']:.3f}")
        
        return scene_info
        
    except Exception as e:
        print(f"SuperGlue SfM failed: {e}")
        import traceback
        traceback.print_exc()
        
        # ì‹¤íŒ¨ì‹œ fallback
        print("Falling back to simple camera arrangement...")
        return False




# ëª…ë ¹ì¤„ ì¸í„°í˜ì´ìŠ¤
def main():
    """ëª…ë ¹ì¤„ì—ì„œ SuperGlue 3DGS íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    import argparse
    
    parser = argparse.ArgumentParser(description="SuperGlue 3DGS Pipeline")
    parser.add_argument("--input", "-i", required=True, 
                       help="Input images directory")
    parser.add_argument("--output", "-o", required=True,
                       help="Output directory for 3DGS training")
    parser.add_argument("--max_images", type=int, default=100,
                       help="Maximum number of images to process")
    parser.add_argument("--config", choices=["indoor", "outdoor"], 
                       default="outdoor", help="SuperGlue configuration")
    parser.add_argument("--device", default="cuda", 
                       help="Device to use (cuda/cpu)")
    
    args = parser.parse_args()
    
    # SuperGlue ì„¤ì •
    config = {
        'matcher': 'superglue',  # ì¶”ê°€: matcher íƒ€ì…(superglue, loftr, lightglue ë“±)
        'superpoint': {
            'nms_radius': 3,  # 4 â†’ 3ìœ¼ë¡œ ì™„í™”
            'keypoint_threshold': 0.001,  # 0.005 â†’ 0.001ë¡œ ëŒ€í­ ì™„í™”
            'max_keypoints': 8192  # 4096 â†’ 8192ë¡œ ì¦ê°€
        },
        'superglue': {
            'weights': args.config,
            'sinkhorn_iterations': 15,  # 20 â†’ 15ë¡œ ì™„í™”
            'match_threshold': 0.05,  # 0.1 â†’ 0.05ë¡œ ì™„í™”
        },
        'loftr': {
            # LoFTR ê´€ë ¨ íŒŒë¼ë¯¸í„° ì˜ˆì‹œ (ì‹¤ì œ ì ìš©ì€ ì´í›„ ë‹¨ê³„)
            'weights': 'outdoor',
        },
        'lightglue': {
            # LightGlue ê´€ë ¨ íŒŒë¼ë¯¸í„° ì˜ˆì‹œ (ì‹¤ì œ ì ìš©ì€ ì´í›„ ë‹¨ê³„)
            'weights': 'outdoor',
        }
    }
    
    print(f"=== SuperGlue 3DGS Pipeline ===")
    print(f"Input: {args.input}")
    print(f"Output: {args.output}")
    print(f"Max images: {args.max_images}")
    print(f"Config: {args.config}")
    print(f"Device: {args.device}")
    
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    pipeline = SuperGlue3DGSPipeline(config, device=args.device)
    
    try:
        scene_info = pipeline.process_images_to_3dgs(
            image_dir=args.input,
            output_dir=args.output,
            max_images=args.max_images
        )
        
        print(f"\n=== Success! ===")
        print(f"Processed {len(scene_info.train_cameras)} training cameras")
        print(f"Generated {len(scene_info.point_cloud.points)} 3D points")
        print(f"Files saved to: {args.output}")
        print(f"\nNext step:")
        print(f"python train.py -s {args.output} -m {args.output}/3dgs_output")
        
    except Exception as e:
        print(f"\n=== Error! ===")
        print(f"Pipeline failed: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
