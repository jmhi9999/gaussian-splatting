# superglue_3dgs_complete.py
# SuperGlueì™€ 3DGS ì™„ì „ í†µí•© íŒŒì´í”„ë¼ì¸

import glob
from tkinter import Image
import numpy as np
import cv2
import torch
from pathlib import Path
import json
from scipy.optimize import least_squares
import matplotlib.pyplot as plt
from collections import defaultdict
import os
import sys
import time
import gc
# import psutil  # ì œê±° - ì˜ì¡´ì„± ë¬¸ì œ í•´ê²°
from scipy.spatial.distance import cdist
import concurrent.futures
import networkx as nx

# CLIP ê´€ë ¨ imports (ì„ íƒì )
CLIP_AVAILABLE = False
try:
    import clip
    from PIL import Image as PILImage
    # CLIP ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸ (ì„ íƒì )
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model, preprocess = clip.load("ViT-B/32", device=device)
    CLIP_AVAILABLE = True
    print("âœ“ CLIP available and tested")
except (ImportError, Exception) as e:
    CLIP_AVAILABLE = False
    print(f"âš ï¸  CLIP not available: {e}. AdaptiveMatcher will use fallback descriptors.")

# SuperGlue ê´€ë ¨ imports
from models.matching import Matching
from models.utils import frame2tensor

# 3DGS ê´€ë ¨ imports - lazy importë¡œ ë³€ê²½
def get_3dgs_imports():
    """3DGS ê´€ë ¨ ëª¨ë“ˆë“¤ì„ lazy import - ê°œì„ ëœ ë²„ì „"""
    # gaussian-splatting ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€
    gaussian_splatting_root = Path(__file__).parent.parent
    if str(gaussian_splatting_root) not in sys.path:
        sys.path.insert(0, str(gaussian_splatting_root))
    
    # ì¶”ê°€ ê²½ë¡œë“¤ ì‹œë„
    additional_paths = [
        gaussian_splatting_root,
        gaussian_splatting_root / "scene",
        gaussian_splatting_root / "utils",
        Path.cwd(),
        Path.cwd().parent
    ]
    
    for path in additional_paths:
        if str(path) not in sys.path:
            sys.path.insert(0, str(path))
    
    try:
        # ë¨¼ì € scene.dataset_readers ì‹œë„
        from scene.dataset_readers import CameraInfo, SceneInfo
        print("âœ“ Successfully imported CameraInfo and SceneInfo from scene.dataset_readers")
    except ImportError as e:
        print(f"âœ— Failed to import from scene.dataset_readers: {e}")
        try:
            # ì§ì ‘ import ì‹œë„
            import scene.dataset_readers as dataset_readers
            CameraInfo = dataset_readers.CameraInfo
            SceneInfo = dataset_readers.SceneInfo
            print("âœ“ Successfully imported CameraInfo and SceneInfo via direct import")
        except ImportError as e2:
            print(f"âœ— Direct import also failed: {e2}")
            # Fallback í´ë˜ìŠ¤ ì •ì˜
            print("âš ï¸  Creating fallback CameraInfo and SceneInfo classes")
            
            class CameraInfo:
                def __init__(self, uid, R, T, FovY, FovX, image_path, image_name, 
                             width, height, depth_params=None, depth_path="", is_test=False):
                    self.uid = uid
                    self.R = R
                    self.T = T
                    self.FovY = FovY
                    self.FovX = FovX
                    self.image_path = image_path
                    self.image_name = image_name
                    self.width = width
                    self.height = height
                    self.depth_params = depth_params
                    self.depth_path = depth_path
                    self.is_test = is_test
            
            class SceneInfo:
                def __init__(self, point_cloud, train_cameras, test_cameras, 
                             nerf_normalization, ply_path="", is_nerf_synthetic=False):
                    self.point_cloud = point_cloud
                    self.train_cameras = train_cameras
                    self.test_cameras = test_cameras
                    self.nerf_normalization = nerf_normalization
                    self.ply_path = ply_path
                    self.is_nerf_synthetic = is_nerf_synthetic
    
    try:
        # utils.graphics_utils ì‹œë„
        from utils.graphics_utils import BasicPointCloud
        print("âœ“ Successfully imported BasicPointCloud from utils.graphics_utils")
    except ImportError as e:
        print(f"âœ— Failed to import BasicPointCloud from utils.graphics_utils: {e}")
        try:
            # ì§ì ‘ import ì‹œë„
            import utils.graphics_utils as graphics_utils
            BasicPointCloud = graphics_utils.BasicPointCloud
            print("âœ“ Successfully imported BasicPointCloud via direct import")
        except ImportError as e2:
            print(f"âœ— Direct import also failed: {e2}")
            # Fallback í´ë˜ìŠ¤ ì •ì˜
            print("âš ï¸  Creating fallback BasicPointCloud class")
            
            class BasicPointCloud:
                def __init__(self, points, colors, normals):
                    self.points = points
                    self.colors = colors
                    self.normals = normals
    
    # ìµœì¢… í™•ì¸
    if 'CameraInfo' not in locals() or 'SceneInfo' not in locals() or 'BasicPointCloud' not in locals():
        print("âŒ Critical: Could not import any 3DGS modules")
        return None, None, None
    
    print("âœ… All 3DGS modules successfully imported or created")
    return CameraInfo, SceneInfo, BasicPointCloud


def test_pipeline_availability():
    """íŒŒì´í”„ë¼ì¸ ê°€ìš©ì„± í…ŒìŠ¤íŠ¸ - ê°œì„ ëœ ë²„ì „"""
    print("ğŸ” Testing SuperGlue 3DGS Pipeline availability...")
    
    # 1. SuperGlue ëª¨ë“ˆ í…ŒìŠ¤íŠ¸
    superglue_available = False
    try:
        from models.matching import Matching
        from models.utils import frame2tensor
        print("âœ“ SuperGlue modules available")
        superglue_available = True
    except ImportError as e:
        print(f"âœ— SuperGlue modules not available: {e}")
        print("  This is expected if SuperGlue models are not installed")
    
    # 2. 3DGS ëª¨ë“ˆ í…ŒìŠ¤íŠ¸
    gs_available = False
    try:
        CameraInfo, SceneInfo, BasicPointCloud = get_3dgs_imports()
        if CameraInfo is not None and SceneInfo is not None and BasicPointCloud is not None:
            print("âœ“ 3DGS modules available")
            gs_available = True
        else:
            print("âœ— 3DGS modules not available")
    except Exception as e:
        print(f"âœ— 3DGS modules test failed: {e}")
        return False
    
    
    print(f"\nğŸ“Š Pipeline Availability Summary:")
    print(f"  SuperGlue: {'âœ“' if superglue_available else 'âœ—'}")
    print(f"  3DGS: {'âœ“' if gs_available else 'âœ—'}")
    
    return True


# íŒŒì´í”„ë¼ì¸ ê°€ìš©ì„± í…ŒìŠ¤íŠ¸ ì‹¤í–‰
PIPELINE_AVAILABLE = test_pipeline_availability()


class TrackManager:
    """íŠ¹ì§•ì  íŠ¸ë™ ê´€ë¦¬ ë° ì‚¼ê°ì¸¡ëŸ‰"""
    
    def __init__(self):
        self.tracks = {}  # track_id -> {points: [(cam_id, kpt_idx), ...], color: [r,g,b]}
        self.track_id_counter = 0
        self.min_views = 3
        self.min_track_length = 2
    
    def build_tracks(self, matches, image_features):
        """ë§¤ì¹­ ê²°ê³¼ë¡œë¶€í„° íŠ¸ë™ ìƒì„±"""
        print("  Building tracks from matches...")
        
        # ë§¤ì¹­ì„ ê·¸ë˜í”„ë¡œ ë³€í™˜
        track_graph = defaultdict(list)
        
        for (cam_i, cam_j), match_list in matches.items():
            for idx_i, idx_j, conf in match_list:
                # ê° ë§¤ì¹­ì„ ë…¸ë“œë¡œ í‘œí˜„
                node_i = (cam_i, idx_i)
                node_j = (cam_j, idx_j)
                
                track_graph[node_i].append((node_j, conf))
                track_graph[node_j].append((node_i, conf))
        
        # ì—°ê²°ëœ ì»´í¬ë„ŒíŠ¸ ì°¾ê¸° (íŠ¸ë™)
        visited = set()
        tracks = []
        
        for node in track_graph:
            if node in visited:
                continue
            
            # BFSë¡œ ì—°ê²°ëœ ë…¸ë“œë“¤ ì°¾ê¸°
            track_nodes = []
            queue = [node]
            visited.add(node)
            
            while queue:
                current = queue.pop(0)
                track_nodes.append(current)
                
                for neighbor, conf in track_graph[current]:
                    if neighbor not in visited:
                        visited.add(neighbor)
                        queue.append(neighbor)
            
            if len(track_nodes) >= self.min_track_length:
                tracks.append(track_nodes)
        
        # íŠ¸ë™ì„ ë‚´ë¶€ êµ¬ì¡°ë¡œ ë³€í™˜
        for track_nodes in tracks:
            track_data = {
                'points': track_nodes,
                'color': self._estimate_track_color(track_nodes, image_features),
                'confidence': self._compute_track_confidence(track_nodes, track_graph)
            }
            
            self.tracks[self.track_id_counter] = track_data
            self.track_id_counter += 1
        
        print(f"    Built {len(self.tracks)} tracks")
    
    def filter_tracks(self, min_views=3):
        """íŠ¸ë™ í•„í„°ë§"""
        print(f"  Filtering tracks (min_views={min_views})...")
        
        tracks_to_remove = []
        for track_id, track_data in self.tracks.items():
            # ê³ ìœ í•œ ì¹´ë©”ë¼ ìˆ˜ ê³„ì‚°
            unique_cameras = set(cam_id for cam_id, _ in track_data['points'])
            
            if len(unique_cameras) < min_views:
                tracks_to_remove.append(track_id)
        
        # í•„í„°ë§ëœ íŠ¸ë™ ì œê±°
        for track_id in tracks_to_remove:
            del self.tracks[track_id]
        
        print(f"    Kept {len(self.tracks)} tracks after filtering")
    
    def triangulate_tracks(self, cameras, image_features):
        """íŠ¸ë™ë“¤ì„ ì‚¼ê°ì¸¡ëŸ‰í•˜ì—¬ 3D í¬ì¸íŠ¸ ìƒì„±"""
        print("  Triangulating tracks...")
        
        triangulated_points = {}
        successful_tracks = 0
        
        for track_id, track_data in self.tracks.items():
            try:
                # íŠ¸ë™ì˜ ê° ê´€ì°°ì  ìˆ˜ì§‘
                observations = []
                for cam_id, kpt_idx in track_data['points']:
                    if cam_id in cameras and cam_id in image_features:
                        kpts = image_features[cam_id]['keypoints']
                        if kpt_idx < len(kpts):
                            observations.append((cam_id, kpts[kpt_idx]))
                
                if len(observations) < 2:
                    continue
                
                # ì‚¼ê°ì¸¡ëŸ‰ ìˆ˜í–‰
                point_3d = self._triangulate_track(observations, cameras)
                
                if point_3d is not None:
                    triangulated_points[track_id] = {
                        'xyz': point_3d,
                        'color': track_data['color'],
                        'observations': observations
                    }
                    successful_tracks += 1
                    
            except Exception as e:
                print(f"    Track {track_id} triangulation failed: {e}")
                continue
        
        print(f"    Successfully triangulated {successful_tracks} tracks")
        return triangulated_points
    
    def _estimate_track_color(self, track_nodes, image_features):
        """íŠ¸ë™ì˜ ìƒ‰ìƒ ì¶”ì •"""
        # ì²« ë²ˆì§¸ ê´€ì°°ì ì˜ ìƒ‰ìƒì„ ì‚¬ìš© (ì‹¤ì œë¡œëŠ” ì´ë¯¸ì§€ì—ì„œ ìƒ˜í”Œë§)
        if track_nodes:
            cam_id, kpt_idx = track_nodes[0]
            if cam_id in image_features:
                # ê°„ë‹¨í•œ ëœë¤ ìƒ‰ìƒ ìƒì„±
                return np.random.rand(3).astype(np.float32)
        
        return np.array([0.5, 0.5, 0.5], dtype=np.float32)
    
    def _compute_track_confidence(self, track_nodes, track_graph):
        """íŠ¸ë™ì˜ ì‹ ë¢°ë„ ê³„ì‚°"""
        if len(track_nodes) < 2:
            return 0.0
        
        # ì—°ê²° ê°•ë„ í‰ê·  ê³„ì‚°
        total_conf = 0.0
        edge_count = 0
        
        for i, node_i in enumerate(track_nodes):
            for j, node_j in enumerate(track_nodes[i+1:], i+1):
                # ë‘ ë…¸ë“œ ê°„ì˜ ì—°ê²° ì°¾ê¸°
                for neighbor, conf in track_graph[node_i]:
                    if neighbor == node_j:
                        total_conf += conf
                        edge_count += 1
                        break
        
        if edge_count > 0:
            return total_conf / edge_count
        else:
            return 0.0
    
    def _triangulate_track(self, observations, cameras):
        """ë‹¨ì¼ íŠ¸ë™ ì‚¼ê°ì¸¡ëŸ‰"""
        if len(observations) < 2:
            return None
        
        # íˆ¬ì˜ í–‰ë ¬ë“¤ ìˆ˜ì§‘
        projection_matrices = []
        points_2d = []
        
        for cam_id, point_2d in observations:
            if cam_id in cameras:
                cam = cameras[cam_id]
                K, R, T = cam['K'], cam['R'], cam['T']
                
                # íˆ¬ì˜ í–‰ë ¬ ìƒì„±
                t = -R @ T
                RT = np.hstack([R, t.reshape(-1, 1)])
                P = K @ RT
                
                projection_matrices.append(P)
                points_2d.append(point_2d)
        
        if len(projection_matrices) < 2:
            return None
        
        try:
            # OpenCV ì‚¼ê°ì¸¡ëŸ‰
            points_2d_array = np.array(points_2d).T
            points_4d = cv2.triangulatePoints(
                projection_matrices[0], 
                projection_matrices[1], 
                points_2d_array[:, 0:1], 
                points_2d_array[:, 1:2]
            )
            
            # 4Dì—ì„œ 3Dë¡œ ë³€í™˜
            if abs(points_4d[3, 0]) > 1e-10:
                point_3d = (points_4d[:3] / points_4d[3]).flatten()
                
                # ìœ íš¨ì„± ê²€ì‚¬
                if self._is_valid_3d_point(point_3d, observations, cameras):
                    return point_3d.astype(np.float32)
            
        except Exception as e:
            print(f"      Triangulation error: {e}")
        
        return None
    
    def _is_valid_3d_point(self, point_3d, observations, cameras):
        """3D í¬ì¸íŠ¸ ìœ íš¨ì„± ê²€ì‚¬"""
        # NaN/Inf ì²´í¬
        if np.any(np.isnan(point_3d)) or np.any(np.isinf(point_3d)):
            return False
        
        # ê±°ë¦¬ ì²´í¬
        distance = np.linalg.norm(point_3d)
        if distance > 1000 or distance < 0.001:
            return False
        
        # ì¬íˆ¬ì˜ ì˜¤ì°¨ ì²´í¬
        max_error = 0.0
        for cam_id, point_2d in observations:
            if cam_id in cameras:
                cam = cameras[cam_id]
                K, R, T = cam['K'], cam['R'], cam['T']
                
                # ì¹´ë©”ë¼ ì¢Œí‘œê³„ë¡œ ë³€í™˜
                point_cam = R @ (point_3d - T)
                
                if point_cam[2] <= 0:  # ì¹´ë©”ë¼ ë’¤ìª½
                    return False
                
                # ì¬íˆ¬ì˜
                point_2d_proj = K @ point_cam
                point_2d_proj = point_2d_proj[:2] / point_2d_proj[2]
                
                error = np.linalg.norm(point_2d_proj - point_2d)
                max_error = max(max_error, error)
        
        return max_error < 10.0  # 10í”½ì…€ ì´í•˜ ì˜¤ì°¨


class PoseGraphOptimizer:
    """í¬ì¦ˆ ê·¸ë˜í”„ ìµœì í™”"""
    
    def __init__(self):
        self.pose_graph = nx.Graph()
        self.edge_weights = {}
        self.min_matches = 10
    
    def build_pose_graph(self, matches, cameras):
        """ë§¤ì¹­ ê²°ê³¼ë¡œë¶€í„° í¬ì¦ˆ ê·¸ë˜í”„ ìƒì„±"""
        print("  Building pose graph...")
        
        # ë…¸ë“œ ì¶”ê°€ (ì¹´ë©”ë¼ë“¤)
        for cam_id in cameras:
            self.pose_graph.add_node(cam_id)
        
        # ì—£ì§€ ì¶”ê°€ (ë§¤ì¹­ì´ ìˆëŠ” ì¹´ë©”ë¼ ìŒ)
        for (cam_i, cam_j), match_list in matches.items():
            if len(match_list) >= self.min_matches:
                weight = len(match_list)  # ë§¤ì¹­ ìˆ˜ë¥¼ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©
                self.pose_graph.add_edge(cam_i, cam_j, weight=weight)
                self.edge_weights[(cam_i, cam_j)] = weight
        
        print(f"    Graph has {self.pose_graph.number_of_nodes()} nodes and {self.pose_graph.number_of_edges()} edges")
    
    def remove_outlier_edges(self, min_matches=10):
        """ì´ìƒì¹˜ ì—£ì§€ ì œê±°"""
        print(f"  Removing outlier edges (min_matches={min_matches})...")
        
        edges_to_remove = []
        for (cam_i, cam_j), weight in self.edge_weights.items():
            if weight < min_matches:
                edges_to_remove.append((cam_i, cam_j))
        
        for cam_i, cam_j in edges_to_remove:
            self.pose_graph.remove_edge(cam_i, cam_j)
            del self.edge_weights[(cam_i, cam_j)]
        
        print(f"    Removed {len(edges_to_remove)} outlier edges")
    
    def get_spanning_tree(self):
        """ìµœì†Œ ì‹ ì¥ íŠ¸ë¦¬ ê³„ì‚°"""
        print("  Computing minimum spanning tree...")
        
        try:
            # ìµœëŒ€ ê°€ì¤‘ì¹˜ ì‹ ì¥ íŠ¸ë¦¬ (ë§¤ì¹­ ìˆ˜ê°€ ë§ì„ìˆ˜ë¡ ì¢‹ìŒ)
            mst = nx.maximum_spanning_tree(self.pose_graph, weight='weight')
            
            print(f"    MST has {mst.number_of_edges()} edges")
            return mst
            
        except Exception as e:
            print(f"    MST computation failed: {e}")
            # ì—°ê²°ëœ ì»´í¬ë„ŒíŠ¸ ì¤‘ ê°€ì¥ í° ê²ƒ ë°˜í™˜
            largest_cc = max(nx.connected_components(self.pose_graph), key=len)
            subgraph = self.pose_graph.subgraph(largest_cc)
            return subgraph
    
    def optimize_poses(self, cameras, matches):
        """í¬ì¦ˆ ê·¸ë˜í”„ ìµœì í™”"""
        print("  Optimizing pose graph...")
        
        # í˜„ì¬ëŠ” ê°„ë‹¨í•œ êµ¬í˜„ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ìµœì í™” í•„ìš”)
        # 1. ìŠ¤íŒ¨ë‹ íŠ¸ë¦¬ ê¸°ë°˜ ì´ˆê¸°í™”
        mst = self.get_spanning_tree()
        
        # 2. ë£¨í”„ í´ë¡œì € ê²€ì¶œ ë° ì œì•½ ì¶”ê°€
        loops = self._detect_loops(mst)
        
        # 3. í¬ì¦ˆ ìµœì í™” (ê°„ë‹¨í•œ ë²„ì „)
        optimized_cameras = self._simple_pose_optimization(cameras, matches, mst)
        
        return optimized_cameras
    
    def _detect_loops(self, mst):
        """ë£¨í”„ í´ë¡œì € ê²€ì¶œ"""
        loops = []
        
        # MSTì— ì—†ëŠ” ì—£ì§€ë“¤ì„ í™•ì¸í•˜ì—¬ ë£¨í”„ í˜•ì„±
        for edge in self.pose_graph.edges():
            if edge not in mst.edges():
                # ì´ ì—£ì§€ë¥¼ ì¶”ê°€í–ˆì„ ë•Œ í˜•ì„±ë˜ëŠ” ë£¨í”„ ì°¾ê¸°
                temp_graph = mst.copy()
                temp_graph.add_edge(*edge)
                
                try:
                    cycle = nx.find_cycle(temp_graph)
                    if len(cycle) > 3:  # 3ê°œ ì´ìƒì˜ ë…¸ë“œë¡œ êµ¬ì„±ëœ ë£¨í”„
                        loops.append(cycle)
                except nx.NetworkXNoCycle:
                    pass
        
        return loops
    
    def _simple_pose_optimization(self, cameras, matches, mst):
        """ê°„ë‹¨í•œ í¬ì¦ˆ ìµœì í™”"""
        # í˜„ì¬ëŠ” ì›ë³¸ ì¹´ë©”ë¼ ë°˜í™˜ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ë³µì¡í•œ ìµœì í™”)
        return cameras.copy()


class BundleAdjuster:
    """Bundle Adjustment ìµœì í™”"""
    
    def __init__(self, loss_type='huber', max_iter=50):
        self.loss_type = loss_type
        self.max_iter = max_iter
        self.quality_metrics = {}
    
    def run(self, cameras, points_3d, point_observations, callback=None):
        """Bundle Adjustment ì‹¤í–‰"""
        print("  Running Bundle Adjustment...")
        
        if len(cameras) < 2 or len(points_3d) < 5:
            print("    Insufficient data for bundle adjustment")
            return False
        
        try:
            # íŒŒë¼ë¯¸í„° íŒ¨í‚¹
            params = self._pack_parameters(cameras, points_3d)
            
            # ìµœì í™” ì‹¤í–‰
            result = least_squares(
                self._compute_residuals,
                params,
                args=(cameras, points_3d, point_observations),
                method='lm',
                max_nfev=self.max_iter,
                verbose=1
            )
            
            # ê²°ê³¼ ì–¸íŒ¨í‚¹
            self._unpack_parameters(result.x, cameras, points_3d)
            
            # í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
            self.quality_metrics = {
                'final_cost': result.cost,
                'iterations': result.nfev,
                'success': result.success
            }
            
            if callback:
                callback(self.quality_metrics)
            
            print(f"    BA completed: cost={result.cost:.6f}, iterations={result.nfev}")
            return True
            
        except Exception as e:
            print(f"    Bundle adjustment failed: {e}")
            return False
    
    def _pack_parameters(self, cameras, points_3d):
        """íŒŒë¼ë¯¸í„°ë¥¼ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ íŒ¨í‚¹"""
        params = []
        
        # ì¹´ë©”ë¼ íŒŒë¼ë¯¸í„° (íšŒì „ + ì´ë™)
        for cam_id in sorted(cameras.keys()):
            cam = cameras[cam_id]
            R, T = cam['R'], cam['T']
            
            # íšŒì „ í–‰ë ¬ì„ ë¡œë“œë¦¬ê²ŒìŠ¤ ë²¡í„°ë¡œ ë³€í™˜
            angle_axis = self._rotation_matrix_to_angle_axis(R)
            params.extend(angle_axis)
            params.extend(T)
        
        # 3D í¬ì¸íŠ¸
        for point_id in sorted(points_3d.keys()):
            point = points_3d[point_id]['xyz']
            params.extend(point)
        
        return np.array(params)
    
    def _unpack_parameters(self, params, cameras, points_3d):
        """ë²¡í„°ì—ì„œ íŒŒë¼ë¯¸í„° ì–¸íŒ¨í‚¹"""
        idx = 0
        
        # ì¹´ë©”ë¼ íŒŒë¼ë¯¸í„° ë³µì›
        for cam_id in sorted(cameras.keys()):
            # ë¡œë“œë¦¬ê²ŒìŠ¤ ë²¡í„° (3ê°œ)
            angle_axis = params[idx:idx+3]
            idx += 3
            
            # ì´ë™ ë²¡í„° (3ê°œ)
            T = params[idx:idx+3]
            idx += 3
            
            # íšŒì „ í–‰ë ¬ë¡œ ë³€í™˜
            R = self._angle_axis_to_rotation_matrix(angle_axis)
            
            cameras[cam_id]['R'] = R.astype(np.float32)
            cameras[cam_id]['T'] = T.astype(np.float32)
        
        # 3D í¬ì¸íŠ¸ ë³µì›
        for point_id in sorted(points_3d.keys()):
            xyz = params[idx:idx+3]
            idx += 3
            points_3d[point_id]['xyz'] = xyz.astype(np.float32)
    
    def _compute_residuals(self, params, cameras, points_3d, point_observations):
        """Bundle Adjustment ì”ì°¨ ê³„ì‚°"""
        # íŒŒë¼ë¯¸í„° ì–¸íŒ¨í‚¹
        self._unpack_parameters(params, cameras, points_3d)
        
        residuals = []
        
        # ê° ê´€ì°°ì— ëŒ€í•œ ì¬íˆ¬ì˜ ì˜¤ì°¨ ê³„ì‚°
        for point_id, observations in point_observations.items():
            if point_id not in points_3d:
                continue
            
            point_3d = points_3d[point_id]['xyz']
            
            for cam_id, observed_pt, conf in observations:
                if cam_id not in cameras:
                    continue
                
                try:
                    cam = cameras[cam_id]
                    K = cam['K']
                    R = cam['R']
                    T = cam['T']
                    
                    # ì¹´ë©”ë¼ ì¢Œí‘œê³„ë¡œ ë³€í™˜
                    point_cam = R @ (point_3d - T)
                    
                    if point_cam[2] <= 0:
                        residuals.extend([10.0, 10.0])
                        continue
                    
                    # ì¬íˆ¬ì˜
                    point_2d_proj = K @ point_cam
                    point_2d_proj = point_2d_proj[:2] / point_2d_proj[2]
                    
                    # ì”ì°¨ ê³„ì‚°
                    residual = point_2d_proj - observed_pt
                    
                    # Huber loss ì ìš©
                    if self.loss_type == 'huber':
                        residual = self._apply_huber_loss(residual, delta=3.0)
                    
                    # ì‹ ë¢°ë„ ê°€ì¤‘ì¹˜
                    weight = np.clip(conf, 0.1, 1.0)
                    residual = residual * weight
                    
                    residuals.extend(residual)
                    
                except Exception:
                    residuals.extend([2.0, 2.0])
        
        return np.array(residuals)
    
    def _apply_huber_loss(self, residual, delta=3.0):
        """Huber loss ì ìš©"""
        abs_residual = np.abs(residual)
        mask = abs_residual <= delta
        
        result = np.zeros_like(residual)
        result[mask] = residual[mask]
        result[~mask] = delta * np.sign(residual[~mask]) * (2 * np.sqrt(abs_residual[~mask] / delta) - 1)
        
        return result
    
    def _rotation_matrix_to_angle_axis(self, R):
        """íšŒì „ í–‰ë ¬ì„ ë¡œë“œë¦¬ê²ŒìŠ¤ ë²¡í„°ë¡œ ë³€í™˜"""
        trace = np.trace(R)
        if trace > 3 - 1e-6:
            return np.zeros(3)
        
        angle = np.arccos((trace - 1) / 2)
        axis = np.array([
            R[2, 1] - R[1, 2],
            R[0, 2] - R[2, 0],
            R[1, 0] - R[0, 1]
        ])
        
        if np.linalg.norm(axis) > 1e-6:
            axis = axis / np.linalg.norm(axis)
        
        return angle * axis
    
    def _angle_axis_to_rotation_matrix(self, angle_axis):
        """ë¡œë“œë¦¬ê²ŒìŠ¤ ë²¡í„°ë¥¼ íšŒì „ í–‰ë ¬ë¡œ ë³€í™˜"""
        angle = np.linalg.norm(angle_axis)
        if angle < 1e-6:
            return np.eye(3)
        
        axis = angle_axis / angle
        K = np.array([
            [0, -axis[2], axis[1]],
            [axis[2], 0, -axis[0]],
            [-axis[1], axis[0], 0]
        ])
        
        R = np.eye(3) + np.sin(angle) * K + (1 - np.cos(angle)) * (K @ K)
        return R
    
    def monitor_quality(self, metrics):
        """í’ˆì§ˆ ë©”íŠ¸ë¦­ ëª¨ë‹ˆí„°ë§"""
        print(f"    BA Quality: cost={metrics['final_cost']:.6f}, "
              f"iterations={metrics['iterations']}, success={metrics['success']}")


class ParallelExecutor:
    """ë³‘ë ¬ ì‹¤í–‰ ê´€ë¦¬"""
    
    def __init__(self, max_workers=4):
        self.max_workers = max_workers
        self.executor = None
    
    def parallel_map(self, func, items):
        """ë³‘ë ¬ë¡œ í•¨ìˆ˜ ì‹¤í–‰"""
        if self.max_workers <= 1:
            return [func(item) for item in items]
        
        try:
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                results = list(executor.map(func, items))
            return results
        except Exception as e:
            print(f"    Parallel execution failed: {e}, falling back to sequential")
            return [func(item) for item in items]
    
    def parallel_process_features(self, image_paths, feature_extractor):
        """íŠ¹ì§•ì  ì¶”ì¶œ ë³‘ë ¬ ì²˜ë¦¬"""
        def extract_features(path):
            return feature_extractor.extract(path)
        
        return self.parallel_map(extract_features, image_paths)
    
    def parallel_match_pairs(self, pairs, matcher, image_features):
        """ë§¤ì¹­ ë³‘ë ¬ ì²˜ë¦¬"""
        def match_pair(pair):
            i, j = pair
            if i in image_features and j in image_features:
                return (pair, matcher.match(image_features[i], image_features[j]))
            return (pair, [])
        
        return self.parallel_map(match_pair, pairs)


class AutoTuner:
    """ìë™ íŒŒë¼ë¯¸í„° íŠœë‹"""
    
    def __init__(self, config):
        self.config = config
        self.quality_history = []
        self.parameter_history = []
        self.best_parameters = config.copy()
        self.best_quality = 0.0
    
    def evaluate_metrics(self, quality_metrics):
        """í’ˆì§ˆ ë©”íŠ¸ë¦­ í‰ê°€"""
        if not quality_metrics:
            return
        
        # ì¢…í•© í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        quality_score = self._compute_quality_score(quality_metrics)
        
        self.quality_history.append(quality_score)
        
        # ìµœê³  í’ˆì§ˆ ì—…ë°ì´íŠ¸
        if quality_score > self.best_quality:
            self.best_quality = quality_score
            self.best_parameters = self.config.copy()
            print(f"    New best quality: {quality_score:.4f}")
    
    def adjust_parameters(self):
        """íŒŒë¼ë¯¸í„° ìë™ ì¡°ì •"""
        if len(self.quality_history) < 2:
            return
        
        # í’ˆì§ˆ ë³€í™” ë¶„ì„
        recent_quality = self.quality_history[-1]
        previous_quality = self.quality_history[-2]
        
        if recent_quality < previous_quality:
            # í’ˆì§ˆì´ ê°ì†Œí–ˆìœ¼ë©´ íŒŒë¼ë¯¸í„°ë¥¼ ë³´ìˆ˜ì ìœ¼ë¡œ ì¡°ì •
            self._adjust_parameters_conservative()
        else:
            # í’ˆì§ˆì´ í–¥ìƒë˜ì—ˆìœ¼ë©´ ë” ì ê·¹ì ìœ¼ë¡œ ì¡°ì •
            self._adjust_parameters_aggressive()
    
    def _compute_quality_score(self, metrics):
        """ì¢…í•© í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        
        # í¬ì¦ˆ ì¶”ì • ì„±ê³µë¥ 
        if 'pose_estimation_success_rate' in metrics:
            score += metrics['pose_estimation_success_rate'] * 0.4
        
        # í‰ê·  ë§¤ì¹­ ìˆ˜
        if 'average_matches_per_pair' in metrics:
            avg_matches = metrics['average_matches_per_pair']
            score += min(avg_matches / 50.0, 1.0) * 0.3  # 50ê°œ ì´ìƒì´ë©´ ë§Œì 
        
        # Bundle Adjustment í’ˆì§ˆ
        if 'final_cost' in metrics:
            cost = metrics['final_cost']
            score += max(0, 1.0 - cost / 1000.0) * 0.3  # costê°€ ë‚®ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
        
        return score
    
    def _adjust_parameters_conservative(self):
        """ë³´ìˆ˜ì  íŒŒë¼ë¯¸í„° ì¡°ì •"""
        print("    Adjusting parameters conservatively...")
        
        # ë§¤ì¹­ ì„ê³„ê°’ ì™„í™”
        if 'superglue' in self.config:
            if 'match_threshold' in self.config['superglue']:
                current_threshold = self.config['superglue']['match_threshold']
                self.config['superglue']['match_threshold'] = max(0.01, current_threshold * 0.8)
        
        # íŠ¹ì§•ì  ìˆ˜ ì¦ê°€
        if 'superpoint' in self.config:
            if 'max_keypoints' in self.config['superpoint']:
                current_max = self.config['superpoint']['max_keypoints']
                self.config['superpoint']['max_keypoints'] = min(16384, current_max * 1.2)
    
    def _adjust_parameters_aggressive(self):
        """ì ê·¹ì  íŒŒë¼ë¯¸í„° ì¡°ì •"""
        print("    Adjusting parameters aggressively...")
        
        # ë§¤ì¹­ ì„ê³„ê°’ ê°•í™”
        if 'superglue' in self.config:
            if 'match_threshold' in self.config['superglue']:
                current_threshold = self.config['superglue']['match_threshold']
                self.config['superglue']['match_threshold'] = min(0.2, current_threshold * 1.1)
        
        # íŠ¹ì§•ì  ìˆ˜ ê°ì†Œ (í’ˆì§ˆ í–¥ìƒ)
        if 'superpoint' in self.config:
            if 'max_keypoints' in self.config['superpoint']:
                current_max = self.config['superpoint']['max_keypoints']
                self.config['superpoint']['max_keypoints'] = max(2048, current_max * 0.9)
    
    def get_best_parameters(self):
        """ìµœê³  í’ˆì§ˆì˜ íŒŒë¼ë¯¸í„° ë°˜í™˜"""
        return self.best_parameters.copy()


class AdaptiveMatcher:
    """Adaptive Matching: CLIP ê¸°ë°˜ global descriptor, cosine similarity, ìƒìœ„ Nê°œ ìŒ ì„ ì •"""
    
    def __init__(self, config, device):
        self.config = config
        self.device = device
        self.top_k = config.get('adaptive_top_k', 20)
        self.global_descriptors = {}
        
        # CLIP ëª¨ë¸ ë¡œë“œ (ì„ íƒì )
        self.clip_available = CLIP_AVAILABLE
        if self.clip_available:
            try:
                # ì „ì—­ì—ì„œ ì´ë¯¸ ë¡œë“œëœ ëª¨ë¸ ì‚¬ìš©
                self.model, self.preprocess = clip.load("ViT-B/32", device=self.device)
                print("âœ“ CLIP model loaded for adaptive matching")
            except Exception as e:
                print(f"âš ï¸  CLIP model loading failed: {e}, using fallback descriptors")
                self.clip_available = False
        else:
            print("âš ï¸  CLIP not available, using fallback global descriptors")
    
    def compute_global_descriptors(self, image_paths):
        """ì „ì—­ ì´ë¯¸ì§€ descriptor ê³„ì‚°"""
        print("  Computing global descriptors...")
        
        if self.clip_available:
            return self._compute_clip_descriptors(image_paths)
        else:
            return self._compute_fallback_descriptors(image_paths)
    
    def _compute_clip_descriptors(self, image_paths):
        """CLIPì„ ì‚¬ìš©í•œ ì „ì—­ descriptor ê³„ì‚°"""
        global_descs = {}
        
        if not self.clip_available:
            print("    CLIP not available, falling back to image statistics")
            return self._compute_fallback_descriptors(image_paths)
        
        for i, path in enumerate(image_paths):
            try:
                img = PILImage.open(path).convert("RGB")
                img_tensor = self.preprocess(img).unsqueeze(0).to(self.device)
                
                with torch.no_grad():
                    desc = self.model.encode_image(img_tensor).cpu().numpy().flatten()
                    desc = desc / (np.linalg.norm(desc) + 1e-10)
                
                global_descs[i] = desc
                
            except Exception as e:
                print(f"    Failed to compute CLIP descriptor for {path}: {e}")
                # Fallback descriptor
                global_descs[i] = np.random.randn(512).astype(np.float32)
                global_descs[i] = global_descs[i] / np.linalg.norm(global_descs[i])
        
        return global_descs
    
    def _compute_fallback_descriptors(self, image_paths):
        """Fallback ì „ì—­ descriptor ê³„ì‚° (SuperPoint íŠ¹ì§•ì  ê¸°ë°˜)"""
        global_descs = {}
        
        for i, path in enumerate(image_paths):
            try:
                # ê°„ë‹¨í•œ ì´ë¯¸ì§€ í†µê³„ ê¸°ë°˜ descriptor
                img = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)
                if img is None:
                    continue
                
                # ì´ë¯¸ì§€ í†µê³„ ê³„ì‚°
                mean_intensity = np.mean(img)
                std_intensity = np.std(img)
                hist = cv2.calcHist([img], [0], None, [256], [0, 256]).flatten()
                hist = hist / (np.sum(hist) + 1e-10)
                
                # ê°„ë‹¨í•œ descriptor ìƒì„±
                desc = np.concatenate([
                    [mean_intensity / 255.0, std_intensity / 255.0],
                    hist[:128],  # íˆìŠ¤í† ê·¸ë¨ì˜ ì ˆë°˜ë§Œ ì‚¬ìš©
                    hist[128:]
                ]).astype(np.float32)
                
                # ì •ê·œí™”
                desc = desc / (np.linalg.norm(desc) + 1e-10)
                global_descs[i] = desc
                
            except Exception as e:
                print(f"    Failed to compute fallback descriptor for {path}: {e}")
                # ëœë¤ descriptor
                global_descs[i] = np.random.randn(258).astype(np.float32)
                global_descs[i] = global_descs[i] / np.linalg.norm(global_descs[i])
        
        return global_descs
    
    def select_topk_pairs(self, global_descs):
        """ìƒìœ„ Kê°œ ì´ë¯¸ì§€ ìŒ ì„ íƒ"""
        print(f"  Selecting top-{self.top_k} pairs...")
        
        n_images = len(global_descs)
        if n_images < 2:
            return []
        
        # ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
        similarity_matrix = np.zeros((n_images, n_images))
        
        for i in range(n_images):
            for j in range(i+1, n_images):
                if i in global_descs and j in global_descs:
                    sim = np.dot(global_descs[i], global_descs[j])
                    similarity_matrix[i, j] = sim
                    similarity_matrix[j, i] = sim
        
        # ìƒìœ„ Kê°œ ìŒ ì„ íƒ
        pairs = set()
        
        for i in range(n_images):
            # ê° ì´ë¯¸ì§€ì— ëŒ€í•´ ê°€ì¥ ìœ ì‚¬í•œ Kê°œ ì´ë¯¸ì§€ ì„ íƒ
            similarities = similarity_matrix[i]
            top_indices = np.argsort(similarities)[::-1][:self.top_k]
            
            for j in top_indices:
                if i != j and similarities[j] > 0.1:  # ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’
                    pairs.add(tuple(sorted([i, j])))
        
        selected_pairs = list(pairs)
        print(f"    Selected {len(selected_pairs)} candidate pairs")
        
        return selected_pairs
    
    def compute_pair_similarity(self, cam_i, cam_j, global_descs):
        """ë‘ ì´ë¯¸ì§€ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°"""
        if cam_i in global_descs and cam_j in global_descs:
            return np.dot(global_descs[cam_i], global_descs[cam_j])
        return 0.0
    
    def filter_pairs_by_similarity(self, pairs, global_descs, min_similarity=0.1):
        """ìœ ì‚¬ë„ ê¸°ë°˜ ìŒ í•„í„°ë§"""
        filtered_pairs = []
        
        for cam_i, cam_j in pairs:
            similarity = self.compute_pair_similarity(cam_i, cam_j, global_descs)
            if similarity >= min_similarity:
                filtered_pairs.append((cam_i, cam_j))
        
        return filtered_pairs


class FeatureExtractor:
    def __init__(self, config, device, matching=None):
        self.config = config
        self.device = device
        self.matcher_type = config.get('matcher', 'superglue')
        self.matching = matching

    def extract(self, image):
        if self.matcher_type == 'superglue' and self.matching is not None:
            # SuperPoint íŠ¹ì§•ì  ì¶”ì¶œ
            from models.utils import frame2tensor
            inp = frame2tensor(image, self.device)
            with torch.no_grad():
                pred = self.matching.superpoint({'image': inp})
            return {
                'keypoints': pred['keypoints'][0].cpu().numpy(),
                'descriptors': pred['descriptors'][0].cpu().numpy(),
                'scores': pred['scores'][0].cpu().numpy(),
                'image_size': image.shape[:2]
            }
        else:
            # Fallback: OpenCV SIFT
            import cv2
            sift = cv2.SIFT_create()
            keypoints, descriptors = sift.detectAndCompute(image, None)
            if keypoints is None or descriptors is None:
                return None
            kpts = np.array([[kp.pt[0], kp.pt[1]] for kp in keypoints])
            scores = np.array([kp.response for kp in keypoints])
            return {
                'keypoints': kpts,
                'descriptors': descriptors.T.astype(np.float32),
                'scores': scores,
                'image_size': image.shape[:2]
            }

class Matcher:
    def __init__(self, config, device, matching=None):
        self.config = config
        self.device = device
        self.matcher_type = config.get('matcher', 'superglue')
        self.matching = matching

    def match(self, features1, features2):
        if self.matcher_type == 'superglue' and self.matching is not None:
            # SuperGlue ë§¤ì¹­
            data = {
                'image0': torch.zeros((1, 1, 480, 640)).to(self.device),
                'image1': torch.zeros((1, 1, 480, 640)).to(self.device),
                'keypoints0': torch.from_numpy(features1['keypoints']).unsqueeze(0).to(self.device),
                'keypoints1': torch.from_numpy(features2['keypoints']).unsqueeze(0).to(self.device),
                'descriptors0': torch.from_numpy(features1['descriptors']).unsqueeze(0).to(self.device),
                'descriptors1': torch.from_numpy(features2['descriptors']).unsqueeze(0).to(self.device),
                'scores0': torch.from_numpy(features1['scores']).unsqueeze(0).to(self.device),
                'scores1': torch.from_numpy(features2['scores']).unsqueeze(0).to(self.device),
            }
            with torch.no_grad():
                result = self.matching.superglue(data)
            indices0 = result['indices0'][0].cpu().numpy()
            indices1 = result['indices1'][0].cpu().numpy()
            mscores0 = result['matching_scores0'][0].cpu().numpy()
            matches = []
            for i, j in enumerate(indices0):
                if j >= 0 and mscores0[i] > 0.00001:
                    if j < len(indices1) and indices1[j] == i:
                        matches.append((i, j, mscores0[i]))
            return matches
        else:
            # Fallback: OpenCV BFMatcher
            import cv2
            bf = cv2.BFMatcher()
            matches = bf.knnMatch(features1['descriptors'].T, features2['descriptors'].T, k=2)
            good_matches = []
            for m, n in matches:
                if m.distance < 0.75 * n.distance:
                    good_matches.append((m.queryIdx, m.trainIdx, 1.0 - m.distance / 1000.0))
            return good_matches

class SuperGlue3DGSPipeline:
    """SuperGlue ê¸°ë°˜ ì™„ì „í•œ 3DGS SfM íŒŒì´í”„ë¼ì¸ (êµ¬ì¡°í™”/ë¦¬íŒ©í† ë§ ë²„ì „)"""
    def __init__(self, config=None, device='cuda'):
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.config = config or {}
        
        # SuperGlue ëª¨ë¸ ë¡œë“œ
        self.superglue_available = False
        try:
            from models.matching import Matching
            self.matching = Matching(self.config).eval().to(self.device)
            self.superglue_available = True
            print(f"âœ“ SuperGlue matching model loaded on {self.device}")
        except Exception as e:
            print(f"âš ï¸  SuperGlue model not available: {e}")
            print("   Will use fallback pose estimation methods")
            self.matching = None
            self.superglue_available = False
        
        # ê° ë‹¨ê³„ë³„ ê°ì²´ ìƒì„±
        self.feature_extractor = FeatureExtractor(self.config, self.device, self.matching)
        self.matcher = Matcher(self.config, self.device, self.matching)
        self.adaptive_matcher = AdaptiveMatcher(self.config, self.device)
        self.track_manager = TrackManager()
        self.pose_graph_optimizer = PoseGraphOptimizer()
        self.bundle_adjuster = BundleAdjuster(loss_type=self.config.get('ba_loss', 'huber'), max_iter=self.config.get('ba_max_iter', 50))
        self.parallel_executor = ParallelExecutor(max_workers=self.config.get('max_workers', 4))
        self.auto_tuner = AutoTuner(self.config)
        # ê¸°ì¡´ ë³€ìˆ˜ë“¤ ìœ ì§€
        self.cameras = {}
        self.points_3d = {}
        self.image_features = {}
        self.matches = {}
        self.camera_graph = defaultdict(list)
        self.point_observations = defaultdict(list)
        self.quality_metrics = {}
        
        print(f'âœ… SuperGlue 3DGS Pipeline initialized on {self.device}')
        if not self.superglue_available:
            print('   Running in fallback mode (SuperGlue not available)')
    
    def process_images_to_3dgs(self, image_dir, output_dir, max_images=120):
        """ì´ë¯¸ì§€ë“¤ì„ 3DGS í˜•ì‹ìœ¼ë¡œ ì²˜ë¦¬ (êµ¬ì¡°í™”/ë¦¬íŒ©í† ë§ ë²„ì „)"""
        # 1. ì´ë¯¸ì§€ ìˆ˜ì§‘
        image_paths = self._collect_images(image_dir, max_images)
        if not image_paths:
            raise RuntimeError("No images found")
        print(f"Found {len(image_paths)} images")

        # 2. íŠ¹ì§•ì  ì¶”ì¶œ (ë³‘ë ¬í™” ì˜ˆì‹œ)
        def extract_features_for_path(path):
            img = self._load_image(path)
            return self.feature_extractor.extract(img)
        features_list = self.parallel_executor.parallel_map(extract_features_for_path, image_paths)
        for i, feat in enumerate(features_list):
            self.image_features[i] = feat

        # 3. Adaptive Matching (global descriptor ê¸°ë°˜ í›„ë³´ ìŒ ì„ ì •)
        global_descs = self.adaptive_matcher.compute_global_descriptors(image_paths)
        candidate_pairs = self.adaptive_matcher.select_topk_pairs(global_descs)

        # 4. ë§¤ì¹­ (ë³‘ë ¬í™” ì˜ˆì‹œ)
        def match_pair(pair):
            i, j = pair
            return (pair, self.matcher.match(self.image_features[i], self.image_features[j]))
        match_results = self.parallel_executor.parallel_map(match_pair, candidate_pairs)
        for pair, matches in match_results:
            self.matches[pair] = matches

        # 5. Track ìƒì„± ë° Multi-view Triangulation
        self.track_manager.build_tracks(self.matches, self.image_features)
        self.track_manager.filter_tracks(min_views=3)
        triangulated_points = self.track_manager.triangulate_tracks(self.cameras, self.image_features)
        # triangulated_pointsë¥¼ self.points_3dì— ë°˜ì˜ (ì‹¤ì œ êµ¬í˜„ í•„ìš”)

        # 6. Pose Graph Optimization
        self.pose_graph_optimizer.build_pose_graph(self.matches, self.cameras)
        self.pose_graph_optimizer.remove_outlier_edges(min_matches=10)
        pose_tree = self.pose_graph_optimizer.get_spanning_tree()
        # pose_treeë¥¼ ì´ìš©í•´ global pose ì´ˆê¸°í™” (ì‹¤ì œ êµ¬í˜„ í•„ìš”)

        # 7. Bundle Adjustment
        ba_result = self.bundle_adjuster.run(self.cameras, self.points_3d, self.point_observations, callback=self.bundle_adjuster.monitor_quality)

        # 8. Auto-tuning (í’ˆì§ˆ ë©”íŠ¸ë¦­ í‰ê°€ ë° íŒŒë¼ë¯¸í„° ì¡°ì •)
        self.auto_tuner.evaluate_metrics(self.quality_metrics)
        self.auto_tuner.adjust_parameters()

        # 9. ê²°ê³¼ ì €ì¥ ë° ë§ˆë¬´ë¦¬
        scene_info = self._create_3dgs_scene_info(image_paths)
        self._save_3dgs_format(scene_info, output_dir)
        self._cleanup_memory()
        return scene_info
    
    def _monitor_memory(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ (psutil ì—†ì´)"""
        try:
            # ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ (psutil ì—†ì´)
            if torch.cuda.is_available():
                # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
                gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB
                print(f"    GPU Memory: {gpu_memory:.1f} MB")
            else:
                # CPU ë©”ëª¨ë¦¬ëŠ” ê°„ë‹¨í•œ ì¶”ì •
                print(f"    Memory monitoring: CPU mode")
        except:
            print(f"    Memory monitoring: Not available")
    
    def _cleanup_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    def _check_colmap_reconstruction(self, output_dir):
        """COLMAP reconstruction ì¡´ì¬ ì—¬ë¶€ ë° ìœ íš¨ì„± í™•ì¸"""
        try:
            reconstruction_path = Path(output_dir) / "sparse" / "0"
            cameras_bin = reconstruction_path / "cameras.bin"
            images_bin = reconstruction_path / "images.bin"
            points3d_bin = reconstruction_path / "points3D.bin"
            
            if not cameras_bin.exists() or not images_bin.exists():
                print(f"    No COLMAP reconstruction found at: {reconstruction_path}")
                return False
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            if cameras_bin.stat().st_size == 0 or images_bin.stat().st_size == 0:
                print(f"    COLMAP files are empty, removing corrupted files")
                self._cleanup_corrupted_colmap_files(reconstruction_path)
                return False
            
            # íŒŒì¼ í˜•ì‹ ê²€ì¦ ì‹œë„
            try:
                from scene.colmap_loader import read_intrinsics_binary, read_extrinsics_binary
                
                # cameras.bin ê²€ì¦
                cameras = read_intrinsics_binary(str(cameras_bin))
                if len(cameras) == 0:
                    print(f"    cameras.bin is empty or corrupted")
                    self._cleanup_corrupted_colmap_files(reconstruction_path)
                    return False
                
                # images.bin ê²€ì¦
                images = read_extrinsics_binary(str(images_bin))
                if len(images) == 0:
                    print(f"    images.bin is empty or corrupted")
                    self._cleanup_corrupted_colmap_files(reconstruction_path)
                    return False
                
                print(f"    Found valid COLMAP reconstruction at: {reconstruction_path}")
                print(f"    - {len(cameras)} cameras, {len(images)} images")
                return True
                
            except Exception as e:
                print(f"    COLMAP files are corrupted: {e}")
                self._cleanup_corrupted_colmap_files(reconstruction_path)
                return False
                
        except Exception as e:
            print(f"    COLMAP reconstruction check failed: {e}")
            return False
    
    def _cleanup_corrupted_colmap_files(self, reconstruction_path):
        """ì†ìƒëœ COLMAP íŒŒì¼ë“¤ ì •ë¦¬"""
        try:
            for file_name in ["cameras.bin", "images.bin", "points3D.bin"]:
                file_path = reconstruction_path / file_name
                if file_path.exists():
                    file_path.unlink()
                    print(f"    Deleted corrupted {file_name}")
        except Exception as e:
            print(f"    Failed to cleanup corrupted files: {e}")
    
    def _process_with_colmap_hybrid(self, image_paths, output_dir):
        """COLMAP reconstructionì„ í™œìš©í•œ í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬"""
        print("  ğŸ”„ Using COLMAP + SuperGlue hybrid approach")
        
        try:
            # 1. COLMAP reconstruction ë¡œë“œ
            colmap_data = self._load_colmap_reconstruction(output_dir)
            if colmap_data is None:
                print("    Failed to load COLMAP reconstruction, falling back to SuperGlue")
                return self._process_superglue_only(image_paths, output_dir)
            
            cameras, images, points3d = colmap_data
            print(f"    Loaded {len(cameras)} cameras, {len(images)} images, {len(points3d)} points from COLMAP")
            
            # 2. COLMAP ë°ì´í„°ë¥¼ SuperGlue í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            self._convert_colmap_to_superglue_format(cameras, images, points3d, image_paths)
            
            # 3. SuperGlue íŠ¹ì§•ì  ì¶”ì¶œ (COLMAP í¬ì¦ˆ ê°œì„ ìš©)
            self._extract_all_features(image_paths)
            
            # 4. COLMAP í¬ì¦ˆë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ê°œì„ ëœ ë§¤ì¹­
            self._intelligent_matching_with_colmap_poses()
            
            # 5. Bundle Adjustment (COLMAP ì´ˆê¸°ê°’ ì‚¬ìš©)
            self._bundle_adjustment_with_colmap_initialization()
            
            # 6. 3DGS SceneInfo ìƒì„±
            scene_info = self._create_3dgs_scene_info(image_paths)
            
            # 7. 3DGS í˜•ì‹ìœ¼ë¡œ ì €ì¥ (ì•ˆì „í•œ ë°©ë²•)
            try:
                self._save_3dgs_format(scene_info, output_dir)
            except Exception as save_error:
                print(f"    Warning: Failed to save 3DGS format: {save_error}")
                import traceback
                traceback.print_exc()
                # ê¸°ë³¸ í…ìŠ¤íŠ¸ íŒŒì¼ë§Œ ì €ì¥
                self._save_basic_format(scene_info, output_dir)
            
            return scene_info
            
        except Exception as e:
            print(f"    Hybrid processing failed: {e}")
            import traceback
            traceback.print_exc()
            print("    Falling back to SuperGlue only")
            return self._process_superglue_only(image_paths, output_dir)
    
    def _save_basic_format(self, scene_info, output_dir):
        """ê¸°ë³¸ í…ìŠ¤íŠ¸ íŒŒì¼ë§Œ ì €ì¥ (fallback)"""
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True, parents=True)
        
        sparse_dir = output_dir / "sparse" / "0"
        sparse_dir.mkdir(exist_ok=True, parents=True)
        
        images_dir = output_dir / "images"
        images_dir.mkdir(exist_ok=True)
        
        # í…ìŠ¤íŠ¸ íŒŒì¼ë§Œ ì €ì¥
        self._write_cameras_txt(scene_info.train_cameras + scene_info.test_cameras, 
                               sparse_dir / "cameras.txt")
        self._write_images_txt(scene_info.train_cameras + scene_info.test_cameras, 
                              sparse_dir / "images.txt")
        self._write_points3d_ply(scene_info.point_cloud, sparse_dir / "points3D.ply")
        
        print(f"    Saved basic format to {output_dir}")
    
    def _load_colmap_reconstruction(self, output_dir):
        """COLMAP reconstruction ë¡œë“œ"""
        try:
            reconstruction_path = Path(output_dir) / "sparse" / "0"
            print(f"    Checking reconstruction path: {reconstruction_path}")
            
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            cameras_bin = reconstruction_path / "cameras.bin"
            images_bin = reconstruction_path / "images.bin"
            points3d_bin = reconstruction_path / "points3D.bin"
            
            print(f"    cameras.bin exists: {cameras_bin.exists()}")
            print(f"    images.bin exists: {images_bin.exists()}")
            print(f"    points3D.bin exists: {points3d_bin.exists()}")
            
            if not cameras_bin.exists() or not images_bin.exists() or not points3d_bin.exists():
                print("    Missing required COLMAP files")
                return None
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            print(f"    cameras.bin size: {cameras_bin.stat().st_size} bytes")
            print(f"    images.bin size: {images_bin.stat().st_size} bytes")
            print(f"    points3D.bin size: {points3d_bin.stat().st_size} bytes")
            
            # COLMAP ëª¨ë“ˆ import
            try:
                from scene.colmap_loader import read_intrinsics_binary, read_extrinsics_binary, read_points3D_binary
                print("    COLMAP loader imported successfully")
            except ImportError as e:
                print(f"    COLMAP loader import failed: {e}")
                return None
            
            # ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„° ë¡œë“œ
            print("    Loading cameras.bin...")
            try:
                cameras = read_intrinsics_binary(str(cameras_bin))
                print(f"    Loaded {len(cameras)} cameras")
            except Exception as e:
                print(f"    Failed to load cameras.bin: {e}")
                return None
            
            # ì¹´ë©”ë¼ ì™¸ë¶€ íŒŒë¼ë¯¸í„° ë¡œë“œ
            print("    Loading images.bin...")
            try:
                images = read_extrinsics_binary(str(images_bin))
                print(f"    Loaded {len(images)} images")
            except Exception as e:
                print(f"    Failed to load images.bin: {e}")
                return None
            
            # 3D í¬ì¸íŠ¸ ë¡œë“œ
            print("    Loading points3D.bin...")
            try:
                points3d = read_points3D_binary(str(points3d_bin))
                print(f"    Loaded {len(points3d)} 3D points")
            except Exception as e:
                print(f"    Failed to load points3D.bin: {e}")
                return None
            
            return cameras, images, points3d
            
        except Exception as e:
            print(f"    Failed to load COLMAP reconstruction: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _convert_colmap_to_superglue_format(self, cameras, images, points3d, image_paths):
        """COLMAP ë°ì´í„°ë¥¼ SuperGlue í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        print("    Converting COLMAP data to SuperGlue format...")
        
        # ì¹´ë©”ë¼ í¬ì¦ˆ ì„¤ì •
        for image_id, image_data in images.items():
            if image_id < len(image_paths):
                # COLMAP í¬ì¦ˆë¥¼ SuperGlue í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                R = image_data.qvec2rotmat()  # ì¿¼í„°ë‹ˆì–¸ì„ íšŒì „ í–‰ë ¬ë¡œ
                T = image_data.tvec  # ì´ë™ ë²¡í„°
                
                # ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„°
                camera_id = image_data.camera_id
                if camera_id in cameras:
                    camera = cameras[camera_id]
                    K = self._colmap_camera_to_intrinsics(camera)
                else:
                    K = self._estimate_intrinsics(image_id)
                
                self.cameras[image_id] = {
                    'R': R.astype(np.float32),
                    'T': T.astype(np.float32),
                    'K': K,
                    'image_path': str(image_paths[image_id])
                }
        
        print(f"    Converted {len(self.cameras)} camera poses from COLMAP")
    
    def _colmap_camera_to_intrinsics(self, camera):
        """COLMAP ì¹´ë©”ë¼ë¥¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„°ë¡œ ë³€í™˜"""
        if camera.model == "PINHOLE":
            fx, fy, cx, cy = camera.params
            K = np.array([
                [fx, 0, cx],
                [0, fy, cy],
                [0, 0, 1]
            ], dtype=np.float32)
        else:
            # ê¸°ë³¸ PINHOLE ëª¨ë¸ ì‚¬ìš©
            width, height = camera.width, camera.height
            focal = max(width, height) * 0.9
            cx, cy = width / 2, height / 2
            K = np.array([
                [focal, 0, cx],
                [0, focal, cy],
                [0, 0, 1]
            ], dtype=np.float32)
        
        return K
    
    def _intelligent_matching_with_colmap_poses(self):
        """COLMAP í¬ì¦ˆë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ê°œì„ ëœ ë§¤ì¹­"""
        print("    Using COLMAP poses for improved matching...")
        
        # COLMAP í¬ì¦ˆê°€ ìˆìœ¼ë©´ ë” ì ê·¹ì ì¸ ë§¤ì¹­
        n_images = len(self.image_features)
        
        # ì „ì—­ descriptors ê³„ì‚°
        self._compute_global_descriptors()
        
        # COLMAP í¬ì¦ˆ ê¸°ë°˜ ë§¤ì¹­ (ë” ì ê·¹ì )
        for i in range(n_images):
            for j in range(i+1, n_images):
                if i in self.cameras and j in self.cameras:
                    # COLMAP í¬ì¦ˆ ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°
                    pose_similarity = self._compute_pose_similarity(i, j)
                    
                    if pose_similarity > 0.1:  # í¬ì¦ˆê°€ ìœ ì‚¬í•œ ê²½ìš°
                        matches = self._match_pair_superglue(i, j)
                        if len(matches) > 1:
                            self.matches[(i, j)] = matches
                            self.camera_graph[i].append(j)
                            self.camera_graph[j].append(i)
    
    def _compute_pose_similarity(self, cam_i, cam_j):
        """ë‘ ì¹´ë©”ë¼ í¬ì¦ˆ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            R_i, T_i = self.cameras[cam_i]['R'], self.cameras[cam_i]['T']
            R_j, T_j = self.cameras[cam_j]['R'], self.cameras[cam_j]['T']
            
            # íšŒì „ ì°¨ì´
            R_diff = R_i @ R_j.T
            rotation_error = np.arccos(np.clip((np.trace(R_diff) - 1) / 2, -1, 1))
            
            # ì´ë™ ì°¨ì´
            translation_error = np.linalg.norm(T_i - T_j)
            
            # ì¢…í•© ìœ ì‚¬ë„ (ì‘ì„ìˆ˜ë¡ ìœ ì‚¬)
            similarity = 1.0 / (1.0 + rotation_error + translation_error * 0.1)
            
            return similarity
            
        except Exception:
            return 0.0
    
    def _bundle_adjustment_with_colmap_initialization(self):
        """COLMAP ì´ˆê¸°ê°’ì„ ì‚¬ìš©í•œ Bundle Adjustment"""
        print("    Using COLMAP poses as initialization for Bundle Adjustment...")
        
        # COLMAP í¬ì¦ˆê°€ ì´ë¯¸ ìˆìœ¼ë¯€ë¡œ ë” ë³´ìˆ˜ì ì¸ BA
        self._bundle_adjustment_robust(max_iterations=30)  # ë°˜ë³µ íšŸìˆ˜ ì¤„ì„
    
    def _process_superglue_only(self, image_paths, output_dir):
        """SuperGlueë§Œ ì‚¬ìš©í•œ ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹)"""
        print("    Using SuperGlue-only processing...")
        
        # íŠ¹ì§•ì  ì¶”ì¶œ
        self._extract_all_features(image_paths)
        
        # ë§¤ì¹­
        self._intelligent_matching()
        
        # ì¹´ë©”ë¼ í¬ì¦ˆ ì¶”ì •
        self._estimate_camera_poses_robust()
        
        # ì‚¼ê°ì¸¡ëŸ‰
        n_points = self._triangulate_all_points_robust()
        
        # Bundle Adjustment
        self._bundle_adjustment_robust()
        
        # í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
        self._compute_quality_metrics()
        
        # 3DGS SceneInfo ìƒì„±
        scene_info = self._create_3dgs_scene_info(image_paths)
        
        # 3DGS í˜•ì‹ìœ¼ë¡œ ì €ì¥
        self._save_3dgs_format(scene_info, output_dir)
        
        return scene_info
    
    def _compute_quality_metrics(self):
        """í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        try:
            # í¬ì¦ˆ ì¶”ì • ì„±ê³µë¥ 
            total_cameras = len(self.image_features)
            estimated_cameras = len([cam for cam in self.cameras.values() if 'R' in cam])
            self.quality_metrics['pose_estimation_success_rate'] = estimated_cameras / total_cameras
            
            # í‰ê·  ë§¤ì¹­ ìˆ˜
            if self.matches:
                avg_matches = np.mean([len(matches) for matches in self.matches.values()])
                self.quality_metrics['average_matches_per_pair'] = avg_matches
            
            # ì²˜ë¦¬ ì‹œê°„
            self.quality_metrics['total_processing_time'] = time.time() - self.start_time
            
            print(f"\n=== Quality Metrics ===")
            print(f"Pose estimation success rate: {self.quality_metrics['pose_estimation_success_rate']:.2%}")
            print(f"Average matches per pair: {self.quality_metrics['average_matches_per_pair']:.1f}")
            print(f"Total processing time: {self.quality_metrics['total_processing_time']:.1f}s")
            
            # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ê°€ëŠ¥í•œ ê²½ìš°)
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024
                print(f"GPU Memory usage: {gpu_memory:.1f} MB")
            
        except Exception as e:
            print(f"Quality metrics calculation failed: {e}")
    
    def _create_fallback_scene_info(self, image_paths):
        """ê°œì„ ëœ fallback scene ìƒì„±"""
        try:
            # Lazy import 3DGS modules
            CameraInfo, SceneInfo, BasicPointCloud = get_3dgs_imports()
            if CameraInfo is None:
                raise ImportError("3DGS modules not available")
            
            print(f"ğŸ“¸ Creating fallback scene for {len(image_paths)} images")
            
            # ì¹´ë©”ë¼ ì •ë³´ ìƒì„±
            cam_infos = []
            for i, image_path in enumerate(image_paths):
                try:
                    # ì´ë¯¸ì§€ í¬ê¸° í™•ì¸
                    image = Image.open(image_path)
                    width, height = image.size
                    
                    # ì›í˜• ë°°ì¹˜ë¡œ ì¹´ë©”ë¼ ë°°ì¹˜
                    angle = i * (2 * np.pi / len(image_paths))
                    radius = 3.0
                    
                    # ì¹´ë©”ë¼ í¬ì¦ˆ (ì›ì„ ë°”ë¼ë³´ë„ë¡)
                    camera_pos = np.array([
                        radius * np.cos(angle),
                        0.0,  # ë†’ì´ ê³ ì •
                        radius * np.sin(angle)
                    ])
                    
                    # ì›ì ì„ í–¥í•˜ëŠ” ë°©í–¥
                    look_at = np.array([0.0, 0.0, 0.0])
                    up = np.array([0.0, 1.0, 0.0])
                    
                    # ì¹´ë©”ë¼ íšŒì „ í–‰ë ¬ ê³„ì‚°
                    forward = look_at - camera_pos
                    forward = forward / np.linalg.norm(forward)
                    right = np.cross(forward, up)
                    right = right / np.linalg.norm(right)
                    up = np.cross(right, forward)
                    
                    R = np.array([right, up, -forward]).T  # OpenCV ì»¨ë²¤ì…˜
                    T = camera_pos
                    
                    # FOV ê³„ì‚° (ë” ì•ˆì „í•œ ê°’ë“¤)
                    focal_length = max(width, height) * 0.8
                    FovX = 2 * np.arctan(width / (2 * focal_length))
                    FovY = 2 * np.arctan(height / (2 * focal_length))
                    
                    # í…ŒìŠ¤íŠ¸ ì¹´ë©”ë¼ ì„ íƒ (ë” ê· ë“±í•˜ê²Œ ë¶„ì‚°)
                    is_test = (i % 8 == 0)  # 8ê°œë§ˆë‹¤ 1ê°œì”© í…ŒìŠ¤íŠ¸
                    
                    cam_info = CameraInfo(
                        uid=i,
                        R=R.astype(np.float32),
                        T=T.astype(np.float32),
                        FovY=float(FovY),
                        FovX=float(FovX),
                        image_path=str(image_path),
                        image_name=Path(image_path).name,
                        width=int(width),
                        height=int(height),
                        depth_params=None,
                        depth_path="",
                        is_test=is_test
                    )
                    cam_infos.append(cam_info)
                    
                except Exception as e:
                    print(f"    Warning: Failed to process {image_path}: {e}")
                    continue
            
            if not cam_infos:
                raise ValueError("No valid cameras created")
            
            # ê°œì„ ëœ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ìƒì„±
            n_points = 12000  # 8000 â†’ 12000ë¡œ ì¦ê°€
            
            # ë” í˜„ì‹¤ì ì¸ 3D í¬ì¸íŠ¸ ë¶„í¬
            # êµ¬í˜• ë¶„í¬ + ì¼ë¶€ í‰ë©´ êµ¬ì¡°
            points_sphere = np.random.randn(n_points // 2, 3).astype(np.float32)
            points_sphere = points_sphere / np.linalg.norm(points_sphere, axis=1, keepdims=True) * 3.0  # 2.0 â†’ 3.0
            
            # í‰ë©´ êµ¬ì¡° ì¶”ê°€ (ë°”ë‹¥ë©´)
            points_plane = np.random.randn(n_points // 2, 3).astype(np.float32)
            points_plane[:, 1] = np.abs(points_plane[:, 1]) * 0.2 - 1.0  # ë°”ë‹¥ ê·¼ì²˜ (0.1 â†’ 0.2, -0.5 â†’ -1.0)
            points_plane[:, [0, 2]] *= 2.0  # 1.5 â†’ 2.0
            
            points = np.vstack([points_sphere, points_plane])
            
            # ë” í˜„ì‹¤ì ì¸ ìƒ‰ìƒ (íšŒìƒ‰ì¡° + ì•½ê°„ì˜ ìƒ‰ìƒ)
            colors = np.random.rand(n_points, 3).astype(np.float32)
            colors = colors * 0.5 + 0.3  # 0.3-0.8 ë²”ìœ„
            
            # ë²•ì„  ë²¡í„° (ë¬´ì‘ìœ„ì§€ë§Œ ì •ê·œí™”ë¨)
            normals = np.random.randn(n_points, 3).astype(np.float32)
            normals = normals / (np.linalg.norm(normals, axis=1, keepdims=True) + 1e-10)
            
            # BasicPointCloud ìƒì„± ì‹œ ì°¨ì› í™•ì¸
            assert points.shape == (n_points, 3), f"Points shape error: {points.shape}"
            assert colors.shape == (n_points, 3), f"Colors shape error: {colors.shape}"
            assert normals.shape == (n_points, 3), f"Normals shape error: {normals.shape}"
            
            # BasicPointCloudê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ fallback
            try:
                pcd = BasicPointCloud(
                    points=points,
                    colors=colors,
                    normals=normals
                )
            except NameError:
                # BasicPointCloudê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ê°„ë‹¨í•œ í´ë˜ìŠ¤ ìƒì„±
                class BasicPointCloud:
                    def __init__(self, points, colors, normals):
                        self.points = points
                        self.colors = colors
                        self.normals = normals
                
                pcd = BasicPointCloud(
                    points=points,
                    colors=colors,
                    normals=normals
                )
            
            # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
            train_cams = [c for c in cam_infos if not c.is_test]
            test_cams = [c for c in cam_infos if c.is_test]
            
            # NeRF ì •ê·œí™” (ê°œì„ ëœ ë²„ì „)
            if train_cams:
                camera_centers = []
                for cam in train_cams:
                    # ì¹´ë©”ë¼ ì¤‘ì‹¬ ê³„ì‚°
                    center = -cam.R.T @ cam.T
                    camera_centers.append(center)
                
                camera_centers = np.array(camera_centers)
                scene_center = np.mean(camera_centers, axis=0)
                distances = np.linalg.norm(camera_centers - scene_center, axis=1)
                scene_radius = np.max(distances) * 1.2
                
                # ìµœì†Œ/ìµœëŒ€ ì œí•œ
                scene_radius = max(scene_radius, 1.0)
                scene_radius = min(scene_radius, 10.0)
            else:
                scene_center = np.zeros(3)
                scene_radius = 3.0
            
            nerf_normalization = {
                "translate": -scene_center.astype(np.float32),
                "radius": float(scene_radius)
            }
            
            scene_info = SceneInfo(
                point_cloud=pcd,
                train_cameras=train_cams,
                test_cameras=test_cams,
                nerf_normalization=nerf_normalization,
                ply_path="",
                is_nerf_synthetic=False
            )
            
            print(f"âœ“ Fallback scene created:")
            print(f"  - {len(train_cams)} training cameras")
            print(f"  - {len(test_cams)} test cameras")
            print(f"  - {n_points} 3D points")
            print(f"  - Scene radius: {scene_radius:.2f}")
            
            return scene_info
            
        except Exception as e:
            print(f"Failed to create fallback scene: {e}")
            raise
    
    def _collect_images(self, image_dir, max_images):
        """ì´ë¯¸ì§€ ìˆ˜ì§‘ ë° ì •ë ¬"""
        image_dir = Path(image_dir)
        image_paths = []
        
        # ì§€ì›í•˜ëŠ” í™•ì¥ì
        extensions = ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']
        for ext in extensions:
            image_paths.extend(list(image_dir.glob(ext)))
        
        # ì •ë ¬ ë° ì œí•œ
        image_paths.sort()
        return image_paths[:max_images]
    
    def _extract_all_features(self, image_paths):
        """ëª¨ë“  ì´ë¯¸ì§€ì—ì„œ SuperPoint íŠ¹ì§•ì  ì¶”ì¶œ (ìˆ˜ì •ëœ ë²„ì „)"""
        if not self.superglue_available:
            print("  Using fallback feature extraction (SuperGlue not available)")
            return self._extract_features_fallback(image_paths)
        
        for i, image_path in enumerate(image_paths):
            print(f"  {i+1:3d}/{len(image_paths)}: {image_path.name}")
            
            # ì´ë¯¸ì§€ ë¡œë“œ
            image = self._load_image(image_path)
            if image is None:
                continue
            
            # SuperPoint íŠ¹ì§•ì  ì¶”ì¶œ
            inp = frame2tensor(image, self.device)
            with torch.no_grad():
                pred = self.matching.superpoint({'image': inp})
            
            # ê²°ê³¼ ì €ì¥ - ëª¨ë“  í•„ìš”í•œ í‚¤ í¬í•¨
            self.image_features[i] = {
                'keypoints': pred['keypoints'][0].cpu().numpy(),
                'descriptors': pred['descriptors'][0].cpu().numpy(), 
                'scores': pred['scores'][0].cpu().numpy(),
                'image_path': str(image_path),
                'image_size': image.shape[:2]  # (H, W)
            }
            
            print(f"    Keypoints: {self.image_features[i]['keypoints'].shape[0]}")
            
        print(f"  Extracted features from {len(self.image_features)} images")
    
    def _extract_features_fallback(self, image_paths):
        """SuperGlueê°€ ì—†ì„ ë•Œ ì‚¬ìš©í•˜ëŠ” fallback íŠ¹ì§•ì  ì¶”ì¶œ"""
        print("  Using OpenCV SIFT for feature extraction")
        
        try:
            import cv2
            sift = cv2.SIFT_create()
        except ImportError:
            print("  OpenCV not available, using random features")
            return self._extract_random_features(image_paths)
        
        for i, image_path in enumerate(image_paths):
            print(f"  {i+1:3d}/{len(image_paths)}: {image_path.name}")
            
            try:
                # ì´ë¯¸ì§€ ë¡œë“œ
                image = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)
                if image is None:
                    continue
                
                # SIFT íŠ¹ì§•ì  ì¶”ì¶œ
                keypoints, descriptors = sift.detectAndCompute(image, None)
                
                if keypoints is None or descriptors is None:
                    continue
                
                # ê²°ê³¼ë¥¼ SuperPoint í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                kpts = np.array([[kp.pt[0], kp.pt[1]] for kp in keypoints])
                scores = np.array([kp.response for kp in keypoints])
                
                # descriptorë¥¼ float32ë¡œ ë³€í™˜
                desc = descriptors.astype(np.float32)
                
                self.image_features[i] = {
                    'keypoints': kpts,
                    'descriptors': desc.T,  # SuperPoint í˜•ì‹ì— ë§ì¶¤
                    'scores': scores,
                    'image_path': str(image_path),
                    'image_size': image.shape[:2]
                }
                
                print(f"    Keypoints: {len(keypoints)}")
                
            except Exception as e:
                print(f"    Error processing {image_path.name}: {e}")
                continue
        
        print(f"  Extracted features from {len(self.image_features)} images (fallback)")
    
    def _extract_random_features(self, image_paths):
        """ëª¨ë“  ë°©ë²•ì´ ì‹¤íŒ¨í–ˆì„ ë•Œ ì‚¬ìš©í•˜ëŠ” ëœë¤ íŠ¹ì§•ì """
        print("  Using random features (no feature extraction available)")
        
        for i, image_path in enumerate(image_paths):
            print(f"  {i+1:3d}/{len(image_paths)}: {image_path.name}")
            
            try:
                # ì´ë¯¸ì§€ í¬ê¸° í™•ì¸
                from PIL import Image
                img = Image.open(image_path)
                width, height = img.size
                
                # ëœë¤ íŠ¹ì§•ì  ìƒì„±
                n_keypoints = 1000
                kpts = np.random.rand(n_keypoints, 2)
                kpts[:, 0] *= width
                kpts[:, 1] *= height
                
                # ëœë¤ descriptor (128ì°¨ì›)
                desc = np.random.randn(128, n_keypoints).astype(np.float32)
                
                # ëœë¤ scores
                scores = np.random.rand(n_keypoints).astype(np.float32)
                
                self.image_features[i] = {
                    'keypoints': kpts,
                    'descriptors': desc,
                    'scores': scores,
                    'image_path': str(image_path),
                    'image_size': (height, width)
                }
                
                print(f"    Random keypoints: {n_keypoints}")
                
            except Exception as e:
                print(f"    Error processing {image_path.name}: {e}")
                continue
        
        print(f"  Generated random features for {len(self.image_features)} images")
    
    def _intelligent_matching(self, max_pairs=3000):
        """ì§€ëŠ¥ì  ì´ë¯¸ì§€ ë§¤ì¹­ (ê·¹ë„ë¡œ ì™„í™”ëœ ë²„ì „)"""
        n_images = len(self.image_features)
        
        # ì „ì—­ descriptors ê³„ì‚°
        self._compute_global_descriptors()
        
        print(f"  Starting intelligent matching for {n_images} images...")
        
        # 1. ìˆœì°¨ì  ë§¤ì¹­ - ëª¨ë“  ì´ë¯¸ì§€ë¥¼ í•œ ë°”í€´ ëŒê¸°
        sequential_count = 0
        for i in range(n_images):
            # ë‹¤ìŒ ì´ë¯¸ì§€ (ë§ˆì§€ë§‰ ì´ë¯¸ì§€ëŠ” ì²« ë²ˆì§¸ì™€ ì—°ê²°)
            next_i = (i + 1) % n_images
            
            matches = self._match_pair_superglue(i, next_i)
            if len(matches) > 1:  # 2 â†’ 1ë¡œ ê·¹ë„ë¡œ ì™„í™”
                self.matches[(i, next_i)] = matches
                self.camera_graph[i].append(next_i)
                self.camera_graph[next_i].append(i)
                sequential_count += 1
        
        print(f"    Sequential pairs: {sequential_count}")
        
        # 2. ìœ ì‚¬ë„ ê¸°ë°˜ ë§¤ì¹­ (ë” ì ê·¹ì ìœ¼ë¡œ)
        similarity_count = self._similarity_based_matching_very_relaxed(max_pairs)
        print(f"    Similarity pairs: {similarity_count}")
        
        # 3. Loop closure ë§¤ì¹­ (ë” ì ê·¹ì ìœ¼ë¡œ)
        loop_count = self._loop_closure_matching_very_relaxed()
        print(f"    Loop closure pairs: {loop_count}")
        
        # 4. ğŸ”§ NEW: ê·¸ë¦¬ë“œ ê¸°ë°˜ ë§¤ì¹­ (ì—°ì†ëœ ì´ë¯¸ì§€ë“¤ ê°„ì˜ ì—°ê²°)
        grid_count = self._grid_based_matching_very_relaxed()
        print(f"    Grid-based pairs: {grid_count}")
        
        # 5. ğŸ”§ NEW: ëœë¤ ìƒ˜í”Œë§ ë§¤ì¹­ (ì—°ê²°ë˜ì§€ ì•Šì€ ì¹´ë©”ë¼ë“¤ì„ ìœ„í•œ fallback)
        random_count = self._random_sampling_matching_very_relaxed(max_pairs)
        print(f"    Random sampling pairs: {random_count}")
        
        print(f"  Total matching pairs: {len(self.matches)}")
        
        # ğŸ”§ NEW: ì—°ê²°ì„± ë¶„ì„ ë° ê°œì„ 
        self._analyze_and_improve_connectivity_very_relaxed()

    def _similarity_based_matching_very_relaxed(self, max_pairs):
        """ê·¹ë„ë¡œ ì™„í™”ëœ ìœ ì‚¬ë„ ê¸°ë°˜ ë§¤ì¹­"""
        # ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
        n_images = len(self.global_descriptors)
        similarity_matrix = np.zeros((n_images, n_images))
        
        for i in range(n_images):
            for j in range(i+1, n_images):
                if i in self.global_descriptors and j in self.global_descriptors:
                    sim = np.dot(self.global_descriptors[i], self.global_descriptors[j])
                    similarity_matrix[i, j] = sim
                    similarity_matrix[j, i] = sim
        
        # ìœ ì‚¬í•œ ì´ë¯¸ì§€ë“¤ ë§¤ì¹­ (ê·¹ë„ë¡œ ì ê·¹ì ìœ¼ë¡œ)
        similarity_count = 0
        for cam_id in range(n_images):
            # ìœ ì‚¬ë„ ë†’ì€ ìƒìœ„ 30ê°œ ì„ íƒ (20 â†’ 30ìœ¼ë¡œ ì¦ê°€)
            similarities = similarity_matrix[cam_id]
            candidates = np.argsort(similarities)[::-1]
            candidates = [c for c in candidates if c != cam_id and similarities[c] > 0.01][:30]  # 0.05 â†’ 0.01ë¡œ ê·¹ë„ë¡œ ì™„í™”
            
            for candidate in candidates:
                pair_key = (min(cam_id, candidate), max(cam_id, candidate))
                if pair_key in self.matches:
                    continue
                
                matches = self._match_pair_superglue(cam_id, candidate)
                if len(matches) > 1:  # 2 â†’ 1ë¡œ ê·¹ë„ë¡œ ì™„í™”
                    self.matches[pair_key] = matches
                    self.camera_graph[cam_id].append(candidate)
                    self.camera_graph[candidate].append(cam_id)
                    similarity_count += 1
                
                if len(self.matches) >= max_pairs:
                    return similarity_count
        
        return similarity_count

    def _loop_closure_matching_very_relaxed(self):
        """ê·¹ë„ë¡œ ì™„í™”ëœ Loop closure ë§¤ì¹­"""
        n_images = len(self.image_features)
        loop_count = 0
        
        # ë” ë„“ì€ ë²”ìœ„ì—ì„œ loop closure ì‹œë„
        for i in range(min(20, n_images//3)):  # 15 â†’ 20ìœ¼ë¡œ ì¦ê°€
            for j in range(max(n_images-20, 2*n_images//3), n_images):  # 15 â†’ 20ìœ¼ë¡œ ì¦ê°€
                if i >= j:
                    continue
                
                pair_key = (i, j)
                if pair_key in self.matches:
                    continue
                
                # ì „ì—­ ìœ ì‚¬ë„ ì²´í¬ (ê·¹ë„ë¡œ ì™„í™”ëœ ì¡°ê±´)
                if hasattr(self, 'global_descriptors') and i in self.global_descriptors and j in self.global_descriptors:
                    sim = np.dot(self.global_descriptors[i], self.global_descriptors[j])
                    if sim > 0.05:  # 0.1 â†’ 0.05ë¡œ ê·¹ë„ë¡œ ì™„í™”
                        matches = self._match_pair_superglue(i, j)
                        if len(matches) > 1:  # 2 â†’ 1ë¡œ ê·¹ë„ë¡œ ì™„í™”
                            self.matches[pair_key] = matches
                            self.camera_graph[i].append(j)
                            self.camera_graph[j].append(i)
                            loop_count += 1
        
        return loop_count

    def _grid_based_matching_very_relaxed(self):
        """ê·¹ë„ë¡œ ì™„í™”ëœ ê·¸ë¦¬ë“œ ê¸°ë°˜ ë§¤ì¹­"""
        n_images = len(self.image_features)
        grid_count = 0
        
        # ì—°ì†ëœ ì´ë¯¸ì§€ë“¤ ê°„ì˜ ì¶”ê°€ ì—°ê²°
        for i in range(n_images - 1):
            # ì¸ì ‘í•œ ì´ë¯¸ì§€ë“¤
            for offset in [1, 2, 3, 4, 5]:  # 1, 2, 3 â†’ 1, 2, 3, 4, 5ë¡œ ì¦ê°€
                j = i + offset
                if j >= n_images:
                    continue
                
                pair_key = (i, j)
                if pair_key in self.matches:
                    continue
                
                matches = self._match_pair_superglue(i, j)
                if len(matches) > 1:  # 1 â†’ 1ë¡œ ìœ ì§€ (ì´ë¯¸ ìµœì†Œê°’)
                    self.matches[pair_key] = matches
                    self.camera_graph[i].append(j)
                    self.camera_graph[j].append(i)
                    grid_count += 1
        
        return grid_count

    def _random_sampling_matching_very_relaxed(self, max_pairs):
        """ê·¹ë„ë¡œ ì™„í™”ëœ ëœë¤ ìƒ˜í”Œë§ ë§¤ì¹­"""
        n_images = len(self.image_features)
        random_count = 0
        
        # ì—°ê²°ë˜ì§€ ì•Šì€ ì¹´ë©”ë¼ë“¤ì„ ì°¾ê¸°
        unconnected_cameras = []
        for cam_id in range(n_images):
            if len(self.camera_graph[cam_id]) == 0:
                unconnected_cameras.append(cam_id)
        
        print(f"    Found {len(unconnected_cameras)} unconnected cameras")
        
        # ì—°ê²°ë˜ì§€ ì•Šì€ ì¹´ë©”ë¼ë“¤ì— ëŒ€í•´ ëœë¤ ë§¤ì¹­ ì‹œë„
        for cam_id in unconnected_cameras:
            # ë‹¤ë¥¸ ëª¨ë“  ì¹´ë©”ë¼ì™€ ë§¤ì¹­ ì‹œë„
            for other_cam in range(n_images):
                if cam_id == other_cam:
                    continue
                
                pair_key = (min(cam_id, other_cam), max(cam_id, other_cam))
                if pair_key in self.matches:
                    continue
                
                matches = self._match_pair_superglue(cam_id, other_cam)
                if len(matches) > 1:  # 1 â†’ 1ë¡œ ìœ ì§€ (ì´ë¯¸ ìµœì†Œê°’)
                    self.matches[pair_key] = matches
                    self.camera_graph[cam_id].append(other_cam)
                    self.camera_graph[other_cam].append(cam_id)
                    random_count += 1
                    break  # í•˜ë‚˜ë¼ë„ ì—°ê²°ë˜ë©´ ë‹¤ìŒ ì¹´ë©”ë¼ë¡œ
        
        return random_count

    def _analyze_and_improve_connectivity_very_relaxed(self):
        """ê·¹ë„ë¡œ ì™„í™”ëœ ì—°ê²°ì„± ë¶„ì„ ë° ê°œì„ """
        n_images = len(self.image_features)
        
        # ì—°ê²°ì„± ë¶„ì„
        connected_cameras = []
        isolated_cameras = []
        
        for cam_id in range(n_images):
            if len(self.camera_graph[cam_id]) > 0:
                connected_cameras.append(cam_id)
            else:
                isolated_cameras.append(cam_id)
        
        print(f"    Connectivity analysis:")
        print(f"      Connected cameras: {len(connected_cameras)}")
        print(f"      Isolated cameras: {len(isolated_cameras)}")
        
        # ì—°ê²°ë˜ì§€ ì•Šì€ ì¹´ë©”ë¼ë“¤ì„ ì—°ê²°ëœ ì¹´ë©”ë¼ì™€ ì—°ê²° ì‹œë„
        if len(connected_cameras) > 0 and len(isolated_cameras) > 0:
            print(f"    Attempting to connect {len(isolated_cameras)} isolated cameras...")
            
            for isolated_cam in isolated_cameras:
                # ê°€ì¥ ê°€ê¹Œìš´ ì—°ê²°ëœ ì¹´ë©”ë¼ ì°¾ê¸°
                best_connection = None
                best_similarity = -1
                
                for connected_cam in connected_cameras:
                    if hasattr(self, 'global_descriptors'):
                        if isolated_cam in self.global_descriptors and connected_cam in self.global_descriptors:
                            sim = np.dot(self.global_descriptors[isolated_cam], self.global_descriptors[connected_cam])
                            if sim > best_similarity:
                                best_similarity = sim
                                best_connection = connected_cam
                
                if best_connection is not None:
                    # ë§¤ì¹­ ì‹œë„
                    matches = self._match_pair_superglue(isolated_cam, best_connection)
                    if len(matches) > 1:  # 1 â†’ 1ë¡œ ìœ ì§€ (ì´ë¯¸ ìµœì†Œê°’)
                        pair_key = (min(isolated_cam, best_connection), max(isolated_cam, best_connection))
                        self.matches[pair_key] = matches
                        self.camera_graph[isolated_cam].append(best_connection)
                        self.camera_graph[best_connection].append(isolated_cam)
                        print(f"      Connected camera {isolated_cam} to {best_connection}")

    def _compute_global_descriptors(self):
        """ì „ì—­ ì´ë¯¸ì§€ descriptor ê³„ì‚° (NEW METHOD)"""
        self.global_descriptors = {}
        
        for cam_id, features in self.image_features.items():
            descriptors = features['descriptors']  # (256, N)
            scores = features['scores']
            
            if len(scores) > 0:
                # Scoreë¡œ ê°€ì¤‘í‰ê· í•˜ì—¬ ì „ì—­ descriptor ê³„ì‚°
                weights = scores / (scores.sum() + 1e-10)
                global_desc = np.average(descriptors.T, weights=weights, axis=0)
                global_desc = global_desc / (np.linalg.norm(global_desc) + 1e-10)
                self.global_descriptors[cam_id] = global_desc
            else:
                self.global_descriptors[cam_id] = np.zeros(256)
                
    def _similarity_based_matching(self, max_pairs):
        """ìœ ì‚¬ë„ ê¸°ë°˜ ë§¤ì¹­ (NEW METHOD)"""
        # ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
        n_images = len(self.global_descriptors)
        similarity_matrix = np.zeros((n_images, n_images))
        
        for i in range(n_images):
            for j in range(i+1, n_images):
                if i in self.global_descriptors and j in self.global_descriptors:
                    sim = np.dot(self.global_descriptors[i], self.global_descriptors[j])
                    similarity_matrix[i, j] = sim
                    similarity_matrix[j, i] = sim
        
        # ìœ ì‚¬í•œ ì´ë¯¸ì§€ë“¤ ë§¤ì¹­
        similarity_count = 0
        for cam_id in range(n_images):
            # ìœ ì‚¬ë„ ë†’ì€ ìƒìœ„ 12ê°œ ì„ íƒ (8 â†’ 12ë¡œ ì¦ê°€)
            similarities = similarity_matrix[cam_id]
            candidates = np.argsort(similarities)[::-1]
            candidates = [c for c in candidates if c != cam_id and similarities[c] > 0.2][:12]  # 0.3 â†’ 0.2, 8 â†’ 12
            
            for candidate in candidates:
                pair_key = (min(cam_id, candidate), max(cam_id, candidate))
                if pair_key in self.matches:
                    continue
                
                matches = self._match_pair_superglue(cam_id, candidate)
                if len(matches) > 10:  # 15 â†’ 10ìœ¼ë¡œ ì™„í™”
                    self.matches[pair_key] = matches
                    self.camera_graph[cam_id].append(candidate)
                    self.camera_graph[candidate].append(cam_id)
                    similarity_count += 1
                
                if len(self.matches) >= max_pairs:
                    return similarity_count
        
        return similarity_count

    def _loop_closure_matching(self):
        """Loop closure ë§¤ì¹­ (NEW METHOD)"""
        n_images = len(self.image_features)
        loop_count = 0
        
        # ì²« ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ ëª‡ ê°œ ì´ë¯¸ì§€ ê°„ ë§¤ì¹­
        for i in range(min(8, n_images//3)):  # 5 â†’ 8ë¡œ ì¦ê°€
            for j in range(max(n_images-8, 2*n_images//3), n_images):  # 5 â†’ 8ë¡œ ì¦ê°€
                if i >= j:
                    continue
                
                pair_key = (i, j)
                if pair_key in self.matches:
                    continue
                
                # ì „ì—­ ìœ ì‚¬ë„ ì²´í¬
                if hasattr(self, 'global_descriptors') and i in self.global_descriptors and j in self.global_descriptors:
                    sim = np.dot(self.global_descriptors[i], self.global_descriptors[j])
                    if sim > 0.3:  # 0.4 â†’ 0.3ìœ¼ë¡œ ì™„í™”
                        matches = self._match_pair_superglue(i, j)
                        if len(matches) > 15:  # 20 â†’ 15ë¡œ ì™„í™”
                            self.matches[pair_key] = matches
                            self.camera_graph[i].append(j)
                            self.camera_graph[j].append(i)
                            loop_count += 1
        
        return loop_count
    
    def _filter_low_quality_matches_very_relaxed(self):
        """ë§¤ìš° ì™„í™”ëœ ë‚®ì€ í’ˆì§ˆì˜ ë§¤ì¹­ í•„í„°ë§"""
        pairs_to_remove = []
        
        for (cam_i, cam_j), matches in self.matches.items():
            if len(matches) < 2:  # ë” ë‚®ì€ ì„ê³„ê°’ (3 â†’ 2)
                pairs_to_remove.append((cam_i, cam_j))
                continue
            
            # ë§¤ì¹­ í’ˆì§ˆ ë¶„ì„
            confidences = [conf for _, _, conf in matches]
            avg_confidence = np.mean(confidences)
            
            if avg_confidence < 0.001:  # ë” ë‚®ì€ ì„ê³„ê°’ (0.1 â†’ 0.001)
                pairs_to_remove.append((cam_i, cam_j))
                continue
            
            # ë§¤ì¹­ ë¶„í¬ ë¶„ì„ (ë” ì™„í™”ëœ ì¡°ê±´)
            if self._has_poor_matching_distribution_very_relaxed(cam_i, cam_j, matches):
                pairs_to_remove.append((cam_i, cam_j))
        
        # í•„í„°ë§ëœ ë§¤ì¹­ ì œê±°
        for pair in pairs_to_remove:
            cam_i, cam_j = pair
            del self.matches[pair]
            
            # ê·¸ë˜í”„ì—ì„œë„ ì œê±°
            if cam_j in self.camera_graph[cam_i]:
                self.camera_graph[cam_i].remove(cam_j)
            if cam_i in self.camera_graph[cam_j]:
                self.camera_graph[cam_j].remove(cam_i)
        
        print(f"  Filtered out {len(pairs_to_remove)} low-quality matches (very relaxed)")
    
    def _has_poor_matching_distribution_very_relaxed(self, cam_i, cam_j, matches):
        """ë§¤ìš° ì™„í™”ëœ ë§¤ì¹­ ë¶„í¬ ê²€ì‚¬"""
        kpts_i = self.image_features[cam_i]['keypoints']
        kpts_j = self.image_features[cam_j]['keypoints']
        
        # ì¸ë±ìŠ¤ ë²”ìœ„ ê²€ì¦
        valid_matches = []
        for idx_i, idx_j, conf in matches:
            if (idx_i < len(kpts_i) and idx_j < len(kpts_j) and 
                idx_i >= 0 and idx_j >= 0):
                valid_matches.append((idx_i, idx_j, conf))
        
        if len(valid_matches) < 1:  # ë” ë‚®ì€ ì„ê³„ê°’ (2 â†’ 1)
            return True
        
        # ë§¤ì¹­ëœ ì ë“¤ì˜ ìœ„ì¹˜ ë¶„ì„
        matched_i = np.array([kpts_i[idx_i] for idx_i, _, _ in valid_matches])
        matched_j = np.array([kpts_j[idx_j] for _, idx_j, _ in valid_matches])
        
        # ì´ë¯¸ì§€ í¬ê¸°
        h_i, w_i = self.image_features[cam_i]['image_size']
        h_j, w_j = self.image_features[cam_j]['image_size']
        
        # ê²½ê³„ ê·¼ì²˜ì˜ ë§¤ì¹­ì´ ë„ˆë¬´ ë§ì€ì§€ í™•ì¸ (ë” ì™„í™”ëœ ì¡°ê±´)
        border_threshold = 10  # ë” ì‘ì€ ê²½ê³„ (20 â†’ 10)
        
        border_matches_i = np.sum((matched_i[:, 0] < border_threshold) | 
                                  (matched_i[:, 0] > w_i - border_threshold) |
                                  (matched_i[:, 1] < border_threshold) | 
                                  (matched_i[:, 1] > h_i - border_threshold))
        
        border_matches_j = np.sum((matched_j[:, 0] < border_threshold) | 
                                  (matched_j[:, 0] > w_j - border_threshold) |
                                  (matched_j[:, 1] < border_threshold) | 
                                  (matched_j[:, 1] > h_j - border_threshold))
        
        # ê²½ê³„ ë§¤ì¹­ì´ ì „ì²´ì˜ 98% ì´ìƒì´ë©´ ë‚˜ìœ ë¶„í¬ (95% â†’ 98%)
        if border_matches_i > len(valid_matches) * 0.98 or border_matches_j > len(valid_matches) * 0.98:
            return True
        
        return False
    
    def _match_pair_superglue(self, cam_i, cam_j):
        """SuperGlue í˜ì–´ ë§¤ì¹­ (ë” ì™„í™”ëœ ë²„ì „)"""
        if not self.superglue_available:
            return self._match_pair_fallback(cam_i, cam_j)
        
        try:
            feat_i = self.image_features[cam_i]
            feat_j = self.image_features[cam_j]
            
            # ì…ë ¥ ë°ì´í„° ì¤€ë¹„
            data = {
                'image0': torch.zeros((1, 1, 480, 640)).to(self.device),
                'image1': torch.zeros((1, 1, 480, 640)).to(self.device),
                'keypoints0': torch.from_numpy(feat_i['keypoints']).unsqueeze(0).to(self.device),
                'keypoints1': torch.from_numpy(feat_j['keypoints']).unsqueeze(0).to(self.device),
                'descriptors0': torch.from_numpy(feat_i['descriptors']).unsqueeze(0).to(self.device),
                'descriptors1': torch.from_numpy(feat_j['descriptors']).unsqueeze(0).to(self.device),
                'scores0': torch.from_numpy(feat_i['scores']).unsqueeze(0).to(self.device),
                'scores1': torch.from_numpy(feat_j['scores']).unsqueeze(0).to(self.device),
            }
            
            # SuperGlue ë§¤ì¹­
            with torch.no_grad():
                result = self.matching.superglue(data)
            
            # ê²°ê³¼ ì¶”ì¶œ
            indices0 = result['indices0'][0].cpu().numpy()
            indices1 = result['indices1'][0].cpu().numpy()
            mscores0 = result['matching_scores0'][0].cpu().numpy()
            
            # ğŸ”§ ë” ì™„í™”ëœ ë§¤ì¹­ í•„í„°ë§
            valid_matches = []
            threshold = 0.00001  # 0.0001 â†’ 0.00001ë¡œ ëŒ€í­ ì™„í™”
            
            for i, j in enumerate(indices0):
                if j >= 0 and mscores0[i] > threshold:
                    # ìƒí˜¸ ë§¤ì¹­ í™•ì¸
                    if j < len(indices1) and indices1[j] == i:
                        # ì¸ë±ìŠ¤ ë²”ìœ„ ê²€ì¦
                        if i < len(feat_i['keypoints']) and j < len(feat_j['keypoints']):
                            valid_matches.append((i, j, mscores0[i]))
            
            # ğŸ”§ ë” ì™„í™”ëœ ê¸°í•˜í•™ì  í•„í„°ë§
            if len(valid_matches) >= 1:  # 1ê°œ ì´ìƒì´ë©´ í•„í„°ë§ ì‹œë„
                valid_matches = self._geometric_filtering_relaxed(valid_matches, feat_i['keypoints'], feat_j['keypoints'])
            
            return valid_matches
            
        except Exception as e:
            print(f"    SuperGlue matching failed for pair {cam_i}-{cam_j}: {e}")
            return self._match_pair_fallback(cam_i, cam_j)
    
    def _match_pair_fallback(self, cam_i, cam_j):
        """SuperGlueê°€ ì—†ì„ ë•Œ ì‚¬ìš©í•˜ëŠ” fallback ë§¤ì¹­"""
        try:
            feat_i = self.image_features[cam_i]
            feat_j = self.image_features[cam_j]
            
            # ê°„ë‹¨í•œ descriptor ë§¤ì¹­
            desc_i = feat_i['descriptors'].T  # (N, D)
            desc_j = feat_j['descriptors'].T  # (M, D)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            desc_i_norm = desc_i / (np.linalg.norm(desc_i, axis=1, keepdims=True) + 1e-10)
            desc_j_norm = desc_j / (np.linalg.norm(desc_j, axis=1, keepdims=True) + 1e-10)
            
            similarity = desc_i_norm @ desc_j_norm.T  # (N, M)
            
            # ìƒìœ„ ë§¤ì¹­ ì°¾ê¸°
            matches = []
            threshold = 0.5  # ìœ ì‚¬ë„ ì„ê³„ê°’
            
            for i in range(len(desc_i)):
                best_j = np.argmax(similarity[i])
                if similarity[i, best_j] > threshold:
                    # ìƒí˜¸ ë§¤ì¹­ í™•ì¸
                    if np.argmax(similarity[:, best_j]) == i:
                        confidence = similarity[i, best_j]
                        matches.append((i, best_j, confidence))
            
            return matches
            
        except Exception as e:
            print(f"    Fallback matching failed for pair {cam_i}-{cam_j}: {e}")
            return []

    def _geometric_filtering_relaxed(self, matches, kpts_i, kpts_j):
        """ì™„í™”ëœ ê¸°í•˜í•™ì  í•„í„°ë§ (NEW METHOD)"""
        try:
            pts_i = np.array([kpts_i[m[0]] for m in matches])
            pts_j = np.array([kpts_j[m[1]] for m in matches])
            
            # í˜¸ëª¨ê·¸ë˜í”¼ ê¸°ë°˜ outlier ì œê±° (ë” ì™„í™”ëœ ì¡°ê±´)
            H, mask = cv2.findHomography(pts_i, pts_j, cv2.RANSAC, 100.0)  # 50.0 â†’ 100.0ìœ¼ë¡œ ë” ì™„í™”
            
            if H is not None and mask is not None:
                inlier_matches = [matches[i] for i, is_inlier in enumerate(mask.flatten()) if is_inlier]
                if len(inlier_matches) >= 1:  # 1ê°œ ì´ìƒì´ë©´ í†µê³¼
                    return inlier_matches
        except:
            pass
        
        return matches
    
    def _estimate_camera_poses_robust(self):
        """ê°œì„ ëœ ì¹´ë©”ë¼ í¬ì¦ˆ ì¶”ì • - ë” ê°•ë ¥í•œ ì—°ê²°ì„±"""
        
        # ì²« ë²ˆì§¸ ì¹´ë©”ë¼ë¥¼ ì›ì ìœ¼ë¡œ ì„¤ì •
        self.cameras[0] = {
            'R': np.eye(3, dtype=np.float32),
            'T': np.zeros(3, dtype=np.float32),
            'K': self._estimate_intrinsics(0)
        }
        
        print(f"  Camera 0: Origin (reference)")
        
        # ğŸ”§ ê°œì„ ëœ í¬ì¦ˆ ì¶”ì • ì „ëµ
        estimated_cameras = {0}
        queue = [0]
        
        # 1ë‹¨ê³„: ì—°ê²°ëœ ì¹´ë©”ë¼ë“¤ë§Œ í¬ì¦ˆ ì¶”ì •
        while queue:
            current_cam = queue.pop(0)
            
            # í˜„ì¬ ì¹´ë©”ë¼ì™€ ì—°ê²°ëœ ì¹´ë©”ë¼ë“¤ í™•ì¸
            for neighbor_cam in self.camera_graph[current_cam]:
                if neighbor_cam in estimated_cameras:
                    continue
                
                # ë§¤ì¹­ ë°ì´í„° ì°¾ê¸°
                pair_key = (current_cam, neighbor_cam) if current_cam < neighbor_cam else (neighbor_cam, current_cam)
                if pair_key not in self.matches:
                    continue
                
                # Essential Matrix ê¸°ë°˜ í¬ì¦ˆ ì¶”ì •
                R_rel, T_rel = self._estimate_relative_pose_robust(current_cam, neighbor_cam, pair_key)
                
                if R_rel is not None and T_rel is not None:
                    # ì›”ë“œ ì¢Œí‘œê³„ì—ì„œì˜ ì ˆëŒ€ í¬ì¦ˆ ê³„ì‚°
                    R_ref, T_ref = self.cameras[current_cam]['R'], self.cameras[current_cam]['T']
                    
                    # ìƒëŒ€ í¬ì¦ˆë¥¼ ì ˆëŒ€ í¬ì¦ˆë¡œ ë³€í™˜
                    R_world = R_rel @ R_ref
                    T_world = R_rel @ T_ref + T_rel
                    
                    # í¬ì¦ˆ ìœ íš¨ì„± ê²€ì‚¬
                    if self._is_valid_rotation_matrix(R_world):
                        self.cameras[neighbor_cam] = {
                            'R': R_world.astype(np.float32),
                            'T': T_world.astype(np.float32),
                            'K': self._estimate_intrinsics(neighbor_cam)
                        }
                        
                        print(f"  Camera {neighbor_cam}: Estimated from camera {current_cam}")
                        estimated_cameras.add(neighbor_cam)
                        queue.append(neighbor_cam)
                    else:
                        print(f"  Camera {neighbor_cam}: Invalid pose, skipping...")
                else:
                    print(f"  Camera {neighbor_cam}: Pose estimation failed, will use default pose")
        
        # 2ë‹¨ê³„: ì—°ê²°ë˜ì§€ ì•Šì€ ì¹´ë©”ë¼ë“¤ì— ëŒ€í•œ ê°œì„ ëœ ê¸°ë³¸ í¬ì¦ˆ ì„¤ì •
        unconnected_count = 0
        for cam_id in range(len(self.image_features)):
            if cam_id not in estimated_cameras:
                unconnected_count += 1
                print(f"  Camera {cam_id}: Using improved default pose (not connected)")
                
                # ğŸ”§ ê°œì„ ëœ ê¸°ë³¸ í¬ì¦ˆ ì„¤ì •
                if len(estimated_cameras) > 0:
                    # ì—°ê²°ëœ ì¹´ë©”ë¼ë“¤ì˜ í‰ê·  ìœ„ì¹˜ ê³„ì‚°
                    connected_positions = []
                    for est_cam in estimated_cameras:
                        R, T = self.cameras[est_cam]['R'], self.cameras[est_cam]['T']
                        # ì¹´ë©”ë¼ ì¤‘ì‹¬ ê³„ì‚°
                        center = -R.T @ T
                        connected_positions.append(center)
                    
                    if connected_positions:
                        # ì—°ê²°ëœ ì¹´ë©”ë¼ë“¤ì˜ ì¤‘ì‹¬ ì£¼ë³€ì— ë°°ì¹˜
                        avg_position = np.mean(connected_positions, axis=0)
                        position_std = np.std(connected_positions, axis=0)
                        
                        # ì¹´ë©”ë¼ IDì— ë”°ë¥¸ ìœ„ì¹˜ ë³€í™”
                        angle = cam_id * (2 * np.pi / len(self.image_features))
                        radius = 2.0 + np.random.normal(0, 0.5)  # ì•½ê°„ì˜ ëœë¤ì„±
                        
                        # ì›í˜• ë°°ì¹˜ + ì¤‘ì‹¬ìœ¼ë¡œë¶€í„°ì˜ ì˜¤í”„ì…‹
                        camera_pos = avg_position + np.array([
                            radius * np.cos(angle),
                            0.5 * np.sin(angle),  # ë†’ì´ ë³€í™”
                            radius * np.sin(angle)
                        ])
                        
                        # ì¤‘ì‹¬ì„ í–¥í•˜ëŠ” ë°©í–¥
                        look_at = avg_position
                        up = np.array([0.0, 1.0, 0.0])
                        
                        # ì¹´ë©”ë¼ íšŒì „ í–‰ë ¬ ê³„ì‚°
                        forward = look_at - camera_pos
                        forward = forward / (np.linalg.norm(forward) + 1e-10)
                        right = np.cross(forward, up)
                        right = right / (np.linalg.norm(right) + 1e-10)
                        up = np.cross(right, forward)
                        
                        R = np.array([right, up, -forward]).T
                        T = camera_pos
                    else:
                        # ê¸°ë³¸ ì›í˜• ë°°ì¹˜
                        angle = cam_id * (2 * np.pi / len(self.image_features))
                        radius = 3.0
                        
                        R = np.array([[np.cos(angle), 0, np.sin(angle)],
                                     [0, 1, 0],
                                     [-np.sin(angle), 0, np.cos(angle)]], dtype=np.float32)
                        T = np.array([radius * np.sin(angle), 0, radius * (1 - np.cos(angle))], dtype=np.float32)
                else:
                    # ê¸°ë³¸ ì›í˜• ë°°ì¹˜
                    angle = cam_id * (2 * np.pi / len(self.image_features))
                    radius = 3.0
                    
                    R = np.array([[np.cos(angle), 0, np.sin(angle)],
                                 [0, 1, 0],
                                 [-np.sin(angle), 0, np.cos(angle)]], dtype=np.float32)
                    T = np.array([radius * np.sin(angle), 0, radius * (1 - np.cos(angle))], dtype=np.float32)
                
                self.cameras[cam_id] = {
                    'R': R.astype(np.float32),
                    'T': T.astype(np.float32),
                    'K': self._estimate_intrinsics(cam_id)
                }
        
        print(f"  Estimated poses for {len(estimated_cameras)} cameras")
        print(f"  Used default poses for {unconnected_count} cameras")
        print(f"  Total cameras with poses: {len(self.cameras)}")
    
    def _estimate_relative_pose_robust(self, cam_i, cam_j, pair_key):
        """ê°œì„ ëœ ë‘ ì¹´ë©”ë¼ ê°„ ìƒëŒ€ í¬ì¦ˆ ì¶”ì • - ê·¹ë„ë¡œ ì™„í™”ëœ ë²„ì „"""
        matches = self.matches[pair_key]
        
        if len(matches) < 4:  # 6 â†’ 4ë¡œ ë” ì™„í™”
            print(f"    Pair {cam_i}-{cam_j}: Insufficient matches ({len(matches)} < 4)")
            return None, None
        
        # ë§¤ì¹­ì ë“¤ ì¶”ì¶œ
        kpts_i = self.image_features[cam_i]['keypoints']
        kpts_j = self.image_features[cam_j]['keypoints']
        
        print(f"    Pair {cam_i}-{cam_j}: kpts_i shape: {kpts_i.shape}, kpts_j shape: {kpts_j.shape}")
        
        # ğŸ”§ ê·¹ë„ë¡œ ì™„í™”ëœ ì‹ ë¢°ë„ ì„ê³„ê°’
        high_conf_matches = [(idx_i, idx_j, conf) for idx_i, idx_j, conf in matches if conf > 0.00001]  # 0.0001 â†’ 0.00001ë¡œ ëŒ€í­ ì™„í™”
        
        if len(high_conf_matches) < 4:  # 6 â†’ 4ë¡œ ë” ì™„í™”
            high_conf_matches = [(idx_i, idx_j, conf) for idx_i, idx_j, conf in matches if conf > 0.000001]  # 0.00001 â†’ 0.000001ë¡œ ëŒ€í­ ì™„í™”
        
        if len(high_conf_matches) < 4:  # 6 â†’ 4ë¡œ ë” ì™„í™”
            # ëª¨ë“  ë§¤ì¹­ì„ ì‚¬ìš©
            high_conf_matches = [(idx_i, idx_j, conf) for idx_i, idx_j, conf in matches]
        
        if len(high_conf_matches) < 4:  # 6 â†’ 4ë¡œ ë” ì™„í™”
            print(f"    Pair {cam_i}-{cam_j}: Insufficient high-confidence matches ({len(high_conf_matches)} < 4)")
            return None, None
        
        # ğŸ”§ ì¸ë±ìŠ¤ ë²”ìœ„ ê²€ì¦ ê°•í™”
        valid_matches = []
        for idx_i, idx_j, conf in high_conf_matches:
            if (isinstance(idx_i, (int, np.integer)) and isinstance(idx_j, (int, np.integer)) and
                idx_i >= 0 and idx_j >= 0 and 
                idx_i < len(kpts_i) and idx_j < len(kpts_j)):
                valid_matches.append((idx_i, idx_j, conf))
        
        if len(valid_matches) < 4:  # 6 â†’ 4ë¡œ ë” ì™„í™”
            print(f"    Pair {cam_i}-{cam_j}: Insufficient valid matches after index validation ({len(valid_matches)} < 4)")
            return None, None
        
        print(f"    Pair {cam_i}-{cam_j}: Using {len(valid_matches)} validated matches")
        
        # ğŸ”§ ê°œì„ ëœ í¬ì¸íŠ¸ ì¶”ì¶œ
        try:
            pts_i = np.array([kpts_i[idx_i] for idx_i, _, _ in valid_matches])
            pts_j = np.array([kpts_j[idx_j] for _, idx_j, _ in valid_matches])
        except IndexError as e:
            print(f"    IndexError during point extraction: {e}")
            return None, None
        
        # ğŸ”§ ê¸°í•˜í•™ì  ì¼ê´€ì„± ì‚¬ì „ ê²€ì¦ (ë” ê´€ëŒ€í•˜ê²Œ)
        if not self._check_geometric_consistency_very_relaxed(pts_i, pts_j):
            print(f"    Pair {cam_i}-{cam_j}: Failed geometric consistency check")
            return None, None
        
        # ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„°
        K_i = self.cameras.get(cam_i, {}).get('K', self._estimate_intrinsics(cam_i))
        K_j = self._estimate_intrinsics(cam_j)
        
        # ğŸ”§ ê·¹ë„ë¡œ ê´€ëŒ€í•œ Essential Matrix ì¶”ì • ë°©ë²•ë“¤
        methods = [
            (cv2.RANSAC, 0.5, 0.99),    # ê·¹ë„ë¡œ ê´€ëŒ€í•œ ì„ê³„ê°’
            (cv2.RANSAC, 1.0, 0.95),
            (cv2.RANSAC, 2.0, 0.90),
            (cv2.LMEDS, 0.5, 0.95),
            (cv2.RANSAC, 5.0, 0.85),    # ë§¤ìš° ê´€ëŒ€í•œ ì„¤ì •
            (cv2.RANSAC, 10.0, 0.80),   # ê·¹ë„ë¡œ ê´€ëŒ€í•œ ì„¤ì •
            (cv2.RANSAC, 20.0, 0.70)    # ìµœëŒ€í•œ ê´€ëŒ€í•œ ì„¤ì •
        ]
        
        best_R, best_T = None, None
        best_inliers = 0
        best_quality = 0
        
        for method, threshold, confidence in methods:
            try:
                # Essential Matrix ì¶”ì •
                E, mask = cv2.findEssentialMat(
                    pts_i, pts_j, K_i,
                    method=method,
                    prob=confidence,
                    threshold=threshold,
                    maxIters=500  # ë°˜ë³µ íšŸìˆ˜ ì¤„ì„
                )
                
                if E is None or E.shape != (3, 3):
                    continue
                
                # í¬ì¦ˆ ë³µì›
                _, R, T, mask = cv2.recoverPose(E, pts_i, pts_j, K_i, mask=mask)
                
                if R is None or T is None:
                    continue
                
                inliers = np.sum(mask)
                
                if inliers >= 2:  # 4 â†’ 2ë¡œ ê·¹ë„ë¡œ ì™„í™”
                    # ğŸ”§ ë” ê´€ëŒ€í•œ í¬ì¦ˆ í’ˆì§ˆ ê²€ì¦
                    quality_score = self._evaluate_pose_quality_very_relaxed(pts_i, pts_j, R, T.flatten(), K_i, K_j, mask)
                    
                    if quality_score > best_quality:
                        best_R, best_T = R, T.flatten()
                        best_inliers = inliers
                        best_quality = quality_score
                        
            except Exception as e:
                print(f"      Method {method} failed: {e}")
                continue
        
        if best_R is not None:
            print(f"    Pair {cam_i}-{cam_j}: Successfully estimated pose with {best_inliers} inliers, quality: {best_quality:.3f}")
        else:
            print(f"    Pair {cam_i}-{cam_j}: Failed to estimate pose")
        
        return best_R, best_T

    def _check_geometric_consistency_very_relaxed(self, pts_i, pts_j):
        """ê·¹ë„ë¡œ ê´€ëŒ€í•œ ê¸°í•˜í•™ì  ì¼ê´€ì„± ì‚¬ì „ ê²€ì¦"""
        try:
            # 1. í˜¸ëª¨ê·¸ë˜í”¼ ê¸°ë°˜ ì¼ê´€ì„± ê²€ì‚¬ (ê·¹ë„ë¡œ ê´€ëŒ€í•˜ê²Œ)
            H, mask = cv2.findHomography(pts_i, pts_j, cv2.RANSAC, 20.0)  # 10.0 â†’ 20.0ìœ¼ë¡œ ë” ì™„í™”
            if H is not None:
                homography_inliers = np.sum(mask)
                if homography_inliers < len(pts_i) * 0.01:  # 5% â†’ 1%ë¡œ ê·¹ë„ë¡œ ì™„í™”
                    return False
            
            # 2. í¬ì¸íŠ¸ ë¶„í¬ ê²€ì‚¬ (ê·¹ë„ë¡œ ê´€ëŒ€í•˜ê²Œ)
            if len(pts_i) > 2:  # 3 â†’ 2ë¡œ ë” ì™„í™”
                # í¬ì¸íŠ¸ë“¤ì˜ ë¶„ì‚° ê³„ì‚°
                var_i = np.var(pts_i, axis=0)
                var_j = np.var(pts_j, axis=0)
                
                # ë¶„ì‚°ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ ë‚˜ìœ ë¶„í¬ (ê·¹ë„ë¡œ ê´€ëŒ€í•˜ê²Œ)
                if np.min(var_i) < 0.1 or np.min(var_j) < 0.1:  # 1 â†’ 0.1ë¡œ ëŒ€í­ ì™„í™”
                    return False
            
            # 3. í¬ì¸íŠ¸ ê°„ ê±°ë¦¬ ê²€ì‚¬ (ê·¹ë„ë¡œ ê´€ëŒ€í•˜ê²Œ)
            if len(pts_i) > 1:  # 2 â†’ 1ë¡œ ë” ì™„í™”
                distances_i = cdist(pts_i, pts_i)
                distances_j = cdist(pts_j, pts_j)
                
                # ëŒ€ê°ì„  ì œê±°
                np.fill_diagonal(distances_i, np.inf)
                np.fill_diagonal(distances_j, np.inf)
                
                min_dist_i = np.min(distances_i)
                min_dist_j = np.min(distances_j)
                
                # ìµœì†Œ ê±°ë¦¬ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ë‚˜ìœ ë¶„í¬ (ê·¹ë„ë¡œ ê´€ëŒ€í•˜ê²Œ)
                if min_dist_i < 0.01 or min_dist_j < 0.01:  # 0.1 â†’ 0.01ë¡œ ëŒ€í­ ì™„í™”
                    return False
            
            return True
            
        except Exception as e:
            print(f"      Geometric consistency check failed: {e}")
            return True  # ì˜¤ë¥˜ì‹œ í†µê³¼

    def _evaluate_pose_quality_very_relaxed(self, pts_i, pts_j, R, T, K_i, K_j, mask):
        """ê·¹ë„ë¡œ ê´€ëŒ€í•œ í¬ì¦ˆ í’ˆì§ˆ í‰ê°€"""
        try:
            # 1. íšŒì „ í–‰ë ¬ ìœ íš¨ì„± í™•ì¸ (ê·¹ë„ë¡œ ê´€ëŒ€í•˜ê²Œ)
            det = np.linalg.det(R)
            if abs(det - 1.0) > 0.5:  # 0.3 â†’ 0.5ë¡œ ë” ì™„í™”
                return 0.0
            
            # 2. ì‚¼ê°ì¸¡ëŸ‰ í’ˆì§ˆ ê²€ì‚¬ (ê·¹ë„ë¡œ ê´€ëŒ€í•˜ê²Œ)
            P_i = K_i @ np.hstack([np.eye(3), np.zeros((3, 1))])
            P_j = K_j @ np.hstack([R, T.reshape(-1, 1)])
            
            # inlier í¬ì¸íŠ¸ë“¤ë§Œ ì‚¬ìš©
            inlier_pts_i = pts_i[mask.flatten()]
            inlier_pts_j = pts_j[mask.flatten()]
            
            if len(inlier_pts_i) < 2:  # 4 â†’ 2ë¡œ ê·¹ë„ë¡œ ì™„í™”
                return 0.0
            
            # ì‚¼ê°ì¸¡ëŸ‰ í…ŒìŠ¤íŠ¸ (ê·¹ë„ë¡œ ê´€ëŒ€í•˜ê²Œ)
            valid_points = 0
            total_error = 0.0
            
            for pt_i, pt_j in zip(inlier_pts_i, inlier_pts_j):
                try:
                    # ì‚¼ê°ì¸¡ëŸ‰
                    pt_4d = cv2.triangulatePoints(P_i, P_j, pt_i.reshape(2, 1), pt_j.reshape(2, 1))
                    
                    if abs(pt_4d[3, 0]) > 1e-10:
                        pt_3d = (pt_4d[:3] / pt_4d[3]).flatten()
                        
                        # ê±°ë¦¬ ì²´í¬ (ê·¹ë„ë¡œ ê´€ëŒ€í•˜ê²Œ)
                        if 0.0001 < np.linalg.norm(pt_3d) < 100000:  # 0.001~10000 â†’ 0.0001~100000ìœ¼ë¡œ ì™„í™”
                            # ì¬íˆ¬ì˜ ì˜¤ì°¨ ê³„ì‚°
                            proj_i = P_i @ np.append(pt_3d, 1)
                            proj_j = P_j @ np.append(pt_3d, 1)
                            
                            if proj_i[2] > 0 and proj_j[2] > 0:
                                proj_i_2d = proj_i[:2] / proj_i[2]
                                proj_j_2d = proj_j[:2] / proj_j[2]
                                
                                error_i = np.linalg.norm(proj_i_2d - pt_i)
                                error_j = np.linalg.norm(proj_j_2d - pt_j)
                                
                                total_error += max(error_i, error_j)
                                valid_points += 1
                                
                except:
                    continue
            
            if valid_points < 2:  # 4 â†’ 2ë¡œ ê·¹ë„ë¡œ ì™„í™”
                return 0.0
            
            # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (ê·¹ë„ë¡œ ê´€ëŒ€í•˜ê²Œ)
            avg_error = total_error / valid_points
            inlier_ratio = len(inlier_pts_i) / len(pts_i)
            
            # ì˜¤ì°¨ê°€ ì‘ê³  inlier ë¹„ìœ¨ì´ ë†’ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜ (ê·¹ë„ë¡œ ê´€ëŒ€í•˜ê²Œ)
            quality_score = inlier_ratio * (1.0 / (1.0 + avg_error * 0.001))  # 0.01 â†’ 0.001ë¡œ ì™„í™”
            
            return quality_score
            
        except Exception as e:
            print(f"      Pose quality evaluation failed: {e}")
            return 0.0
    
    def _estimate_pose_fallback(self, pts_i, pts_j, K_i, K_j):
        """OpenCV ì‹¤íŒ¨ì‹œ ì‚¬ìš©í•  fallback í¬ì¦ˆ ì¶”ì •"""
        try:
            # ê°„ë‹¨í•œ í˜¸ëª¨ê·¸ë˜í”¼ ê¸°ë°˜ ë°©ë²•
            H, mask = cv2.findHomography(pts_i, pts_j, cv2.RANSAC, 5.0)
            
            if H is None:
                return None, None
            
            # í˜¸ëª¨ê·¸ë˜í”¼ì—ì„œ íšŒì „ê³¼ ì´ë™ ì¶”ì¶œ (ê·¼ì‚¬)
            # ì´ëŠ” ì •í™•í•˜ì§€ ì•Šì§€ë§Œ ê¸°ë³¸ì ì¸ í¬ì¦ˆë¥¼ ì œê³µ
            K_inv = np.linalg.inv(K_i)
            R_approx = K_inv @ H @ K_i
            
            # SVDë¥¼ ì‚¬ìš©í•˜ì—¬ íšŒì „ í–‰ë ¬ë¡œ ì •ê·œí™”
            U, _, Vt = np.linalg.svd(R_approx)
            R = U @ Vt
            
            # íšŒì „ í–‰ë ¬ ìœ íš¨ì„± ê²€ì‚¬
            if not self._is_valid_rotation_matrix(R):
                # ê¸°ë³¸ íšŒì „ í–‰ë ¬ ì‚¬ìš©
                R = np.eye(3)
            
            # ì´ë™ ë²¡í„° ì¶”ì • (ê°„ë‹¨í•œ ê·¼ì‚¬)
            T = np.array([0.1, 0.0, 0.0])  # ê¸°ë³¸ ì´ë™
            
            return R, T
            
        except Exception as e:
            print(f"      Fallback pose estimation failed: {e}")
            return None, None
    
    def _is_valid_rotation_matrix(self, R):
        """íšŒì „ í–‰ë ¬ì´ ìœ íš¨í•œì§€ í™•ì¸"""
        try:
            # í–‰ë ¬ì‹ì´ 1ì— ê°€ê¹Œìš´ì§€ í™•ì¸
            det = np.linalg.det(R)
            if abs(det - 1.0) > 0.1:
                return False
            
            # R * R^T = Iì¸ì§€ í™•ì¸
            I = np.eye(3)
            RRt = R @ R.T
            if np.max(np.abs(RRt - I)) > 0.1:
                return False
            
            return True
        except:
            return False
    
    def _verify_pose_quality_very_relaxed(self, pts_i, pts_j, R, T, K_i, K_j):
        """ë§¤ìš° ì™„í™”ëœ í¬ì¦ˆ í’ˆì§ˆ ê²€ì¦"""
        # ì¬íˆ¬ì˜ ì˜¤ì°¨ ê³„ì‚°
        P_i = K_i @ np.hstack([np.eye(3), np.zeros((3, 1))])
        P_j = K_j @ np.hstack([R, T.reshape(-1, 1)])
        
        errors = []
        depths_i = []
        depths_j = []
        
        for pt_i, pt_j in zip(pts_i, pts_j):
            # ì‚¼ê°ì¸¡ëŸ‰
            point_4d = cv2.triangulatePoints(P_i, P_j, pt_i.reshape(2, 1), pt_j.reshape(2, 1))
            if abs(point_4d[3, 0]) > 1e-10:
                point_3d = (point_4d[:3] / point_4d[3]).flatten()
                
                # ì¬íˆ¬ì˜ (3D ì¢Œí‘œê³„ì—ì„œ)
                proj_i_3d = P_i @ np.append(point_3d, 1)
                proj_j_3d = P_j @ np.append(point_3d, 1)
                
                # 2D ì¢Œí‘œë¡œ ë³€í™˜
                proj_i_2d = proj_i_3d[:2] / proj_i_3d[2]
                proj_j_2d = proj_j_3d[:2] / proj_j_3d[2]
                
                error_i = np.linalg.norm(proj_i_2d - pt_i)
                error_j = np.linalg.norm(proj_j_2d - pt_j)
                errors.append(max(error_i, error_j))
                
                # ê¹Šì´ ì •ë³´ ì €ì¥ (3D ì¢Œí‘œê³„ì—ì„œ)
                depths_i.append(proj_i_3d[2])
                depths_j.append(proj_j_3d[2])
        
        if len(errors) < 2:  # ë” ë‚®ì€ ì„ê³„ê°’ (3 â†’ 2)
            return False
        
        # ì˜¤ì°¨ í†µê³„
        median_error = np.median(errors)
        mean_error = np.mean(errors)
        max_error = np.max(errors)
        
        # ê¹Šì´ ê²€ì¦ (ë” ì™„í™”ëœ ì¡°ê±´)
        if depths_i and depths_j:
            depths_i = np.array(depths_i)
            depths_j = np.array(depths_j)
            
            # ê¹Šì´ê°€ ì–‘ìˆ˜ì¸ì§€ í™•ì¸
            if np.any(depths_i <= 0) or np.any(depths_j <= 0):
                return False
            
            # ê¹Šì´ ë¹„ìœ¨ì´ í•©ë¦¬ì ì¸ì§€ í™•ì¸ (ë” ì™„í™”ëœ ì¡°ê±´)
            depth_ratios = depths_j / depths_i
            if np.median(depth_ratios) < 0.01 or np.median(depth_ratios) > 50:  # 0.05~20 â†’ 0.01~50
                return False
        
        # ì˜¤ì°¨ ì„ê³„ê°’ ê²€ì¦ (ë” ì™„í™”ëœ ì¡°ê±´)
        pose_quality = (median_error < 15.0 and   # 8.0 â†’ 15.0
                mean_error < 20.0 and    # 10.0 â†’ 20.0
                max_error < 50.0)        # 20.0 â†’ 50.0
        
        if not pose_quality:
            print(f"      Pose quality check failed: median={median_error:.2f}, mean={mean_error:.2f}, max={max_error:.2f}")
        
        return pose_quality
    
    def _estimate_intrinsics(self, cam_id):
        """ê°œì„ ëœ ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„° ì¶”ì • (COLMAP ìš°ì„ )"""
        h, w = self.image_features[cam_id]['image_size']
        
        # COLMAP reconstructionì´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©
        try:
            colmap_cameras = self._get_colmap_intrinsics()
            if colmap_cameras and cam_id in colmap_cameras:
                camera = colmap_cameras[cam_id]
                width, height = camera.width, camera.height
                
                # PINHOLE ëª¨ë¸ ê°€ì • (fx, fy, cx, cy)
                if len(camera.params) == 4:
                    fx, fy, cx, cy = camera.params
                    # COLMAPì—ì„œ ì¶”ì •í•œ ì •í™•í•œ focal length ì‚¬ìš©
                    K = np.array([
                        [fx, 0, cx],
                        [0, fy, cy],
                        [0, 0, 1]
                    ], dtype=np.float32)
                    print(f"    Camera {cam_id}: Using COLMAP focal length (fx={fx:.1f}, fy={fy:.1f})")
                    return K
        except Exception as e:
            print(f"    Camera {cam_id}: COLMAP intrinsics failed, using default: {e}")
            # ì†ìƒëœ COLMAP íŒŒì¼ë“¤ì„ ì •ë¦¬
            try:
                output_dir = getattr(self, 'output_dir', None)
                if output_dir:
                    reconstruction_path = Path(output_dir) / "sparse" / "0"
                    for file_name in ["cameras.bin", "images.bin", "points3D.bin"]:
                        file_path = reconstruction_path / file_name
                        if file_path.exists():
                            file_path.unlink()
                            print(f"    Deleted corrupted {file_name}")
            except Exception as cleanup_error:
                print(f"    Failed to cleanup corrupted files: {cleanup_error}")
        
        # COLMAPì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì¶”ì • ì‚¬ìš©
        focal = max(w, h) * 0.9  # ì•½ê°„ ë³´ìˆ˜ì ì¸ ì¶”ì •
        cx, cy = w / 2, h / 2
        
        K = np.array([
            [focal, 0, cx],
            [0, focal, cy],
            [0, 0, 1]
        ], dtype=np.float32)
        
        print(f"    Camera {cam_id}: Using default focal length ({focal:.1f})")
        return K
    
    def _get_colmap_intrinsics(self):
        """COLMAP reconstructionì—ì„œ ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„° ì½ê¸°"""
        try:
            # COLMAP reconstruction ê²½ë¡œ í™•ì¸
            output_dir = getattr(self, 'output_dir', None)
            if output_dir is None:
                return None
            
            reconstruction_path = Path(output_dir) / "sparse" / "0"
            cameras_bin = reconstruction_path / "cameras.bin"
            
            if not cameras_bin.exists():
                print(f"    cameras.bin not found at {cameras_bin}")
                return None
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            file_size = cameras_bin.stat().st_size
            if file_size == 0:
                print(f"    cameras.bin is empty")
                return None
            
            print(f"    Reading COLMAP intrinsics from {cameras_bin} ({file_size} bytes)")
            
            # COLMAP reconstruction íŒŒì‹±
            try:
                from scene.colmap_loader import read_intrinsics_binary
                cameras = read_intrinsics_binary(str(cameras_bin))
                print(f"    Successfully loaded {len(cameras)} cameras from COLMAP")
                
                # ì´ë¯¸ì§€ IDì™€ ì¹´ë©”ë¼ ID ë§¤í•‘
                images_bin = reconstruction_path / "images.bin"
                if images_bin.exists():
                    try:
                        from scene.colmap_loader import read_extrinsics_binary
                        images = read_extrinsics_binary(str(images_bin))
                        
                        # ì´ë¯¸ì§€ ID -> ì¹´ë©”ë¼ ID ë§¤í•‘
                        image_to_camera = {}
                        for image_id, image in images.items():
                            image_to_camera[image_id] = image.camera_id
                        
                        return image_to_camera, cameras
                    except Exception as e:
                        print(f"    Failed to read images.bin: {e}")
                        return None
                
                return None
                
            except Exception as e:
                print(f"    Failed to read cameras.bin: {e}")
                return None
            
        except Exception as e:
            print(f"    COLMAP intrinsics ì½ê¸° ì‹¤íŒ¨: {e}")
            # íŒŒì¼ì´ ì†ìƒë˜ì—ˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì‚­ì œ ì‹œë„
            try:
                if cameras_bin.exists():
                    cameras_bin.unlink()
                    print(f"    Deleted corrupted cameras.bin")
                if images_bin.exists():
                    images_bin.unlink()
                    print(f"    Deleted corrupted images.bin")
                if points3d_bin.exists():
                    points3d_bin.unlink()
                    print(f"    Deleted corrupted points3D.bin")
            except Exception as del_error:
                print(f"    Failed to delete corrupted files: {del_error}")
            return None
    
    def _triangulate_all_points_robust(self):
        """ê°œì„ ëœ ì‚¼ê°ì¸¡ëŸ‰ - ë” ë§ì€ í¬ì¸íŠ¸ ìƒì„±"""
        point_id = 0
        total_matches_processed = 0
        total_valid_matches = 0
        total_triangulated = 0
        total_validated = 0
        
        print(f"  Processing {len(self.matches)} image pairs for triangulation...")
        
        for (cam_i, cam_j), matches in self.matches.items():
            if cam_i not in self.cameras or cam_j not in self.cameras:
                continue
            
            try:
                # íˆ¬ì˜ í–‰ë ¬ ìƒì„±
                P_i = self._get_projection_matrix(cam_i)
                P_j = self._get_projection_matrix(cam_j)
                
                kpts_i = self.image_features[cam_i]['keypoints']
                kpts_j = self.image_features[cam_j]['keypoints']
                
                # ğŸ”§ ë” ì™„í™”ëœ ì‹ ë¢°ë„ ì„ê³„ê°’
                high_conf_matches = [(idx_i, idx_j, conf) for idx_i, idx_j, conf in matches if conf > 0.05]  # 0.2 â†’ 0.05ë¡œ ì™„í™”
                total_matches_processed += len(matches)
                
                # ì¸ë±ìŠ¤ ë²”ìœ„ ê²€ì¦
                valid_matches = []
                for idx_i, idx_j, conf in high_conf_matches:
                    if (idx_i < len(kpts_i) and idx_j < len(kpts_j) and 
                        idx_i >= 0 and idx_j >= 0):
                        valid_matches.append((idx_i, idx_j, conf))
                
                total_valid_matches += len(valid_matches)
                
                # ğŸ”§ ë” ì ê·¹ì ì¸ ì‚¼ê°ì¸¡ëŸ‰
                if len(valid_matches) > 5:  # 10 â†’ 5ë¡œ ì™„í™”
                    batch_size = min(100, len(valid_matches))  # ë°°ì¹˜ í¬ê¸° ì¦ê°€
                    for batch_start in range(0, len(valid_matches), batch_size):
                        batch_end = min(batch_start + batch_size, len(valid_matches))
                        batch_matches = valid_matches[batch_start:batch_end]
                        
                        # ë°°ì¹˜ ì‚¼ê°ì¸¡ëŸ‰
                        pts_i_batch = np.array([kpts_i[idx_i] for idx_i, _, _ in batch_matches])
                        pts_j_batch = np.array([kpts_j[idx_j] for _, idx_j, _ in batch_matches])
                        
                        try:
                            # OpenCV ë°°ì¹˜ ì‚¼ê°ì¸¡ëŸ‰
                            points_4d = cv2.triangulatePoints(P_i, P_j, pts_i_batch.T, pts_j_batch.T)
                            
                            # 4Dì—ì„œ 3Dë¡œ ë³€í™˜ (ë” ì™„í™”ëœ ê²€ì¦)
                            for i in range(points_4d.shape[1]):
                                point_4d = points_4d[:, i]
                                
                                if abs(point_4d[3]) < 1e-10:
                                    continue
                                
                                point_3d = (point_4d[:3] / point_4d[3]).flatten()
                                total_triangulated += 1
                                
                                # ğŸ”§ ë” ì™„í™”ëœ ìœ íš¨ì„± ê²€ì‚¬
                                if self._is_point_valid_relaxed(point_3d, cam_i, cam_j, pts_i_batch[i], pts_j_batch[i]):
                                    # ìƒ‰ìƒ ì¶”ì •
                                    color = self._estimate_point_color_robust(point_3d, cam_i, batch_matches[i][0])
                                    
                                    # 3D í¬ì¸íŠ¸ ì €ì¥
                                    self.points_3d[point_id] = {
                                        'xyz': point_3d.astype(np.float32),
                                        'color': color,
                                        'observations': [(cam_i, pts_i_batch[i], batch_matches[i][2]), 
                                                        (cam_j, pts_j_batch[i], batch_matches[i][2])]
                                    }
                                    
                                    # ê´€ì°° ë°ì´í„° ì¶”ê°€
                                    self.point_observations[point_id].append((cam_i, pts_i_batch[i], batch_matches[i][2]))
                                    self.point_observations[point_id].append((cam_j, pts_j_batch[i], batch_matches[i][2]))
                                    
                                    point_id += 1
                                    total_validated += 1
                                    
                        except Exception as e:
                            print(f"    Batch triangulation failed for pair {cam_i}-{cam_j}: {e}")
                            continue
                else:
                    # ê°œë³„ ì‚¼ê°ì¸¡ëŸ‰ (ê¸°ì¡´ ë°©ì‹)
                    for idx_i, idx_j, conf in valid_matches:
                        try:
                            # ì‚¼ê°ì¸¡ëŸ‰
                            pt_i = kpts_i[idx_i].astype(np.float32)
                            pt_j = kpts_j[idx_j].astype(np.float32)
                            
                            point_4d = cv2.triangulatePoints(P_i, P_j, pt_i.reshape(2, 1), pt_j.reshape(2, 1))
                            
                            if abs(point_4d[3, 0]) < 1e-10:
                                continue
                                
                            point_3d = (point_4d[:3] / point_4d[3]).flatten()
                            total_triangulated += 1
                            
                            # ğŸ”§ ë” ì™„í™”ëœ ìœ íš¨ì„± ê²€ì‚¬
                            if self._is_point_valid_relaxed(point_3d, cam_i, cam_j, pt_i, pt_j):
                                # ìƒ‰ìƒ ì¶”ì •
                                color = self._estimate_point_color_robust(point_3d, cam_i, idx_i)
                                
                                # 3D í¬ì¸íŠ¸ ì €ì¥
                                self.points_3d[point_id] = {
                                    'xyz': point_3d.astype(np.float32),
                                    'color': color,
                                    'observations': [(cam_i, pt_i, conf), (cam_j, pt_j, conf)]
                                }
                                
                                # ê´€ì°° ë°ì´í„° ì¶”ê°€
                                self.point_observations[point_id].append((cam_i, pt_i, conf))
                                self.point_observations[point_id].append((cam_j, pt_j, conf))
                                
                                point_id += 1
                                total_validated += 1
                                
                        except Exception as e:
                            continue
                        
            except Exception as e:
                print(f"    Error processing pair {cam_i}-{cam_j}: {e}")
                continue
        
        print(f"  Triangulation statistics:")
        print(f"    Total matches processed: {total_matches_processed}")
        print(f"    Valid matches: {total_valid_matches}")
        print(f"    Successfully triangulated: {total_triangulated}")
        print(f"    Passed validation: {total_validated}")
        print(f"    Final 3D points: {len(self.points_3d)}")
        
        return len(self.points_3d)

    def _is_point_valid_relaxed(self, point_3d, cam_i, cam_j, pt_i, pt_j):
        """ì™„í™”ëœ 3D í¬ì¸íŠ¸ ìœ íš¨ì„± ê²€ì‚¬"""
        
        # 1. ê¸°ë³¸ NaN/Inf ì²´í¬
        if np.any(np.isnan(point_3d)) or np.any(np.isinf(point_3d)):
            return False
        
        # 2. ê±°ë¦¬ ì œí•œ (ë” ê´€ëŒ€í•œ ë²”ìœ„)
        distance = np.linalg.norm(point_3d)
        if distance > 500 or distance < 0.001:  # 100 â†’ 500, 0.01 â†’ 0.001ë¡œ ì™„í™”
            return False
        
        # 3. ì™„í™”ëœ ì¬íˆ¬ì˜ ì˜¤ì°¨ ì²´í¬
        try:
            max_reprojection_error = 0.0
            
            for cam_id, pt_observed in [(cam_i, pt_i), (cam_j, pt_j)]:
                if cam_id not in self.cameras:
                    continue
                
                cam = self.cameras[cam_id]
                K, R, T = cam['K'], cam['R'], cam['T']
                
                # ì¹´ë©”ë¼ ì¢Œí‘œê³„ë¡œ ë³€í™˜
                point_cam = R @ (point_3d - T)
                
                # ê¹Šì´ ì²´í¬ (ë” ì™„í™”ëœ ì¡°ê±´)
                if point_cam[2] <= 0.001:  # 0.01 â†’ 0.001ë¡œ ì™„í™”
                    return False
                
                # ì¬íˆ¬ì˜
                point_2d_proj = K @ point_cam
                
                if abs(point_2d_proj[2]) < 1e-10:
                    return False
                    
                point_2d_proj = point_2d_proj[:2] / point_2d_proj[2]
                
                # ì¬íˆ¬ì˜ ì˜¤ì°¨ ê³„ì‚°
                error = np.linalg.norm(point_2d_proj - pt_observed)
                max_reprojection_error = max(max_reprojection_error, error)
            
            # ì¬íˆ¬ì˜ ì˜¤ì°¨ ì„ê³„ê°’ (ë” ê´€ëŒ€í•˜ê²Œ)
            if max_reprojection_error > 50.0:  # 10 â†’ 50ìœ¼ë¡œ ì™„í™”
                return False
            
            return True
            
        except Exception as e:
            return False

    def _estimate_point_color_robust(self, point_3d, cam_id, kpt_idx):
        """ê°œì„ ëœ 3D í¬ì¸íŠ¸ ìƒ‰ìƒ ì¶”ì •"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì´ë¯¸ì§€ì—ì„œ ìƒ‰ìƒì„ ìƒ˜í”Œë§
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ëœë¤ ìƒ‰ìƒ ì‚¬ìš©
        return np.random.rand(3).astype(np.float32)
    
    def _bundle_adjustment_robust(self, max_iterations=50):
        """ê°œì„ ëœ Bundle Adjustment - ë” ì™„í™”ëœ ì¡°ê±´"""
        
        n_cameras = len(self.cameras)
        n_points = len(self.points_3d)
        
        if n_cameras < 2 or n_points < 5:  # 20 â†’ 5ë¡œ ëŒ€í­ ì™„í™”
            print("  Insufficient data for bundle adjustment")
            return
        
        # ê´€ì°° ë°ì´í„° ìˆ˜ ê³„ì‚°
        total_observations = sum(len(obs) for obs in self.point_observations.values())
        n_residuals = total_observations * 2  # ê° ê´€ì°°ë‹¹ 2ê°œ ì”ì°¨ (x, y)
        n_variables = n_cameras * 6 + n_points * 3  # ì¹´ë©”ë¼ 6DOF + í¬ì¸íŠ¸ 3DOF
        
        print(f"  BA Statistics:")
        print(f"    Cameras: {n_cameras}, Points: {n_points}")
        print(f"    Observations: {total_observations}")
        print(f"    Residuals: {n_residuals}, Variables: {n_variables}")
        
        # ğŸ”§ ë” ì™„í™”ëœ ë°©ë²• ì„ íƒ
        if n_residuals < n_variables:  # 2ë°° ì¡°ê±´ ì œê±°
            print(f"  âš ï¸  Under-constrained problem: {n_residuals} residuals < {n_variables} variables")
            print("  Using 'trf' method with very conservative settings")
            method = 'trf'
        else:
            print("  Using 'lm' method")
            method = 'lm'
        
        try:
            params = self._pack_parameters()
        except Exception as e:
            print(f"  Parameter packing failed: {e}")
            return
        
        try:
            # ğŸ”§ ë” ì™„í™”ëœ BA ì„¤ì •
            if method == 'trf':
                result = least_squares(
                    self._compute_residuals_improved,
                    params,
                    method='trf',
                    max_nfev=max_iterations,  # ë°˜ë³µ íšŸìˆ˜ ì¤„ì„
                    verbose=1,
                    ftol=1e-3,  # ë” ì™„í™”ëœ ìˆ˜ë ´ ì¡°ê±´
                    xtol=1e-3,
                    bounds=(-np.inf, np.inf)
                )
            else:
                result = least_squares(
                    self._compute_residuals_improved,
                    params,
                    method='lm',
                    max_nfev=max_iterations * 2,  # ë°˜ë³µ íšŸìˆ˜ ì¤„ì„
                    verbose=1,
                    ftol=1e-4,  # ë” ì™„í™”ëœ ìˆ˜ë ´ ì¡°ê±´
                    xtol=1e-4
                )
            
            # ê²°ê³¼ ì–¸íŒ¨í‚¹
            self._unpack_parameters(result.x)
            
            print(f"  Bundle adjustment completed. Final cost: {result.cost:.6f}")
            print(f"  Method: {method}, Iterations: {result.nfev}")
            
            # ğŸ”§ ë” ì™„í™”ëœ cost í‰ê°€
            if result.cost > 1000:
                print(f"  âš ï¸  ë†’ì€ BA cost: {result.cost:.2f}")
                print("  í¬ì¸íŠ¸ í´ë¼ìš°ë“œ í’ˆì§ˆì´ ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
            elif result.cost > 100:
                print(f"  âš ï¸  ì¤‘ê°„ BA cost: {result.cost:.2f}")
            else:
                print(f"  âœ… ì¢‹ì€ BA cost: {result.cost:.2f}")
            
        except Exception as e:
            print(f"  Bundle adjustment failed: {e}")
            print("  Continuing without bundle adjustment...")

    def _compute_residuals_improved(self, params):
        """ê°œì„ ëœ Bundle Adjustment ì”ì°¨ ê³„ì‚° (ë” ì™„í™”ëœ ë²„ì „)"""
        residuals = []
        
        # íŒŒë¼ë¯¸í„° ì–¸íŒ¨í‚¹
        try:
            self._unpack_parameters(params)
        except Exception as e:
            print(f"    Warning: Parameter unpacking failed: {e}")
            return np.ones(100) * 1e6
        
        # ê° ê´€ì°°ì— ëŒ€í•œ ì¬íˆ¬ì˜ ì˜¤ì°¨ ê³„ì‚°
        for point_id, observations in self.point_observations.items():
            if point_id not in self.points_3d:
                continue
                
            point_3d = self.points_3d[point_id]['xyz']
            
            for cam_id, observed_pt, conf in observations:
                if cam_id not in self.cameras:
                    continue
                
                try:
                    cam = self.cameras[cam_id]
                    K = cam['K']
                    R = cam['R']
                    T = cam['T']
                    
                    # ì¹´ë©”ë¼ ì¢Œí‘œê³„ë¡œ ë³€í™˜
                    point_cam = R @ (point_3d - T)
                    
                    # ê¹Šì´ ì²´í¬ (ë” ì™„í™”ëœ ì¡°ê±´)
                    if point_cam[2] <= 0:
                        residuals.extend([10.0, 10.0])  # ë” ì‘ì€ í˜ë„í‹°
                        continue
                    
                    # ì¬íˆ¬ì˜
                    point_2d_proj = K @ point_cam
                    if abs(point_2d_proj[2]) < 1e-10:
                        residuals.extend([10.0, 10.0])
                        continue
                    point_2d_proj = point_2d_proj[:2] / point_2d_proj[2]
                    
                    # ğŸ”§ ë” ì™„í™”ëœ ì”ì°¨ ê³„ì‚°
                    residual = point_2d_proj - observed_pt
                    
                    # ğŸ”§ ë” ì™„í™”ëœ Huber loss
                    residual = self._apply_huber_loss_improved(residual, delta=10.0)  # 3.0 â†’ 10.0ìœ¼ë¡œ ì™„í™”
                    
                    # ğŸ”§ ë” ì™„í™”ëœ ì‹ ë¢°ë„ ê°€ì¤‘ì¹˜
                    weight = np.clip(conf, 0.05, 1.0)  # 0.1 â†’ 0.05ë¡œ ì™„í™”
                    
                    # ğŸ”§ ë” ì™„í™”ëœ ìŠ¤ì¼€ì¼ë§
                    residual = residual * weight * 0.01  # 0.05 â†’ 0.01ë¡œ ì™„í™”
                    
                    residuals.extend(residual)
                    
                except Exception as e:
                    residuals.extend([2.0, 2.0])  # ë” ì‘ì€ ê¸°ë³¸ ì˜¤ì°¨
        
        if len(residuals) == 0:
            return np.ones(100) * 1e6
        
        residuals = np.array(residuals)
        
        # NaNì´ë‚˜ ë¬´í•œëŒ€ ê°’ ì²´í¬
        if np.any(np.isnan(residuals)) or np.any(np.isinf(residuals)):
            return np.ones(len(residuals)) * 1e6
        
        return residuals

    def _apply_huber_loss_improved(self, residual, delta=3.0):
        """ê°œì„ ëœ Huber loss ì ìš©"""
        abs_residual = np.abs(residual)
        mask = abs_residual <= delta
        
        result = np.zeros_like(residual)
        result[mask] = residual[mask]
        result[~mask] = delta * np.sign(residual[~mask]) * (2 * np.sqrt(abs_residual[~mask] / delta) - 1)
        
        return result
    
    def _expand_point_observations(self):
        """í¬ì¸íŠ¸ ê´€ì°° ë°ì´í„° í™•ì¥ìœ¼ë¡œ ì”ì°¨ ìˆ˜ ì¦ê°€"""
        
        print("  Expanding point observations...")
        
        original_obs = sum(len(obs) for obs in self.point_observations.values())
        
        # ê° 3D í¬ì¸íŠ¸ì— ëŒ€í•´ ë‹¤ë¥¸ ì¹´ë©”ë¼ì—ì„œì˜ ì¬íˆ¬ì˜ í™•ì¸
        for point_id, point_data in self.points_3d.items():
            point_3d = point_data['xyz']
            current_cams = set([obs[0] for obs in self.point_observations[point_id]])
            
            # ë‹¤ë¥¸ ì¹´ë©”ë¼ë“¤ì—ì„œë„ ì´ í¬ì¸íŠ¸ê°€ ë³´ì´ëŠ”ì§€ í™•ì¸
            for cam_id in self.cameras:
                if cam_id in current_cams:
                    continue
                
                try:
                    # ì¬íˆ¬ì˜ ê³„ì‚°
                    cam = self.cameras[cam_id]
                    K, R, T = cam['K'], cam['R'], cam['T']
                    
                    point_cam = R @ (point_3d - T)
                    if point_cam[2] <= 0:  # ì¹´ë©”ë¼ ë’¤ìª½
                        continue
                    
                    point_2d_proj = K @ point_cam
                    point_2d_proj = point_2d_proj[:2] / point_2d_proj[2]
                    
                    # ì´ë¯¸ì§€ ê²½ê³„ í™•ì¸
                    h, w = self.image_features[cam_id]['image_size']
                    if (0 <= point_2d_proj[0] < w and 0 <= point_2d_proj[1] < h):
                        
                        # í•´ë‹¹ ì¹´ë©”ë¼ì˜ í‚¤í¬ì¸íŠ¸ì™€ ê°€ê¹Œìš´ì§€ í™•ì¸
                        kpts = self.image_features[cam_id]['keypoints']
                        distances = np.linalg.norm(kpts - point_2d_proj, axis=1)
                        min_idx = np.argmin(distances)
                        
                        if distances[min_idx] < 30.0:  # 30 í”½ì…€ ë‚´
                            # ê´€ì°° ì¶”ê°€
                            confidence = 0.1  # ë‚®ì€ ì‹ ë¢°ë„
                            self.point_observations[point_id].append((cam_id, point_2d_proj, confidence))
                            
                except Exception:
                    continue
        
        expanded_obs = sum(len(obs) for obs in self.point_observations.values())
        print(f"    Expanded observations: {original_obs} â†’ {expanded_obs}")
    
    def _rotation_matrix_to_angle_axis(self, R):
        """íšŒì „ í–‰ë ¬ì„ ë¡œë“œë¦¬ê²ŒìŠ¤ ë²¡í„°ë¡œ ë³€í™˜"""
        # ê°„ë‹¨í•œ êµ¬í˜„ (ì‹¤ì œë¡œëŠ” ë” ì •í™•í•œ ë³€í™˜ì´ í•„ìš”)
        trace = np.trace(R)
        if trace > 3 - 1e-6:
            return np.zeros(3)
        
        angle = np.arccos((trace - 1) / 2)
        axis = np.array([
            R[2, 1] - R[1, 2],
            R[0, 2] - R[2, 0],
            R[1, 0] - R[0, 1]
        ])
        
        if np.linalg.norm(axis) > 1e-6:
            axis = axis / np.linalg.norm(axis)
        
        return angle * axis
    
    def _angle_axis_to_rotation_matrix(self, angle_axis):
        """ë¡œë“œë¦¬ê²ŒìŠ¤ ë²¡í„°ë¥¼ íšŒì „ í–‰ë ¬ë¡œ ë³€í™˜"""
        angle = np.linalg.norm(angle_axis)
        if angle < 1e-6:
            return np.eye(3)
        
        axis = angle_axis / angle
        K = np.array([
            [0, -axis[2], axis[1]],
            [axis[2], 0, -axis[0]],
            [-axis[1], axis[0], 0]
        ])
        
        R = np.eye(3) + np.sin(angle) * K + (1 - np.cos(angle)) * (K @ K)
        return R
    
    def _refine_point_cloud(self):
        """í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ì •ì œ (ë” ì™„í™”ëœ ë²„ì „)"""
        print(f"  Refining point cloud...")
        
        # 1. ì¤‘ë³µ í¬ì¸íŠ¸ ì œê±° (ë” ì™„í™”ëœ ì¡°ê±´)
        points_to_remove = set()
        points_list = list(self.points_3d.items())
        
        for i, (id1, point1) in enumerate(points_list):
            for j, (id2, point2) in enumerate(points_list[i+1:], i+1):
                if id1 in points_to_remove or id2 in points_to_remove:
                    continue
                
                dist = np.linalg.norm(point1['xyz'] - point2['xyz'])
                if dist < 0.0001:  # 0.001 â†’ 0.0001ë¡œ ë” ì—„ê²©í•˜ê²Œ
                    points_to_remove.add(id2)
        
        # ì¤‘ë³µ í¬ì¸íŠ¸ ì œê±°
        for point_id in points_to_remove:
            del self.points_3d[point_id]
            if point_id in self.point_observations:
                del self.point_observations[point_id]
        
        print(f"  Removed {len(points_to_remove)} duplicate points")
        print(f"  Final point cloud: {len(self.points_3d)} points")
    
    def _get_projection_matrix(self, cam_id):
        """ì¹´ë©”ë¼ íˆ¬ì˜ í–‰ë ¬ ìƒì„± (ìˆ˜ì •ëœ ë²„ì „)"""
        cam = self.cameras[cam_id]
        K, R, T = cam['K'], cam['R'], cam['T']
        
        # Tê°€ ì›”ë“œ ì¢Œí‘œê³„ì˜ ì¹´ë©”ë¼ ì¤‘ì‹¬ì´ë¼ê³  ê°€ì •
        # P = K[R|t] where t = -R * T (ì¹´ë©”ë¼ ì¤‘ì‹¬ì„ ì¹´ë©”ë¼ ì¢Œí‘œê³„ë¡œ ë³€í™˜)
        t = -R @ T  # ì¹´ë©”ë¼ ì¤‘ì‹¬ì„ ì¹´ë©”ë¼ ì¢Œí‘œê³„ë¡œ ë³€í™˜
        RT = np.hstack([R, t.reshape(-1, 1)])
        P = K @ RT
        
        return P
    
    def _create_3dgs_scene_info(self, image_paths):
        """3DGSìš© SceneInfo ìƒì„± (ê°œì„ ëœ ë²„ì „)"""
        
        # Lazy import 3DGS modules
        CameraInfo, SceneInfo, BasicPointCloud = get_3dgs_imports()
        if CameraInfo is None:
            raise ImportError("3DGS modules not available")
        
        # CameraInfo ë¦¬ìŠ¤íŠ¸ ìƒì„±
        cam_infos = []
        for cam_id in sorted(self.cameras.keys()):
            cam = self.cameras[cam_id]
            image_path = self.image_features[cam_id]['image_path']
            h, w = self.image_features[cam_id]['image_size']
            
            # FoV ê³„ì‚°
            K = cam['K']
            focal_x, focal_y = K[0, 0], K[1, 1]
            FovX = 2 * np.arctan(w / (2 * focal_x))
            FovY = 2 * np.arctan(h / (2 * focal_y))
            
            # ë” ë‚˜ì€ í…ŒìŠ¤íŠ¸ ë¶„í•  (ì—°ê²°ì„± ê¸°ë°˜)
            is_test = self._should_be_test_camera(cam_id)
            
            cam_info = CameraInfo(
                uid=cam_id,
                R=cam['R'],
                T=cam['T'],
                FovY=float(FovY),
                FovX=float(FovX),
                image_path=image_path,
                image_name=Path(image_path).name,
                width=w,
                height=h,
                depth_params=None,
                depth_path="",
                is_test=is_test
            )
            cam_infos.append(cam_info)
        
        # í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ìƒì„±
        if self.points_3d:
            points = np.array([pt['xyz'] for pt in self.points_3d.values()])
            colors = np.array([pt['color'] for pt in self.points_3d.values()])
            
            # ë²•ì„  ë²¡í„° (ê°œì„ ëœ ê³„ì‚°)
            normals = self._compute_point_normals(points)
            
            pcd = BasicPointCloud(points=points, colors=colors, normals=normals)
        else:
            # ê¸°ë³¸ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ (ë” ë§ì€ ìˆ˜)
            n_points = 25000  # 15000 â†’ 25000ë¡œ ì¦ê°€
            points = np.random.randn(n_points, 3).astype(np.float32) * 4  # 3 â†’ 4ë¡œ ì¦ê°€
            colors = np.random.rand(n_points, 3).astype(np.float32)
            normals = np.random.randn(n_points, 3).astype(np.float32)
            normals = normals / np.linalg.norm(normals, axis=1, keepdims=True)
            
            # BasicPointCloudê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ fallback
            try:
                pcd = BasicPointCloud(points=points, colors=colors, normals=normals)
            except NameError:
                # BasicPointCloudê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ê°„ë‹¨í•œ í´ë˜ìŠ¤ ìƒì„±
                class BasicPointCloud:
                    def __init__(self, points, colors, normals):
                        self.points = points
                        self.colors = colors
                        self.normals = normals
                
                pcd = BasicPointCloud(points=points, colors=colors, normals=normals)
        
        # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
        train_cams = [c for c in cam_infos if not c.is_test]
        test_cams = [c for c in cam_infos if c.is_test]
        
        # NeRF ì •ê·œí™”
        nerf_norm = self._compute_nerf_normalization(train_cams)
        
        return SceneInfo(
            point_cloud=pcd,
            train_cameras=train_cams,
            test_cameras=test_cams,
            nerf_normalization=nerf_norm,
            ply_path="",
            is_nerf_synthetic=False
        )
    
    def _should_be_test_camera(self, cam_id):
        """ë” ë‚˜ì€ í…ŒìŠ¤íŠ¸ ì¹´ë©”ë¼ ì„ íƒ"""
        # ì—°ê²°ì„±ì´ ë‚®ì€ ì¹´ë©”ë¼ë¥¼ í…ŒìŠ¤íŠ¸ë¡œ ì„ íƒ
        connectivity = len(self.camera_graph.get(cam_id, []))
        
        # ì—°ê²°ì„±ì´ 1 ì´í•˜ì´ê±°ë‚˜, íŠ¹ì • ê°„ê²©ìœ¼ë¡œ ì„ íƒ
        if connectivity <= 1:
            return True
        
        # 10ê°œë§ˆë‹¤ 1ê°œì”© í…ŒìŠ¤íŠ¸ë¡œ ì„ íƒ (ì—°ê²°ì„±ì´ ë†’ì€ ì¹´ë©”ë¼ë“¤ ì¤‘ì—ì„œ)
        if cam_id % 10 == 0 and connectivity >= 2:
            return True
        
        return False
    
    def _compute_point_normals(self, points):
        """í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ë²•ì„  ë²¡í„° ê³„ì‚°"""
        if len(points) < 3:
            return np.random.randn(len(points), 3).astype(np.float32)
        
        # ê°„ë‹¨í•œ ë²•ì„  ê³„ì‚° (sklearn ì˜ì¡´ì„± ì œê±°)
        try:
            normals = np.zeros_like(points)
            
            for i in range(len(points)):
                # í˜„ì¬ í¬ì¸íŠ¸
                current_point = points[i]
                
                # ë‹¤ë¥¸ ëª¨ë“  í¬ì¸íŠ¸ì™€ì˜ ê±°ë¦¬ ê³„ì‚°
                distances = np.linalg.norm(points - current_point, axis=1)
                
                # ê°€ì¥ ê°€ê¹Œìš´ 10ê°œ í¬ì¸íŠ¸ ì„ íƒ (ìê¸° ìì‹  ì œì™¸)
                nearest_indices = np.argsort(distances)[1:11]  # ìê¸° ìì‹  ì œì™¸
                
                if len(nearest_indices) < 3:
                    normals[i] = np.random.randn(3)
                    continue
                
                # ì´ì›ƒ í¬ì¸íŠ¸ë“¤ì˜ ì¤‘ì‹¬ ê³„ì‚°
                neighbors = points[nearest_indices]
                centroid = np.mean(neighbors, axis=0)
                
                # ê³µë¶„ì‚° í–‰ë ¬ ê³„ì‚°
                centered = neighbors - centroid
                cov_matrix = centered.T @ centered
                
                # ê°€ì¥ ì‘ì€ ê³ ìœ ê°’ì— í•´ë‹¹í•˜ëŠ” ê³ ìœ ë²¡í„°ê°€ ë²•ì„ 
                eigenvals, eigenvecs = np.linalg.eigh(cov_matrix)
                normal = eigenvecs[:, 0]  # ê°€ì¥ ì‘ì€ ê³ ìœ ê°’
                
                # ë°©í–¥ ì¼ê´€ì„± í™•ì¸
                if normal[2] < 0:
                    normal = -normal
                
                normals[i] = normal
            
            # ì •ê·œí™”
            norms = np.linalg.norm(normals, axis=1, keepdims=True)
            norms[norms == 0] = 1
            normals = normals / norms
            
        except Exception as e:
            print(f"    Warning: Normal computation failed: {e}")
            # ì‹¤íŒ¨ì‹œ ëœë¤ ë²•ì„ 
            normals = np.random.randn(len(points), 3).astype(np.float32)
            normals = normals / np.linalg.norm(normals, axis=1, keepdims=True)
        
        return normals.astype(np.float32)
    
    def _create_default_pointcloud(self):
        """ê¸°ë³¸ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ìƒì„± (ê°œì„ ëœ ë²„ì „)"""
        # Lazy import 3DGS modules
        _, _, BasicPointCloud = get_3dgs_imports()
        if BasicPointCloud is None:
            # Fallback: ê°„ë‹¨í•œ í´ë˜ìŠ¤ ì •ì˜
            class BasicPointCloud:
                def __init__(self, points, colors, normals):
                    self.points = points
                    self.colors = colors
                    self.normals = normals
        
        # ì¹´ë©”ë¼ ìœ„ì¹˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ë” í˜„ì‹¤ì ì¸ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ
        if len(self.cameras) > 0:
            # ì¹´ë©”ë¼ ì¤‘ì‹¬ ê³„ì‚°
            camera_centers = []
            for cam_id in self.cameras:
                R, T = self.cameras[cam_id]['R'], self.cameras[cam_id]['T']
                center = -R.T @ T
                camera_centers.append(center)
            
            if camera_centers:
                camera_centers = np.array(camera_centers)
                center_mean = np.mean(camera_centers, axis=0)
                center_std = np.std(camera_centers, axis=0)
                
                # ì‹¤ì œ í¬ì¸íŠ¸ê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±
                if self.points_3d:
                    actual_points = np.array([pt['xyz'] for pt in self.points_3d.values()])
                    if len(actual_points) > 0:
                        # ì‹¤ì œ í¬ì¸íŠ¸ ì£¼ë³€ì— ì¶”ê°€ í¬ì¸íŠ¸ ìƒì„±
                        n_additional = 15000  # ë” ë§ì€ ì¶”ê°€ í¬ì¸íŠ¸ (5000 â†’ 15000)
                        points = np.random.randn(n_additional, 3).astype(np.float32)
                        points = points * np.std(actual_points, axis=0) * 0.8 + np.mean(actual_points, axis=0)
                        
                        # ì‹¤ì œ í¬ì¸íŠ¸ì™€ í•©ì¹˜ê¸°
                        points = np.vstack([actual_points, points])
                        colors = np.random.rand(len(points), 3).astype(np.float32)
                        normals = self._compute_point_normals(points)
                    else:
                        # ì¹´ë©”ë¼ ë¶„í¬ë¥¼ ê³ ë ¤í•œ í¬ì¸íŠ¸ ìƒì„±
                        n_points = 20000  # ë” ë§ì€ ìˆ˜ (10000 â†’ 20000)
                        points = np.random.randn(n_points, 3).astype(np.float32)
                        points = points * center_std * 0.8 + center_mean
                        colors = np.random.rand(n_points, 3).astype(np.float32)
                        normals = self._compute_point_normals(points)
                else:
                    # ì¹´ë©”ë¼ ë¶„í¬ë¥¼ ê³ ë ¤í•œ í¬ì¸íŠ¸ ìƒì„±
                    n_points = 20000  # ë” ë§ì€ ìˆ˜ (10000 â†’ 20000)
                    points = np.random.randn(n_points, 3).astype(np.float32)
                    points = points * center_std * 0.8 + center_mean
                    colors = np.random.rand(n_points, 3).astype(np.float32)
                    normals = self._compute_point_normals(points)
            else:
                # ê¸°ë³¸ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ (ë” ì ì€ ìˆ˜)
                points = np.random.randn(10000, 3).astype(np.float32) * 3
                colors = np.random.rand(10000, 3).astype(np.float32)
                normals = np.random.randn(10000, 3).astype(np.float32)
                normals = normals / np.linalg.norm(normals, axis=1, keepdims=True)
        else:
            # ê¸°ë³¸ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ (ë” ì ì€ ìˆ˜)
            points = np.random.randn(10000, 3).astype(np.float32) * 3
            colors = np.random.rand(10000, 3).astype(np.float32)
            normals = np.random.randn(10000, 3).astype(np.float32)
            normals = normals / np.linalg.norm(normals, axis=1, keepdims=True)
        
        return BasicPointCloud(points=points, colors=colors, normals=normals)
    
    def _compute_nerf_normalization(self, cam_infos):
        """NeRF ì •ê·œí™” íŒŒë¼ë¯¸í„° ê³„ì‚° (ê°œì„ ëœ ë²„ì „)"""
        # Lazy import 3DGS modules
        try:
            from utils.graphics_utils import getWorld2View2
        except ImportError:
            # Fallback: ê°„ë‹¨í•œ í•¨ìˆ˜ ì •ì˜
            def getWorld2View2(R, t):
                Rt = np.zeros((4, 4))
                Rt[:3, :3] = R
                Rt[:3, 3] = t
                Rt[3, 3] = 1.0
                return Rt
        
        if not cam_infos:
            return {"translate": np.zeros(3), "radius": 1.0}
        
        # ì¹´ë©”ë¼ ì¤‘ì‹¬ ê³„ì‚°
        cam_centers = []
        for cam in cam_infos:
            W2C = getWorld2View2(cam.R, cam.T)
            C2W = np.linalg.inv(W2C)
            cam_centers.append(C2W[:3, 3:4])
        
        cam_centers = np.hstack(cam_centers)
        
        # ë” ì•ˆì •ì ì¸ ì¤‘ì‹¬ ê³„ì‚° (ì¤‘ê°„ê°’ ì‚¬ìš©)
        center = np.median(cam_centers, axis=1, keepdims=True).flatten()
        
        # ê±°ë¦¬ ê³„ì‚°
        distances = np.linalg.norm(cam_centers - center.reshape(-1, 1), axis=0)
        
        # ë” ë³´ìˆ˜ì ì¸ ë°˜ì§€ë¦„ ê³„ì‚° (95 í¼ì„¼íƒ€ì¼ ì‚¬ìš©)
        radius = np.percentile(distances, 95) * 1.2
        
        # ìµœì†Œ ë°˜ì§€ë¦„ ë³´ì¥
        radius = max(radius, 1.0)
        
        return {"translate": -center, "radius": radius}
    
    def _save_3dgs_format(self, scene_info, output_dir):
        """3DGS í•™ìŠµì„ ìœ„í•œ íŒŒì¼ êµ¬ì¡° ìƒì„±"""
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True, parents=True)
        
        # COLMAP í˜¸í™˜ ë””ë ‰í† ë¦¬ êµ¬ì¡°
        sparse_dir = output_dir / "sparse" / "0"
        sparse_dir.mkdir(exist_ok=True, parents=True)
        
        images_dir = output_dir / "images"
        images_dir.mkdir(exist_ok=True)
        
        # 1. ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„° ì €ì¥ (cameras.txt + cameras.bin)
        self._write_cameras_txt(scene_info.train_cameras + scene_info.test_cameras, 
                               sparse_dir / "cameras.txt")
        self._write_cameras_bin(scene_info.train_cameras + scene_info.test_cameras, 
                               sparse_dir / "cameras.bin")
        
        # 2. ì¹´ë©”ë¼ í¬ì¦ˆ ì €ì¥ (images.txt + images.bin)
        self._write_images_txt(scene_info.train_cameras + scene_info.test_cameras, 
                              sparse_dir / "images.txt")
        self._write_images_bin(scene_info.train_cameras + scene_info.test_cameras, 
                              sparse_dir / "images.bin")
        
        # 3. 3D í¬ì¸íŠ¸ ì €ì¥ (points3D.ply + points3D.bin)
        self._write_points3d_ply(scene_info.point_cloud, sparse_dir / "points3D.ply")
        self._write_points3d_bin(scene_info.point_cloud, sparse_dir / "points3D.bin")
        
        # 4. ì´ë¯¸ì§€ ë³µì‚¬ ë˜ëŠ” ì‹¬ë³¼ë¦­ ë§í¬
        self._setup_images_directory(scene_info.train_cameras + scene_info.test_cameras, 
                                    images_dir)
        
        print(f"  3DGS-compatible files saved to {output_dir}")
        print(f"  Use: python train.py -s {output_dir} -m {output_dir}/3dgs_output")
    
    def _write_cameras_txt(self, cam_infos, output_path):
        """COLMAP í˜•ì‹ cameras.txt ìƒì„±"""
        with open(output_path, 'w') as f:
            f.write("# Camera list with one line of data per camera:\n")
            f.write("# CAMERA_ID, MODEL, WIDTH, HEIGHT, PARAMS[]\n")
            f.write(f"# Number of cameras: {len(cam_infos)}\n")
            
            for cam in cam_infos:
                # PINHOLE ëª¨ë¸ ì‚¬ìš©
                focal_x = cam.width / (2 * np.tan(cam.FovX / 2))
                focal_y = cam.height / (2 * np.tan(cam.FovY / 2))
                cx, cy = cam.width / 2, cam.height / 2
                
                f.write(f"{cam.uid} PINHOLE {cam.width} {cam.height} "
                       f"{focal_x:.6f} {focal_y:.6f} {cx:.6f} {cy:.6f}\n")
    
    def _write_cameras_bin(self, cam_infos, output_path):
        """COLMAP í˜•ì‹ cameras.bin ìƒì„±"""
        try:
            from scene.colmap_loader import write_intrinsics_binary
            cameras = {}
            
            for cam in cam_infos:
                # PINHOLE ëª¨ë¸ ì‚¬ìš©
                focal_x = cam.width / (2 * np.tan(cam.FovX / 2))
                focal_y = cam.height / (2 * np.tan(cam.FovY / 2))
                cx, cy = cam.width / 2, cam.height / 2
                
                # COLMAP Camera ê°ì²´ ìƒì„±
                from scene.colmap_loader import Camera
                camera = Camera(
                    id=cam.uid,
                    model="PINHOLE",
                    width=cam.width,
                    height=cam.height,
                    params=[focal_x, focal_y, cx, cy]
                )
                cameras[cam.uid] = camera
            
            write_intrinsics_binary(cameras, str(output_path))
            print(f"    Created cameras.bin with {len(cameras)} cameras")
            
        except ImportError:
            print(f"    Warning: COLMAP loader not available, creating simple cameras.bin")
            self._write_cameras_bin_simple(cam_infos, output_path)
        except Exception as e:
            print(f"    Warning: Failed to create cameras.bin: {e}")
            self._write_cameras_bin_simple(cam_infos, output_path)
    
    def _write_cameras_bin_simple(self, cam_infos, output_path):
        """ê°„ë‹¨í•œ cameras.bin ìƒì„± (COLMAP loader ì—†ì´)"""
        try:
            import struct
            
            with open(output_path, 'wb') as f:
                # ì¹´ë©”ë¼ ìˆ˜
                f.write(struct.pack('<Q', len(cam_infos)))
                
                for cam in cam_infos:
                    # PINHOLE ëª¨ë¸ ì‚¬ìš©
                    focal_x = cam.width / (2 * np.tan(cam.FovX / 2))
                    focal_y = cam.height / (2 * np.tan(cam.FovY / 2))
                    cx, cy = cam.width / 2, cam.height / 2
                    
                    # ì¹´ë©”ë¼ ID (int)
                    f.write(struct.pack('<i', cam.uid))
                    
                    # ëª¨ë¸ ID (PINHOLE = 1)
                    model_id = 1
                    f.write(struct.pack('<i', model_id))
                    
                    # ë„ˆë¹„, ë†’ì´ (unsigned long long)
                    f.write(struct.pack('<Q', cam.width))
                    f.write(struct.pack('<Q', cam.height))
                    
                    # íŒŒë¼ë¯¸í„°ë“¤ (double)
                    params = [focal_x, focal_y, cx, cy]
                    for param in params:
                        f.write(struct.pack('<d', param))
            
            print(f"    Created simple cameras.bin with {len(cam_infos)} cameras")
            
        except Exception as e:
            print(f"    Error creating simple cameras.bin: {e}")
            import traceback
            traceback.print_exc()
    
    def _write_images_bin(self, cam_infos, output_path):
        """COLMAP í˜•ì‹ images.bin ìƒì„±"""
        try:
            from scene.colmap_loader import write_extrinsics_binary
            images = {}
            
            for cam in cam_infos:
                # íšŒì „ í–‰ë ¬ì„ ì¿¼í„°ë‹ˆì–¸ìœ¼ë¡œ ë³€í™˜
                R = cam.R
                trace = np.trace(R)
                
                if trace > 0:
                    s = np.sqrt(trace + 1.0) * 2
                    qw = 0.25 * s
                    qx = (R[2, 1] - R[1, 2]) / s
                    qy = (R[0, 2] - R[2, 0]) / s
                    qz = (R[1, 0] - R[0, 1]) / s
                else:
                    qw = np.sqrt(1 + R[0,0] + R[1,1] + R[2,2]) / 2
                    qx = (R[2,1] - R[1,2]) / (4 * qw) if qw != 0 else 0
                    qy = (R[0,2] - R[2,0]) / (4 * qw) if qw != 0 else 0
                    qz = (R[1,0] - R[0,1]) / (4 * qw) if qw != 0 else 0
                
                # ì •ê·œí™”
                q_norm = np.sqrt(qw*qw + qx*qx + qy*qy + qz*qz)
                if q_norm > 0:
                    qw, qx, qy, qz = qw/q_norm, qx/q_norm, qy/q_norm, qz/q_norm
                
                # COLMAP Image ê°ì²´ ìƒì„±
                from scene.colmap_loader import Image
                image = Image(
                    id=cam.uid,
                    qvec=np.array([qw, qx, qy, qz]),
                    tvec=cam.T,
                    camera_id=cam.uid,
                    name=cam.image_name,
                    xys=np.array([]),  # ë¹ˆ íŠ¹ì§•ì  ë°°ì—´
                    point3D_ids=np.array([])  # ë¹ˆ 3D í¬ì¸íŠ¸ ID ë°°ì—´
                )
                images[cam.uid] = image
            
            write_extrinsics_binary(images, str(output_path))
            print(f"    Created images.bin with {len(images)} images")
            
        except ImportError:
            print(f"    Warning: COLMAP loader not available, creating simple images.bin")
            self._write_images_bin_simple(cam_infos, output_path)
        except Exception as e:
            print(f"    Warning: Failed to create images.bin: {e}")
            self._write_images_bin_simple(cam_infos, output_path)
    
    def _write_images_bin_simple(self, cam_infos, output_path):
        """ê°„ë‹¨í•œ images.bin ìƒì„± (COLMAP loader ì—†ì´)"""
        try:
            import struct
            
            with open(output_path, 'wb') as f:
                # ì´ë¯¸ì§€ ìˆ˜
                f.write(struct.pack('<Q', len(cam_infos)))
                
                for cam in cam_infos:
                    # íšŒì „ í–‰ë ¬ì„ ì¿¼í„°ë‹ˆì–¸ìœ¼ë¡œ ë³€í™˜
                    R = cam.R
                    trace = np.trace(R)
                    
                    if trace > 0:
                        s = np.sqrt(trace + 1.0) * 2
                        qw = 0.25 * s
                        qx = (R[2, 1] - R[1, 2]) / s
                        qy = (R[0, 2] - R[2, 0]) / s
                        qz = (R[1, 0] - R[0, 1]) / s
                    else:
                        qw = np.sqrt(1 + R[0,0] + R[1,1] + R[2,2]) / 2
                        qx = (R[2,1] - R[1,2]) / (4 * qw) if qw != 0 else 0
                        qy = (R[0,2] - R[2,0]) / (4 * qw) if qw != 0 else 0
                        qz = (R[1,0] - R[0,1]) / (4 * qw) if qw != 0 else 0
                    
                    # ì •ê·œí™”
                    q_norm = np.sqrt(qw*qw + qx*qx + qy*qy + qz*qz)
                    if q_norm > 0:
                        qw, qx, qy, qz = qw/q_norm, qx/q_norm, qy/q_norm, qz/q_norm
                    
                    # ì´ë¯¸ì§€ ID
                    f.write(struct.pack('<Q', cam.uid))
                    
                    # ì¿¼í„°ë‹ˆì–¸ (qw, qx, qy, qz)
                    f.write(struct.pack('<dddd', qw, qx, qy, qz))
                    
                    # ì´ë™ ë²¡í„° (tx, ty, tz)
                    f.write(struct.pack('<ddd', cam.T[0], cam.T[1], cam.T[2]))
                    
                    # ì¹´ë©”ë¼ ID
                    f.write(struct.pack('<Q', cam.uid))
                    
                    # ì´ë¯¸ì§€ ì´ë¦„ ê¸¸ì´ì™€ ì´ë¦„
                    name_bytes = cam.image_name.encode('utf-8')
                    f.write(struct.pack('<Q', len(name_bytes)))
                    f.write(name_bytes)
                    
                    # íŠ¹ì§•ì  ìˆ˜ (0ê°œ)
                    f.write(struct.pack('<Q', 0))
                    
                    # 3D í¬ì¸íŠ¸ ID ìˆ˜ (0ê°œ)
                    f.write(struct.pack('<Q', 0))
            
            print(f"    Created simple images.bin with {len(cam_infos)} images")
            
        except Exception as e:
            print(f"    Error creating simple images.bin: {e}")
    
    def _write_points3d_bin(self, point_cloud, output_path):
        """COLMAP í˜•ì‹ points3D.bin ìƒì„±"""
        try:
            from scene.colmap_loader import write_points3D_binary
            points3d = {}
            
            points = point_cloud.points
            colors = point_cloud.colors
            
            for i in range(len(points)):
                # COLMAP Point3D ê°ì²´ ìƒì„±
                from scene.colmap_loader import Point3D
                point3d = Point3D(
                    id=i,
                    xyz=points[i],
                    rgb=colors[i] * 255,  # 0-1 ë²”ìœ„ë¥¼ 0-255ë¡œ ë³€í™˜
                    error=0.0,  # ê¸°ë³¸ ì˜¤ì°¨
                    track=[]  # ë¹ˆ íŠ¸ë™ (ê´€ì°° ì •ë³´ ì—†ìŒ)
                )
                points3d[i] = point3d
            
            write_points3D_binary(points3d, str(output_path))
            print(f"    Created points3D.bin with {len(points3d)} points")
            
        except ImportError:
            print(f"    Warning: COLMAP loader not available, creating simple points3D.bin")
            self._write_points3d_bin_simple(point_cloud, output_path)
        except Exception as e:
            print(f"    Warning: Failed to create points3D.bin: {e}")
            self._write_points3d_bin_simple(point_cloud, output_path)
    
    def _write_points3d_bin_simple(self, point_cloud, output_path):
        """ê°„ë‹¨í•œ points3D.bin ìƒì„± (COLMAP loader ì—†ì´)"""
        try:
            import struct
            
            points = point_cloud.points
            colors = point_cloud.colors
            
            with open(output_path, 'wb') as f:
                # í¬ì¸íŠ¸ ìˆ˜
                f.write(struct.pack('<Q', len(points)))
                
                for i in range(len(points)):
                    # í¬ì¸íŠ¸ ID
                    f.write(struct.pack('<Q', i))
                    
                    # 3D ì¢Œí‘œ (x, y, z)
                    f.write(struct.pack('<ddd', points[i][0], points[i][1], points[i][2]))
                    
                    # RGB ìƒ‰ìƒ (0-255 ë²”ìœ„ë¡œ ë³€í™˜)
                    rgb = (colors[i] * 255).astype(np.uint8)
                    f.write(struct.pack('<BBB', rgb[0], rgb[1], rgb[2]))
                    
                    # ì˜¤ì°¨ (0.0)
                    f.write(struct.pack('<d', 0.0))
                    
                    # íŠ¸ë™ ê¸¸ì´ (0ê°œ)
                    f.write(struct.pack('<Q', 0))
            
            print(f"    Created simple points3D.bin with {len(points)} points")
            
        except Exception as e:
            print(f"    Error creating simple points3D.bin: {e}")
    
    def _write_images_txt(self, cam_infos, output_path):
        """COLMAP í˜•ì‹ images.txt ìƒì„±"""
        with open(output_path, 'w') as f:
            f.write("# Image list with two lines of data per image:\n")
            f.write("# IMAGE_ID, QW, QX, QY, QZ, TX, TY, TZ, CAMERA_ID, NAME\n")
            f.write("# POINTS2D[] as (X, Y, POINT3D_ID)\n")
            f.write(f"# Number of images: {len(cam_infos)}\n")
            
            for cam in cam_infos:
                # íšŒì „ í–‰ë ¬ì„ ì¿¼í„°ë‹ˆì–¸ìœ¼ë¡œ ë³€í™˜
                R = cam.R
                trace = np.trace(R)
                
                if trace > 0:
                    s = np.sqrt(trace + 1.0) * 2  # s = 4 * qw
                    qw = 0.25 * s
                    qx = (R[2, 1] - R[1, 2]) / s
                    qy = (R[0, 2] - R[2, 0]) / s
                    qz = (R[1, 0] - R[0, 1]) / s
                else:
                    # ì•ˆì •ì ì¸ ì¿¼í„°ë‹ˆì–¸ ë³€í™˜
                    qw = np.sqrt(1 + R[0,0] + R[1,1] + R[2,2]) / 2
                    qx = (R[2,1] - R[1,2]) / (4 * qw) if qw != 0 else 0
                    qy = (R[0,2] - R[2,0]) / (4 * qw) if qw != 0 else 0
                    qz = (R[1,0] - R[0,1]) / (4 * qw) if qw != 0 else 0
                
                # ì •ê·œí™”
                q_norm = np.sqrt(qw*qw + qx*qx + qy*qy + qz*qz)
                if q_norm > 0:
                    qw, qx, qy, qz = qw/q_norm, qx/q_norm, qy/q_norm, qz/q_norm
                
                f.write(f"{cam.uid} {qw:.6f} {qx:.6f} {qy:.6f} {qz:.6f} "
                       f"{cam.T[0]:.6f} {cam.T[1]:.6f} {cam.T[2]:.6f} "
                       f"{cam.uid} {cam.image_name}\n")
                f.write("\n")  # ë¹ˆ íŠ¹ì§•ì  ë¼ì¸
    
    def _write_points3d_ply(self, point_cloud, output_path):
        """PLY í˜•ì‹ìœ¼ë¡œ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ì €ì¥"""
        points = point_cloud.points
        colors = (point_cloud.colors * 255).astype(np.uint8)
        
        with open(output_path, 'w') as f:
            # PLY í—¤ë”
            f.write("ply\n")
            f.write("format ascii 1.0\n")
            f.write(f"element vertex {len(points)}\n")
            f.write("property float x\n")
            f.write("property float y\n")
            f.write("property float z\n")
            f.write("property float nx\n")
            f.write("property float ny\n")
            f.write("property float nz\n")
            f.write("property uchar red\n")
            f.write("property uchar green\n")
            f.write("property uchar blue\n")
            f.write("end_header\n")
            
            # ë°ì´í„°
            for i in range(len(points)):
                x, y, z = points[i]
                nx, ny, nz = point_cloud.normals[i]
                r, g, b = colors[i]
                f.write(f"{x:.6f} {y:.6f} {z:.6f} "
                       f"{nx:.6f} {ny:.6f} {nz:.6f} "
                       f"{r} {g} {b}\n")
    
    def _setup_images_directory(self, cam_infos, images_dir):
        """ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ì„¤ì • (ì‹¬ë³¼ë¦­ ë§í¬ ë˜ëŠ” ë³µì‚¬)"""
        import shutil
        
        for cam in cam_infos:
            src_path = Path(cam.image_path)
            dst_path = images_dir / cam.image_name
            
            if not dst_path.exists():
                try:
                    # ì‹¬ë³¼ë¦­ ë§í¬ ì‹œë„
                    dst_path.symlink_to(src_path.resolve())
                except (OSError, NotImplementedError):
                    # ì‹¤íŒ¨ì‹œ ë³µì‚¬
                    shutil.copy2(src_path, dst_path)
    
    def _calculate_adaptive_resize(self, image_path, max_dim=1600):
        """SuperGlue ê¶Œì¥ í•´ìƒë„(ìµœëŒ€ max_dim)ë¡œ ë¹„ìœ¨ ìœ ì§€ ë¦¬ì‚¬ì´ì¦ˆ ê³„ì‚°"""
        try:
            img = cv2.imread(str(image_path))
            if img is None:
                return [1024, 768]  # ê¸°ë³¸ê°’
            h, w = img.shape[:2]
            largest = max(h, w)
            if largest <= max_dim:
                return None  # ì›ë³¸ í¬ê¸° ìœ ì§€
            scale = max_dim / largest
            return [int(w * scale), int(h * scale)]
        except:
            return [1024, 768]  # ê¸°ë³¸ê°’

    def _load_image(self, image_path, resize=None):
        """ì´ë¯¸ì§€ ë¡œë“œ ë° SuperGlue ê¶Œì¥ í•´ìƒë„ ì ìš©"""
        try:
            image = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)
            if image is None:
                print(f"    Warning: Failed to load {image_path}")
                return None

            # SuperGlue ê¶Œì¥ í•´ìƒë„ ì ìš© (ìµœëŒ€ 1600px)
            if resize is None:
                resize = self._calculate_adaptive_resize(image_path, max_dim=1600)
            if resize is not None:
                image = cv2.resize(image, tuple(resize))
            return image.astype(np.float32)
        except Exception as e:
            print(f"    Error loading {image_path}: {e}")
            return None
    
    def _pack_parameters(self):
        """ì¹´ë©”ë¼ í¬ì¦ˆì™€ 3D í¬ì¸íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ íŒ¨í‚¹"""
        params = []
        
        # ì¹´ë©”ë¼ í¬ì¦ˆ (íšŒì „ + ì´ë™)
        for cam_id in sorted(self.cameras.keys()):
            cam = self.cameras[cam_id]
            R = cam['R']
            T = cam['T']
            
            # íšŒì „ í–‰ë ¬ì„ ë¡œë“œë¦¬ê²ŒìŠ¤ ë²¡í„°ë¡œ ë³€í™˜
            angle_axis = self._rotation_matrix_to_angle_axis(R)
            params.extend(angle_axis)
            params.extend(T)
        
        # 3D í¬ì¸íŠ¸
        for point_id in sorted(self.points_3d.keys()):
            point = self.points_3d[point_id]['xyz']
            params.extend(point)
        
        params = np.array(params)
        
        # NaNì´ë‚˜ ë¬´í•œëŒ€ ê°’ ì²´í¬
        if np.any(np.isnan(params)) or np.any(np.isinf(params)):
            raise ValueError("Invalid parameters detected (NaN or Inf)")
        
        return params
    
    def _unpack_parameters(self, params):
        """ë²¡í„°ì—ì„œ ì¹´ë©”ë¼ í¬ì¦ˆì™€ 3D í¬ì¸íŠ¸ ì–¸íŒ¨í‚¹"""
        idx = 0
        
        # ì¹´ë©”ë¼ í¬ì¦ˆ ë³µì›
        for cam_id in sorted(self.cameras.keys()):
            # ë¡œë“œë¦¬ê²ŒìŠ¤ ë²¡í„° (3ê°œ)
            angle_axis = params[idx:idx+3]
            idx += 3
            
            # ì´ë™ ë²¡í„° (3ê°œ)
            T = params[idx:idx+3]
            idx += 3
            
            # íšŒì „ í–‰ë ¬ë¡œ ë³€í™˜
            R = self._angle_axis_to_rotation_matrix(angle_axis)
            
            self.cameras[cam_id]['R'] = R.astype(np.float32)
            self.cameras[cam_id]['T'] = T.astype(np.float32)
        
        # 3D í¬ì¸íŠ¸ ë³µì›
        for point_id in sorted(self.points_3d.keys()):
            xyz = params[idx:idx+3]
            idx += 3
            self.points_3d[point_id]['xyz'] = xyz.astype(np.float32)

class AdaptiveMatcher:
    """Adaptive Matching: CLIP ê¸°ë°˜ global descriptor, cosine similarity, ìƒìœ„ Nê°œ ìŒ ì„ ì •"""
    def __init__(self, config, device):
        self.config = config
        self.device = device
        self.top_k = config.get('adaptive_top_k', 20)
        self.model, self.preprocess = clip.load("ViT-B/32", device=self.device)

    def compute_global_descriptors(self, image_paths):
        global_descs = []
        for path in image_paths:
            img = PILImage.open(path).convert("RGB")
            img_tensor = self.preprocess(img).unsqueeze(0).to(self.device)
            with torch.no_grad():
                desc = self.model.encode_image(img_tensor).cpu().numpy().flatten()
                desc = desc / (np.linalg.norm(desc) + 1e-10)
            global_descs.append(desc)
        return global_descs

    def select_topk_pairs(self, global_descs):
        import numpy as np
        n = len(global_descs)
        sim_matrix = np.zeros((n, n))
        for i in range(n):
            for j in range(i+1, n):
                sim = np.dot(global_descs[i], global_descs[j])
                sim_matrix[i, j] = sim
                sim_matrix[j, i] = sim
        pairs = set()
        for i in range(n):
            topk = np.argsort(sim_matrix[i])[::-1][:self.top_k]
            for j in topk:
                if i != j:
                    pairs.add(tuple(sorted((i, j))))
        return list(pairs)

def readSuperGlueSceneInfo(path, images, eval, train_test_exp=False, llffhold=8, 
                          superglue_config="outdoor", max_images=100):
    """SuperGlue ê¸°ë°˜ ì™„ì „í•œ SfMìœ¼ë¡œ SceneInfo ìƒì„±"""
    
    print("=== SuperGlue Complete SfM Pipeline ===")
    print(f"ğŸš€ Pipeline available: {PIPELINE_AVAILABLE}")
    
    if not PIPELINE_AVAILABLE:
        print("âŒ Pipeline not available. Using fallback scene creation...")
        # ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        images_folder = Path(path) / (images if images else "images")
        return _create_fallback_scene_info(images_folder, max_images)
    
    # ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ê²½ë¡œ
    images_folder = Path(path) / (images if images else "images")
    output_folder = Path(path) / "superglue_sfm_output"
    
    # SuperGlue ì„¤ì • (ë” ì™„í™”ëœ ì„¤ì •)
    config = {
        'matcher': 'superglue',  # ì¶”ê°€: matcher íƒ€ì…(superglue, loftr, lightglue ë“±)
        'superpoint': {
            'nms_radius': 3,  # 4 â†’ 3ìœ¼ë¡œ ì™„í™”
            'keypoint_threshold': 0.001,  # 0.005 â†’ 0.001ë¡œ ëŒ€í­ ì™„í™”
            'max_keypoints': 8192  # 4096 â†’ 8192ë¡œ ì¦ê°€
        },
        'superglue': {
            'weights': superglue_config,  # 'indoor' ë˜ëŠ” 'outdoor'
            'sinkhorn_iterations': 15,  # 20 â†’ 15ë¡œ ì™„í™”
            'match_threshold': 0.05,  # 0.1 â†’ 0.05ë¡œ ì™„í™”
        },
        'loftr': {
            # LoFTR ê´€ë ¨ íŒŒë¼ë¯¸í„° ì˜ˆì‹œ (ì‹¤ì œ ì ìš©ì€ ì´í›„ ë‹¨ê³„)
            'weights': 'outdoor',
        },
        'lightglue': {
            # LightGlue ê´€ë ¨ íŒŒë¼ë¯¸í„° ì˜ˆì‹œ (ì‹¤ì œ ì ìš©ì€ ì´í›„ ë‹¨ê³„)
            'weights': 'outdoor',
        }
    }
    
    # SuperGlue 3DGS íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    try:
        pipeline = SuperGlue3DGSPipeline(config)
        print("âœ… SuperGlue pipeline initialized successfully")
    except Exception as e:
        print(f"âŒ Failed to initialize SuperGlue pipeline: {e}")
        print("Falling back to simple camera arrangement...")
        return _create_fallback_scene_info(images_folder, max_images)
    
    try:
        scene_info = pipeline.process_images_to_3dgs(
            image_dir=images_folder,
            output_dir=output_folder,
            max_images=max_images
        )
        
        print(f"\n=== SuperGlue SfM Results ===")
        print(f"Training cameras: {len(scene_info.train_cameras)}")
        print(f"Test cameras: {len(scene_info.test_cameras)}")
        print(f"3D points: {len(scene_info.point_cloud.points)}")
        print(f"Scene radius: {scene_info.nerf_normalization['radius']:.3f}")
        
        return scene_info
        
    except Exception as e:
        print(f"SuperGlue SfM failed: {e}")
        import traceback
        traceback.print_exc()
        
        # ì‹¤íŒ¨ì‹œ fallback
        print("Falling back to simple camera arrangement...")
        return _create_fallback_scene_info(images_folder, max_images)


def _create_fallback_scene_info(images_folder, max_images):
    """ê°œì„ ëœ fallback scene ìƒì„±"""
    try:
        # 3DGS ëª¨ë“ˆ import ì‹œë„
        try:
            CameraInfo, SceneInfo, BasicPointCloud = get_3dgs_imports()
        except Exception as e:
            print(f"âš ï¸  3DGS modules not available: {e}")
            # Fallback í´ë˜ìŠ¤ ì •ì˜
            class CameraInfo:
                def __init__(self, uid, R, T, FovY, FovX, image_path, image_name, 
                             width, height, depth_params=None, depth_path="", is_test=False):
                    self.uid = uid
                    self.R = R
                    self.T = T
                    self.FovY = FovY
                    self.FovX = FovX
                    self.image_path = image_path
                    self.image_name = image_name
                    self.width = width
                    self.height = height
                    self.depth_params = depth_params
                    self.depth_path = depth_path
                    self.is_test = is_test
            
            class SceneInfo:
                def __init__(self, point_cloud, train_cameras, test_cameras, 
                             nerf_normalization, ply_path="", is_nerf_synthetic=False):
                    self.point_cloud = point_cloud
                    self.train_cameras = train_cameras
                    self.test_cameras = test_cameras
                    self.nerf_normalization = nerf_normalization
                    self.ply_path = ply_path
                    self.is_nerf_synthetic = is_nerf_synthetic
        
        # ì´ë¯¸ì§€ ìˆ˜ì§‘
        image_paths = []
        for ext in ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']:
            image_paths.extend(glob.glob(str(Path(images_folder) / ext)))
        
        image_paths.sort()
        image_paths = image_paths[:max_images]
        
        if not image_paths:
            raise ValueError(f"No images found in {images_folder}")
        
        print(f"ğŸ“¸ Found {len(image_paths)} images")
        
        # ì¹´ë©”ë¼ ì •ë³´ ìƒì„±
        cam_infos = []
        for i, image_path in enumerate(image_paths):
            try:
                # ì´ë¯¸ì§€ í¬ê¸° í™•ì¸
                image = Image.open(image_path)
                width, height = image.size
                
                # ì›í˜• ë°°ì¹˜ë¡œ ì¹´ë©”ë¼ ë°°ì¹˜
                angle = i * (2 * np.pi / len(image_paths))
                radius = 3.0
                
                # ì¹´ë©”ë¼ í¬ì¦ˆ (ì›ì„ ë°”ë¼ë³´ë„ë¡)
                camera_pos = np.array([
                    radius * np.cos(angle),
                    0.0,  # ë†’ì´ ê³ ì •
                    radius * np.sin(angle)
                ])
                
                # ì›ì ì„ í–¥í•˜ëŠ” ë°©í–¥
                look_at = np.array([0.0, 0.0, 0.0])
                up = np.array([0.0, 1.0, 0.0])
                
                # ì¹´ë©”ë¼ íšŒì „ í–‰ë ¬ ê³„ì‚°
                forward = look_at - camera_pos
                forward = forward / np.linalg.norm(forward)
                right = np.cross(forward, up)
                right = right / np.linalg.norm(right)
                up = np.cross(right, forward)
                
                R = np.array([right, up, -forward]).T  # OpenCV ì»¨ë²¤ì…˜
                T = camera_pos
                
                # FOV ê³„ì‚° (ë” ì•ˆì „í•œ ê°’ë“¤)
                focal_length = max(width, height) * 0.8
                FovX = 2 * np.arctan(width / (2 * focal_length))
                FovY = 2 * np.arctan(height / (2 * focal_length))
                
                # í…ŒìŠ¤íŠ¸ ì¹´ë©”ë¼ ì„ íƒ (ë” ê· ë“±í•˜ê²Œ ë¶„ì‚°)
                is_test = (i % 8 == 0)  # 8ê°œë§ˆë‹¤ 1ê°œì”© í…ŒìŠ¤íŠ¸
                
                cam_info = CameraInfo(
                    uid=i,
                    R=R.astype(np.float32),
                    T=T.astype(np.float32),
                    FovY=float(FovY),
                    FovX=float(FovX),
                    image_path=image_path,
                    image_name=Path(image_path).name,
                    width=int(width),
                    height=int(height),
                    depth_params=None,
                    depth_path="",
                    is_test=is_test
                )
                cam_infos.append(cam_info)
                
            except Exception as e:
                print(f"    Warning: Failed to process {image_path}: {e}")
                continue
        
        if not cam_infos:
            raise ValueError("No valid cameras created")
        
        # ê°œì„ ëœ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ìƒì„±
        n_points = 12000  # 8000 â†’ 12000ë¡œ ì¦ê°€
        
        # ë” í˜„ì‹¤ì ì¸ 3D í¬ì¸íŠ¸ ë¶„í¬
        # êµ¬í˜• ë¶„í¬ + ì¼ë¶€ í‰ë©´ êµ¬ì¡°
        points_sphere = np.random.randn(n_points // 2, 3).astype(np.float32)
        points_sphere = points_sphere / np.linalg.norm(points_sphere, axis=1, keepdims=True) * 3.0  # 2.0 â†’ 3.0
        
        # í‰ë©´ êµ¬ì¡° ì¶”ê°€ (ë°”ë‹¥ë©´)
        points_plane = np.random.randn(n_points // 2, 3).astype(np.float32)
        points_plane[:, 1] = np.abs(points_plane[:, 1]) * 0.2 - 1.0  # ë°”ë‹¥ ê·¼ì²˜ (0.1 â†’ 0.2, -0.5 â†’ -1.0)
        points_plane[:, [0, 2]] *= 2.0  # 1.5 â†’ 2.0
        
        points = np.vstack([points_sphere, points_plane])
        
        # ë” í˜„ì‹¤ì ì¸ ìƒ‰ìƒ (íšŒìƒ‰ì¡° + ì•½ê°„ì˜ ìƒ‰ìƒ)
        colors = np.random.rand(n_points, 3).astype(np.float32)
        colors = colors * 0.5 + 0.3  # 0.3-0.8 ë²”ìœ„
        
        # ë²•ì„  ë²¡í„° (ë¬´ì‘ìœ„ì§€ë§Œ ì •ê·œí™”ë¨)
        normals = np.random.randn(n_points, 3).astype(np.float32)
        normals = normals / (np.linalg.norm(normals, axis=1, keepdims=True) + 1e-10)
        
        # BasicPointCloud ìƒì„± ì‹œ ì°¨ì› í™•ì¸
        assert points.shape == (n_points, 3), f"Points shape error: {points.shape}"
        assert colors.shape == (n_points, 3), f"Colors shape error: {colors.shape}"
        assert normals.shape == (n_points, 3), f"Normals shape error: {normals.shape}"
        
        # BasicPointCloudê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ fallback
        try:
            pcd = BasicPointCloud(
                points=points,
                colors=colors,
                normals=normals
            )
        except NameError:
            # BasicPointCloudê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ê°„ë‹¨í•œ í´ë˜ìŠ¤ ìƒì„±
            class BasicPointCloud:
                def __init__(self, points, colors, normals):
                    self.points = points
                    self.colors = colors
                    self.normals = normals
            
            pcd = BasicPointCloud(
                points=points,
                colors=colors,
                normals=normals
            )
        
        # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
        train_cams = [c for c in cam_infos if not c.is_test]
        test_cams = [c for c in cam_infos if c.is_test]
        
        # NeRF ì •ê·œí™” (ê°œì„ ëœ ë²„ì „)
        if train_cams:
            camera_centers = []
            for cam in train_cams:
                # ì¹´ë©”ë¼ ì¤‘ì‹¬ ê³„ì‚°
                center = -cam.R.T @ cam.T
                camera_centers.append(center)
            
            camera_centers = np.array(camera_centers)
            scene_center = np.mean(camera_centers, axis=0)
            distances = np.linalg.norm(camera_centers - scene_center, axis=1)
            scene_radius = np.max(distances) * 1.2
            
            # ìµœì†Œ/ìµœëŒ€ ì œí•œ
            scene_radius = max(scene_radius, 1.0)
            scene_radius = min(scene_radius, 10.0)
        else:
            scene_center = np.zeros(3)
            scene_radius = 3.0
        
        nerf_normalization = {
            "translate": -scene_center.astype(np.float32),
            "radius": float(scene_radius)
        }
        
        scene_info = SceneInfo(
            point_cloud=pcd,
            train_cameras=train_cams,
            test_cameras=test_cams,
            nerf_normalization=nerf_normalization,
            ply_path="",
            is_nerf_synthetic=False
        )
        
        print(f"âœ“ Fallback scene created:")
        print(f"  - {len(train_cams)} training cameras")
        print(f"  - {len(test_cams)} test cameras")
        print(f"  - {n_points} 3D points")
        print(f"  - Scene radius: {scene_radius:.2f}")
        
        return scene_info
        
    except Exception as e:
        print(f"Failed to create fallback scene: {e}")
        raise


# ëª…ë ¹ì¤„ ì¸í„°í˜ì´ìŠ¤
def main():
    """ëª…ë ¹ì¤„ì—ì„œ SuperGlue 3DGS íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    import argparse
    
    parser = argparse.ArgumentParser(description="SuperGlue 3DGS Pipeline")
    parser.add_argument("--input", "-i", required=True, 
                       help="Input images directory")
    parser.add_argument("--output", "-o", required=True,
                       help="Output directory for 3DGS training")
    parser.add_argument("--max_images", type=int, default=100,
                       help="Maximum number of images to process")
    parser.add_argument("--config", choices=["indoor", "outdoor"], 
                       default="outdoor", help="SuperGlue configuration")
    parser.add_argument("--device", default="cuda", 
                       help="Device to use (cuda/cpu)")
    
    args = parser.parse_args()
    
    # SuperGlue ì„¤ì •
    config = {
        'matcher': 'superglue',  # ì¶”ê°€: matcher íƒ€ì…(superglue, loftr, lightglue ë“±)
        'superpoint': {
            'nms_radius': 3,  # 4 â†’ 3ìœ¼ë¡œ ì™„í™”
            'keypoint_threshold': 0.001,  # 0.005 â†’ 0.001ë¡œ ëŒ€í­ ì™„í™”
            'max_keypoints': 8192  # 4096 â†’ 8192ë¡œ ì¦ê°€
        },
        'superglue': {
            'weights': args.config,
            'sinkhorn_iterations': 15,  # 20 â†’ 15ë¡œ ì™„í™”
            'match_threshold': 0.05,  # 0.1 â†’ 0.05ë¡œ ì™„í™”
        },
        'loftr': {
            # LoFTR ê´€ë ¨ íŒŒë¼ë¯¸í„° ì˜ˆì‹œ (ì‹¤ì œ ì ìš©ì€ ì´í›„ ë‹¨ê³„)
            'weights': 'outdoor',
        },
        'lightglue': {
            # LightGlue ê´€ë ¨ íŒŒë¼ë¯¸í„° ì˜ˆì‹œ (ì‹¤ì œ ì ìš©ì€ ì´í›„ ë‹¨ê³„)
            'weights': 'outdoor',
        }
    }
    
    print(f"=== SuperGlue 3DGS Pipeline ===")
    print(f"Input: {args.input}")
    print(f"Output: {args.output}")
    print(f"Max images: {args.max_images}")
    print(f"Config: {args.config}")
    print(f"Device: {args.device}")
    
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    pipeline = SuperGlue3DGSPipeline(config, device=args.device)
    
    try:
        scene_info = pipeline.process_images_to_3dgs(
            image_dir=args.input,
            output_dir=args.output,
            max_images=args.max_images
        )
        
        print(f"\n=== Success! ===")
        print(f"Processed {len(scene_info.train_cameras)} training cameras")
        print(f"Generated {len(scene_info.point_cloud.points)} 3D points")
        print(f"Files saved to: {args.output}")
        print(f"\nNext step:")
        print(f"python train.py -s {args.output} -m {args.output}/3dgs_output")
        
    except Exception as e:
        print(f"\n=== Error! ===")
        print(f"Pipeline failed: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    sys.exit(main())