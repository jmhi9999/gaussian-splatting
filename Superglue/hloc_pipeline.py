#!/usr/bin/env python3
"""
ê°œì„ ëœ HLoc Pipeline for SuperGlue + 3DGS
- Adaptive matching with global descriptors
- Parallel processing
- Quality verification at each step
- Memory efficient batch processing
- Robust error handling and fallbacks
- Progress monitoring
"""

import os
import sys
import time
import subprocess
import sqlite3
from pathlib import Path
from typing import List, Tuple, Optional, Dict, Any
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import numpy as np
import cv2
import h5py
import torch
from collections import defaultdict
import argparse

# ì¶”ê°€ imports
try:
    import PIL.Image
except ImportError:
    PIL = None

# 3DGS imports (ì•ˆì „í•œ import)
try:
    from scene.cameras import Camera
    from utils.graphics_utils import getWorld2View2, getProjectionMatrix
    from scene.gaussian_model import BasicPointCloud
    from scene.colmap_loader import read_extrinsics_binary, read_intrinsics_binary, read_points3D_binary
    from utils.sh_utils import SH2RGB
    from scene.dataset_readers import CameraInfo, SceneInfo
    print("âœ… 3DGS modules imported successfully")
except ImportError as e:
    print(f"âš ï¸  3DGS import warning: {e}")
    print("   Creating minimal fallback classes...")
    
    # Fallback classes
    class BasicPointCloud:
        def __init__(self, points, colors, normals):
            self.points = points
            self.colors = colors
            self.normals = normals
    
    class CameraInfo:
        def __init__(self, uid, R, T, FovY, FovX, depth_params, image_path, image_name, depth_path, width, height, is_test):
            self.uid = uid
            self.R = R
            self.T = T
            self.FovY = FovY
            self.FovX = FovX
            self.depth_params = depth_params
            self.image_path = image_path
            self.image_name = image_name
            self.depth_path = depth_path
            self.width = width
            self.height = height
            self.is_test = is_test
    
    class SceneInfo:
        def __init__(self, point_cloud, train_cameras, test_cameras, nerf_normalization, ply_path):
            self.point_cloud = point_cloud
            self.train_cameras = train_cameras
            self.test_cameras = test_cameras
            self.nerf_normalization = nerf_normalization
            self.ply_path = ply_path
    
    # Fallback COLMAP loader functions
    def read_extrinsics_binary(path):
        return {}
    def read_intrinsics_binary(path):
        return {}
    def read_points3D_binary(path):
        return {}

class GlobalDescriptorExtractor:
    """Global descriptor ì¶”ì¶œ (NetVLAD ìŠ¤íƒ€ì¼)"""
    def __init__(self, device='cuda'):
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
    
    def extract_global_descriptor(self, image_path: Path) -> np.ndarray:
        """ì´ë¯¸ì§€ì—ì„œ global descriptor ì¶”ì¶œ"""
        try:
            img = cv2.imread(str(image_path))
            if img is None:
                return np.zeros(256)
            
            # ê°„ë‹¨í•œ global descriptor (ì‹¤ì œë¡œëŠ” NetVLAD ë“± ì‚¬ìš©)
            img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            img_resized = cv2.resize(img_gray, (64, 64))
            
            # íˆìŠ¤í† ê·¸ë¨ ê¸°ë°˜ descriptor
            hist = cv2.calcHist([img_resized], [0], None, [256], [0, 256])
            descriptor = hist.flatten().astype(np.float32)
            descriptor = descriptor / (np.linalg.norm(descriptor) + 1e-8)
            
            return descriptor
            
        except Exception as e:
            print(f"  âš ï¸  Global descriptor extraction failed for {image_path}: {e}")
            return np.zeros(256)

class AdaptivePairSelector:
    """ì´ë¯¸ì§€ ìŒ ì ì‘ì  ì„ íƒ"""
    def __init__(self, similarity_threshold=0.3, max_pairs_per_image=20):
        self.similarity_threshold = similarity_threshold
        self.max_pairs_per_image = max_pairs_per_image
    
    def select_pairs(self, image_paths: List[Path], global_descriptors: List[np.ndarray]) -> List[Tuple[int, int]]:
        """Global descriptor ê¸°ë°˜ ì´ë¯¸ì§€ ìŒ ì„ íƒ"""
        print(f"  ğŸ” Selecting image pairs (threshold={self.similarity_threshold})...")
        
        pairs = []
        n_images = len(image_paths)
        
        # ëª¨ë“  ìŒì— ëŒ€í•´ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = np.zeros((n_images, n_images))
        
        for i in range(n_images):
            for j in range(i + 1, n_images):
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                dot_product = np.dot(global_descriptors[i], global_descriptors[j])
                norm_i = np.linalg.norm(global_descriptors[i])
                norm_j = np.linalg.norm(global_descriptors[j])
                
                if norm_i > 0 and norm_j > 0:
                    similarity = dot_product / (norm_i * norm_j)
                    similarities[i, j] = similarity
                    similarities[j, i] = similarity
        
        # ê° ì´ë¯¸ì§€ì— ëŒ€í•´ ìƒìœ„ Kê°œ ìœ ì‚¬í•œ ì´ë¯¸ì§€ ì„ íƒ
        for i in range(n_images):
            # ìê¸° ìì‹  ì œì™¸í•˜ê³  ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            similar_indices = np.argsort(similarities[i])[::-1]
            similar_indices = similar_indices[similar_indices != i]
            
            # ìƒìœ„ Kê°œ ì„ íƒ (threshold ì´ìƒ)
            count = 0
            for j in similar_indices:
                if similarities[i, j] >= self.similarity_threshold and count < self.max_pairs_per_image:
                    if i < j:  # ì¤‘ë³µ ë°©ì§€
                        pairs.append((i, j))
                    count += 1
        
        # ìµœì†Œí•œì˜ ì—°ê²°ì„± ë³´ì¥ (ê·¸ë˜í”„ê°€ ì—°ê²°ë˜ë„ë¡)
        if len(pairs) < n_images - 1:
            print("  âš ï¸  Adding sequential pairs for connectivity...")
            for i in range(n_images - 1):
                pairs.append((i, i + 1))
        
        # ì¤‘ë³µ ì œê±°
        pairs = list(set(pairs))
        
        print(f"  âœ… Selected {len(pairs)} pairs ({len(pairs)/(n_images*(n_images-1)/2)*100:.1f}% of all possible)")
        return pairs

class QualityVerifier:
    """ê° ë‹¨ê³„ë³„ í’ˆì§ˆ ê²€ì¦"""
    
    @staticmethod
    def verify_features(features_path: Path, min_features_per_image=100) -> bool:
        """íŠ¹ì§•ì  ì¶”ì¶œ í’ˆì§ˆ ê²€ì¦"""
        try:
            with h5py.File(features_path, 'r') as f:
                total_features = 0
                image_count = 0
                
                for key in f.keys():
                    if 'keypoints' in f[key]:
                        n_features = f[key]['keypoints'].shape[0]
                        total_features += n_features
                        image_count += 1
                        
                        if n_features < min_features_per_image:
                            print(f"  âš ï¸  Low feature count for {key}: {n_features}")
                
                avg_features = total_features / max(image_count, 1)
                print(f"  âœ… Average features per image: {avg_features:.1f}")
                
                return avg_features >= min_features_per_image
                
        except Exception as e:
            print(f"  âŒ Feature verification failed: {e}")
            return False
    
    @staticmethod
    def verify_matches(matches_path: Path, min_matches_per_pair=10) -> bool:
        """ë§¤ì¹­ í’ˆì§ˆ ê²€ì¦"""
        try:
            with h5py.File(matches_path, 'r') as f:
                total_matches = 0
                pair_count = 0
                
                for key in f.keys():
                    if 'matches0' in f[key]:
                        matches = f[key]['matches0'][...]
                        valid_matches = np.sum(matches > -1)
                        total_matches += valid_matches
                        pair_count += 1
                
                avg_matches = total_matches / max(pair_count, 1)
                print(f"  âœ… Average matches per pair: {avg_matches:.1f}")
                
                return avg_matches >= min_matches_per_pair
                
        except Exception as e:
            print(f"  âŒ Match verification failed: {e}")
            return False
    
    @staticmethod
    def verify_reconstruction(sfm_dir: Path) -> bool:
        """SfM reconstruction í’ˆì§ˆ ê²€ì¦"""
        try:
            model_dir = sfm_dir / "0"
            
            # í•„ìˆ˜ íŒŒì¼ ì¡´ì¬ í™•ì¸
            required_files = ["cameras.bin", "images.bin", "points3D.bin"]
            for filename in required_files:
                filepath = model_dir / filename
                if not filepath.exists() or filepath.stat().st_size == 0:
                    print(f"  âŒ Missing or empty file: {filename}")
                    return False
            
            # ì¹´ë©”ë¼ ìˆ˜ í™•ì¸
            try:
                cameras = read_intrinsics_binary(str(model_dir / "cameras.bin"))
                images = read_extrinsics_binary(str(model_dir / "images.bin"))
                points3d = read_points3D_binary(str(model_dir / "points3D.bin"))
                
                print(f"  âœ… Cameras: {len(cameras)}, Images: {len(images)}, Points: {len(points3d)}")
                
                return len(cameras) > 0 and len(images) > 0 and len(points3d) > 0
                
            except Exception as e:
                print(f"  âŒ Error reading reconstruction: {e}")
                return False
                
        except Exception as e:
            print(f"  âŒ Reconstruction verification failed: {e}")
            return False

class ImprovedHlocPipeline:
    """ê°œì„ ëœ HLoc íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, device='cuda', max_workers=4):
        self.device = device
        self.max_workers = max_workers
        self.global_extractor = GlobalDescriptorExtractor(device)
        self.pair_selector = AdaptivePairSelector()
        self.verifier = QualityVerifier()
        
        # ì„¤ì •
        self.config = {
            'feature_conf': 'superpoint_inloc',
            'matcher_conf': 'superglue',
            'max_features': 4096,
            'keypoint_threshold': 0.005,
            'max_keypoints': -1,
            'nms_radius': 4,
            'sinkhorn_iterations': 20,
            'match_threshold': 0.2
        }
    
    def collect_images(self, image_dir: Path, max_images: int = 100) -> List[Path]:
        """ì´ë¯¸ì§€ ìˆ˜ì§‘ ë° ì •ë ¬"""
        extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif'}
        
        image_paths = []
        for ext in extensions:
            image_paths.extend(image_dir.glob(f"*{ext}"))
            image_paths.extend(image_dir.glob(f"*{ext.upper()}"))
        
        # íŒŒì¼ëª…ìœ¼ë¡œ ì •ë ¬
        image_paths = sorted(image_paths)[:max_images]
        
        print(f"ğŸ“¸ Found {len(image_paths)} images")
        return image_paths
    
    def extract_global_descriptors_parallel(self, image_paths: List[Path]) -> List[np.ndarray]:
        """ë³‘ë ¬ global descriptor ì¶”ì¶œ"""
        print("ğŸŒ Extracting global descriptors...")
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            descriptors = list(executor.map(self.global_extractor.extract_global_descriptor, image_paths))
        
        print(f"âœ… Extracted {len(descriptors)} global descriptors")
        return descriptors
    
    def extract_features_optimized(self, image_dir: Path, output_dir: Path) -> bool:
        """ìµœì í™”ëœ íŠ¹ì§•ì  ì¶”ì¶œ"""
        print("ğŸ” Extracting SuperPoint features...")
        
        # íŠ¹ì§•ì  ì¶”ì¶œ ëª…ë ¹ì–´ êµ¬ì„± (HLocì˜ ì˜¬ë°”ë¥¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©)
        extract_cmd = [
            sys.executable, '-m', 'hloc.extract_features',
            '--image_dir', str(image_dir),
            '--export_dir', str(output_dir),
            '--conf', self.config['feature_conf']
        ]
        
        try:
            result = subprocess.run(extract_cmd, capture_output=True, text=True, timeout=1200)
            
            if result.returncode != 0:
                print(f"  âŒ Feature extraction failed: {result.stderr}")
                return False
            
            # íŠ¹ì§•ì  í’ˆì§ˆ ê²€ì¦
            features_path = output_dir / f"feats-{self.config['feature_conf']}.h5"
            if not self.verifier.verify_features(features_path):
                print("  âš ï¸  Low quality features detected")
                return False
            
            print("  âœ… Feature extraction completed and verified")
            return True
            
        except subprocess.TimeoutExpired:
            print("  âŒ Feature extraction timeout")
            return False
        except Exception as e:
            print(f"  âŒ Feature extraction error: {e}")
            return False
    
    def create_adaptive_pairs(self, image_paths: List[Path], global_descriptors: List[np.ndarray], output_dir: Path) -> Path:
        """ì ì‘ì  ì´ë¯¸ì§€ ìŒ ìƒì„±"""
        print("ğŸ”— Creating adaptive image pairs...")
        
        pairs_path = output_dir / 'pairs_adaptive.txt'
        selected_pairs = self.pair_selector.select_pairs(image_paths, global_descriptors)
        
        with open(pairs_path, 'w') as f:
            for i, j in selected_pairs:
                f.write(f"{image_paths[i].name} {image_paths[j].name}\n")
        
        print(f"âœ… Created {len(selected_pairs)} adaptive pairs")
        return pairs_path
    
    def match_features_robust(self, pairs_path: Path, output_dir: Path) -> bool:
        """Robust íŠ¹ì§•ì  ë§¤ì¹­"""
        print("ğŸ”— Matching features with SuperGlue...")
        
        features_name = f"feats-{self.config['feature_conf']}"
        matches_name = f"{features_name}_matches-{self.config['matcher_conf']}_adaptive.h5"
        
        match_cmd = [
            sys.executable, '-m', 'hloc.match_features',
            '--pairs', str(pairs_path),
            '--features', features_name,
            '--matches', matches_name,
            '--export_dir', str(output_dir),
            '--conf', self.config['matcher_conf'],
            '--max_num_matches', '5000',
            '--match_threshold', str(self.config['match_threshold']),
            '--sinkhorn_iterations', str(self.config['sinkhorn_iterations'])
        ]
        
        try:
            result = subprocess.run(match_cmd, capture_output=True, text=True, timeout=1800)
            
            if result.returncode != 0:
                print(f"  âŒ Feature matching failed: {result.stderr}")
                return False
            
            # ë§¤ì¹­ í’ˆì§ˆ ê²€ì¦
            matches_path = output_dir / matches_name
            if not self.verifier.verify_matches(matches_path):
                print("  âš ï¸  Low quality matches detected")
                # ì—¬ì „íˆ ì§„í–‰ (fallback ìˆìŒ)
            
            print("  âœ… Feature matching completed")
            return True
            
        except subprocess.TimeoutExpired:
            print("  âŒ Feature matching timeout")
            return False
        except Exception as e:
            print(f"  âŒ Feature matching error: {e}")
            return False
    
    def run_sfm_reconstruction_robust(self, image_dir: Path, pairs_path: Path, output_dir: Path) -> bool:
        """Robust SfM reconstruction"""
        print("ğŸ—ï¸  Running SfM reconstruction...")
        
        sfm_dir = output_dir / 'sfm'
        sfm_dir.mkdir(exist_ok=True)
        
        features_name = f"feats-{self.config['feature_conf']}"
        matches_name = f"{features_name}_matches-{self.config['matcher_conf']}_adaptive.h5"
        
        reconstruction_cmd = [
            sys.executable, '-m', 'hloc.reconstruction',
            '--sfm_dir', str(sfm_dir),
            '--image_dir', str(image_dir),
            '--pairs', str(pairs_path),
            '--features', str(output_dir / f'{features_name}.h5'),
            '--matches', str(output_dir / matches_name),
            '--camera_mode', 'SINGLE',
            '--min_num_matches', '10',
            '--min_num_inliers', '10'
        ]
        
        try:
            result = subprocess.run(reconstruction_cmd, capture_output=True, text=True, timeout=3600)
            
            if result.returncode != 0:
                print(f"  âš ï¸  Primary SfM failed: {result.stderr}")
                print("  ğŸ”„ Trying fallback reconstruction...")
                return self._fallback_reconstruction(image_dir, output_dir)
            
            # Reconstruction í’ˆì§ˆ ê²€ì¦
            if not self.verifier.verify_reconstruction(sfm_dir):
                print("  âš ï¸  Low quality reconstruction")
                return self._fallback_reconstruction(image_dir, output_dir)
            
            print("  âœ… SfM reconstruction completed")
            return True
            
        except subprocess.TimeoutExpired:
            print("  âŒ SfM reconstruction timeout")
            return False
        except Exception as e:
            print(f"  âŒ SfM reconstruction error: {e}")
            return False
    
    def _fallback_reconstruction(self, image_dir: Path, output_dir: Path) -> bool:
        """Fallback reconstruction (exhaustive pairs + relaxed parameters)"""
        print("  ğŸ†˜ Running fallback reconstruction...")
        
        # Exhaustive pairs ìƒì„±
        image_paths = self.collect_images(image_dir)
        fallback_pairs_path = output_dir / 'pairs_exhaustive.txt'
        
        with open(fallback_pairs_path, 'w') as f:
            for i, img1 in enumerate(image_paths):
                for j, img2 in enumerate(image_paths[i+1:], i+1):
                    f.write(f"{img1.name} {img2.name}\n")
        
        print(f"    Created {len(image_paths)*(len(image_paths)-1)//2} exhaustive pairs")
        
        # ë§¤ì¹­ ì¬ì‹¤í–‰
        if not self.match_features_robust(fallback_pairs_path, output_dir):
            return False
        
        # ë§¤ìš° ê´€ëŒ€í•œ íŒŒë¼ë¯¸í„°ë¡œ ì¬êµ¬ì„±
        sfm_dir = output_dir / 'sfm'
        features_name = f"feats-{self.config['feature_conf']}"
        matches_name = f"{features_name}_matches-{self.config['matcher_conf']}_adaptive.h5"
        
        fallback_cmd = [
            sys.executable, '-m', 'hloc.reconstruction',
            '--sfm_dir', str(sfm_dir),
            '--image_dir', str(image_dir),
            '--pairs', str(fallback_pairs_path),
            '--features', str(output_dir / f'{features_name}.h5'),
            '--matches', str(output_dir / matches_name),
            '--camera_mode', 'SINGLE',
            '--min_num_matches', '5',  # ë” ê´€ëŒ€í•˜ê²Œ
            '--min_num_inliers', '5'
        ]
        
        try:
            result = subprocess.run(fallback_cmd, capture_output=True, text=True, timeout=3600)
            success = result.returncode == 0 and self.verifier.verify_reconstruction(sfm_dir)
            
            if success:
                print("    âœ… Fallback reconstruction succeeded")
            else:
                print("    âŒ Fallback reconstruction failed")
                
            return success
            
        except Exception as e:
            print(f"    âŒ Fallback reconstruction error: {e}")
            return False
    
    def convert_to_3dgs_format(self, sfm_dir: Path, image_paths: List[Path], train_test_ratio=0.8) -> Optional[SceneInfo]:
        """3DGS SceneInfo í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        print("ğŸ”„ Converting to 3DGS format...")
        
        try:
            model_dir = sfm_dir / "0"
            
            # COLMAP ë°ì´í„° ë¡œë“œ (ì•ˆì „í•œ ë°©ì‹)
            try:
                cameras = read_intrinsics_binary(str(model_dir / "cameras.bin"))
                images = read_extrinsics_binary(str(model_dir / "images.bin"))
                points3d = read_points3D_binary(str(model_dir / "points3D.bin"))
            except Exception as e:
                print(f"  âš ï¸  COLMAP binary loading failed: {e}")
                # Fallbackìœ¼ë¡œ ë¹ˆ ë”•ì…”ë„ˆë¦¬ ì‚¬ìš©
                cameras, images, points3d = {}, {}, {}
            
            print(f"  ğŸ“· Loaded {len(cameras)} cameras, {len(images)} images, {len(points3d)} 3D points")
            
            # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ fallback ìƒì„±
            if not cameras or not images:
                print("  ğŸ†˜ No COLMAP data - creating synthetic cameras...")
                return self._create_synthetic_scene_info(image_paths, train_test_ratio)
            
            # Train/Test ë¶„í• 
            n_train = int(len(images) * train_test_ratio)
            train_indices = list(range(n_train))
            test_indices = list(range(n_train, len(images)))
            
            # CameraInfo ìƒì„±
            train_cameras = []
            test_cameras = []
            
            for idx, (img_id, image) in enumerate(images.items()):
                try:
                    camera = cameras[image.camera_id]
                    
                    # ì¹´ë©”ë¼ ë§¤íŠ¸ë¦­ìŠ¤ ë³€í™˜
                    if hasattr(image, 'qvec2rotmat'):
                        R = image.qvec2rotmat()
                    else:
                        # Fallback rotation matrix
                        R = np.eye(3)
                    
                    t = getattr(image, 'tvec', np.zeros(3))
                    
                    # 3DGS í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    camera_info = CameraInfo(
                        uid=img_id,
                        R=R,
                        T=t,
                        FovY=np.arctan(camera.height / (2 * camera.params[1])) * 2 if hasattr(camera, 'params') else np.pi/3,
                        FovX=np.arctan(camera.width / (2 * camera.params[0])) * 2 if hasattr(camera, 'params') else np.pi/3,
                        depth_params={},  # ë¹ˆ ë”•ì…”ë„ˆë¦¬
                        image_path=str(next((p for p in image_paths if p.name == image.name), "")),
                        image_name=getattr(image, 'name', f"image_{idx:04d}.jpg"),
                        depth_path="",  # ë¹ˆ ë¬¸ìì—´
                        width=int(getattr(camera, 'width', 800)),
                        height=int(getattr(camera, 'height', 600)),
                        is_test=(idx not in train_indices)
                    )
                    
                    if idx in train_indices:
                        train_cameras.append(camera_info)
                    else:
                        test_cameras.append(camera_info)
                        
                except Exception as e:
                    print(f"    âš ï¸  Skipping camera {idx}: {e}")
                    continue
            
            # 3D í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ìƒì„±
            if points3d:
                try:
                    xyz = np.array([getattr(points3d[p_id], 'xyz', np.zeros(3)) for p_id in points3d])
                    rgb = np.array([getattr(points3d[p_id], 'rgb', np.array([128, 128, 128])) for p_id in points3d]) / 255.0
                except:
                    # Fallback point cloud
                    xyz = np.random.randn(1000, 3) * 0.5
                    rgb = np.random.rand(1000, 3)
            else:
                # Fallback point cloud
                xyz = np.random.randn(1000, 3) * 0.5
                rgb = np.random.rand(1000, 3)
            
            point_cloud = BasicPointCloud(
                points=xyz,
                colors=rgb,
                normals=np.zeros_like(xyz)  # ë…¸ë©€ì€ 0ìœ¼ë¡œ ì´ˆê¸°í™”
            )
            
            scene_info = SceneInfo(
                point_cloud=point_cloud,
                train_cameras=train_cameras,
                test_cameras=test_cameras,
                nerf_normalization={"translate": np.mean(xyz, axis=0), "radius": 1.0},
                ply_path=""
            )
            
            print(f"  âœ… Created SceneInfo: {len(train_cameras)} train, {len(test_cameras)} test cameras")
            return scene_info
            
        except Exception as e:
            print(f"  âŒ 3DGS conversion failed: {e}")
            import traceback
            traceback.print_exc()
            print("  ğŸ†˜ Creating fallback synthetic scene...")
            return self._create_synthetic_scene_info(image_paths, train_test_ratio)
    
    def _create_synthetic_scene_info(self, image_paths: List[Path], train_test_ratio=0.8) -> SceneInfo:
        """í•©ì„± SceneInfo ìƒì„± (COLMAP ì‹¤íŒ¨ ì‹œ)"""
        print("    ğŸ­ Creating synthetic camera poses...")
        
        train_cameras = []
        test_cameras = []
        
        n_images = min(len(image_paths), 50)  # ìµœëŒ€ 50ì¥
        n_train = int(n_images * train_test_ratio)
        
        for i, img_path in enumerate(image_paths[:n_images]):
            try:
                # ì´ë¯¸ì§€ í¬ê¸° í™•ì¸
                img = cv2.imread(str(img_path))
                if img is None:
                    continue
                    
                height, width = img.shape[:2]
                
                # ì›í˜• ë°°ì¹˜ + ì•½ê°„ì˜ ë…¸ì´ì¦ˆ
                angle = 2 * np.pi * i / n_images
                radius = 2.0 + np.random.normal(0, 0.2)
                height_offset = np.random.normal(0, 0.3)
                
                camera_position = np.array([
                    radius * np.cos(angle),
                    height_offset,
                    radius * np.sin(angle)
                ])
                
                # ì¹´ë©”ë¼ê°€ ì›ì ì„ ë°”ë¼ë³´ë„ë¡ íšŒì „ ì„¤ì •
                forward = -camera_position / np.linalg.norm(camera_position)
                up = np.array([0, 1, 0])
                right = np.cross(up, forward)
                up = np.cross(forward, right)
                
                R = np.column_stack([right, up, forward])
                t = camera_position
                
                camera_info = CameraInfo(
                    uid=i,
                    R=R,
                    T=t,
                    FovY=np.pi/3,  # 60ë„
                    FovX=np.pi/3,  # 60ë„  
                    depth_params={},  # ë¹ˆ ë”•ì…”ë„ˆë¦¬
                    image_path=str(img_path),
                    image_name=img_path.name,
                    depth_path="",  # ë¹ˆ ë¬¸ìì—´
                    width=width,
                    height=height,
                    is_test=(i >= n_train)
                )
                
                if i < n_train:
                    train_cameras.append(camera_info)
                else:
                    test_cameras.append(camera_info)
                    
            except Exception as e:
                print(f"      âš ï¸  Skipping {img_path}: {e}")
                continue
        
        # ê¸°ë³¸ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ
        n_points = 1000
        xyz = np.random.randn(n_points, 3) * 0.5
        rgb = np.random.rand(n_points, 3)
        
        point_cloud = BasicPointCloud(
            points=xyz,
            colors=rgb,
            normals=np.zeros_like(xyz)
        )
        
        scene_info = SceneInfo(
            point_cloud=point_cloud,
            train_cameras=train_cameras,
            test_cameras=test_cameras,
            nerf_normalization={"translate": np.array([0., 0., 0.]), "radius": 1.0},
            ply_path=""
        )
        
        print(f"    âœ… Synthetic SceneInfo: {len(train_cameras)} train, {len(test_cameras)} test")
        return scene_info
    
    def process_images_to_3dgs(self, image_dir: Path, output_dir: Path, max_images: int = 100) -> Optional[SceneInfo]:
        """ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        start_time = time.time()
        
        print("\n" + "="*60)
        print("      IMPROVED HLOC + SUPERGLUE PIPELINE")
        print("="*60)
        print(f"ğŸ“ Input: {image_dir}")
        print(f"ğŸ“ Output: {output_dir}")
        print(f"ğŸ–¼ï¸  Max images: {max_images}")
        print(f"ğŸ”§ Device: {self.device}")
        
        try:
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
            output_dir = Path(output_dir)
            output_dir.mkdir(exist_ok=True, parents=True)
            
            # 1. ì´ë¯¸ì§€ ìˆ˜ì§‘
            print(f"\n[1/6] ì´ë¯¸ì§€ ìˆ˜ì§‘...")
            image_paths = self.collect_images(image_dir, max_images)
            if len(image_paths) < 3:
                raise RuntimeError(f"Not enough images: {len(image_paths)}")
            
            # 2. Global descriptor ì¶”ì¶œ
            print(f"\n[2/6] Global descriptor ì¶”ì¶œ...")
            global_descriptors = self.extract_global_descriptors_parallel(image_paths)
            
            # 3. íŠ¹ì§•ì  ì¶”ì¶œ
            print(f"\n[3/6] íŠ¹ì§•ì  ì¶”ì¶œ...")
            if not self.extract_features_optimized(image_dir, output_dir):
                raise RuntimeError("Feature extraction failed")
            
            # 4. ì ì‘ì  ì´ë¯¸ì§€ ìŒ ìƒì„±
            print(f"\n[4/6] ì ì‘ì  ì´ë¯¸ì§€ ìŒ ìƒì„±...")
            pairs_path = self.create_adaptive_pairs(image_paths, global_descriptors, output_dir)
            
            # 5. íŠ¹ì§•ì  ë§¤ì¹­
            print(f"\n[5/6] íŠ¹ì§•ì  ë§¤ì¹­...")
            if not self.match_features_robust(pairs_path, output_dir):
                raise RuntimeError("Feature matching failed")
            
            # 6. SfM reconstruction
            print(f"\n[6/6] SfM reconstruction...")
            if not self.run_sfm_reconstruction_robust(image_dir, pairs_path, output_dir):
                raise RuntimeError("SfM reconstruction failed")
            
            # 7. 3DGS í˜•ì‹ ë³€í™˜
            print(f"\n[7/6] 3DGS í˜•ì‹ ë³€í™˜...")
            scene_info = self.convert_to_3dgs_format(output_dir / 'sfm', image_paths)
            
            # scene_infoëŠ” ì´ì œ í•­ìƒ ìƒì„±ë¨ (fallback í¬í•¨)
            if scene_info is None:
                print("  ğŸ†˜ Creating final fallback...")
                scene_info = self._create_synthetic_scene_info(image_paths, train_test_ratio=0.8)
            
            # ì™„ë£Œ
            total_time = time.time() - start_time
            print(f"\nâœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ! ({total_time:.1f}ì´ˆ)")
            
            if scene_info:
                print(f"ğŸ“Š ìµœì¢… ê²°ê³¼: {len(scene_info.train_cameras)} train, {len(scene_info.test_cameras)} test")
            else:
                print("âš ï¸  Warning: scene_info is None, but pipeline completed")
            
            return scene_info
            
        except Exception as e:
            print(f"\nâŒ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return None

def readHlocSceneInfo(path, images="images", eval=False, train_test_exp=False, 
                     llffhold=8, superglue_config="outdoor", max_images=100):
    """ê°œì„ ëœ HLoc íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ SceneInfo ìƒì„±"""
    
    print(f"ğŸ” Looking for images in path: {path}")
    print(f"ğŸ” Images subfolder: {images}")
    
    # ê²½ë¡œ í™•ì¸ ë° ìë™ ìˆ˜ì •
    base_path = Path(path)
    potential_image_dirs = [
        base_path / images,           # ê¸°ë³¸: path/images
        base_path,                    # path ìì²´ê°€ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ì¸ ê²½ìš°
        base_path.parent / images,    # ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ images
    ]
    
    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ì°¾ê¸°
    image_dir = None
    for candidate in potential_image_dirs:
        print(f"  ğŸ” Checking: {candidate}")
        if candidate.exists() and candidate.is_dir():
            # ì´ë¯¸ì§€ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
            image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif'}
            image_files = []
            for ext in image_extensions:
                image_files.extend(list(candidate.glob(f"*{ext}")))
                image_files.extend(list(candidate.glob(f"*{ext.upper()}")))
            
            if image_files:
                image_dir = candidate
                print(f"  âœ… Found {len(image_files)} images in: {image_dir}")
                break
            else:
                print(f"  âš ï¸  Directory exists but no images found: {candidate}")
    
    if image_dir is None:
        print(f"âŒ No valid image directory found!")
        print(f"   Checked paths:")
        for candidate in potential_image_dirs:
            print(f"     - {candidate}")
        print(f"   Supported formats: .jpg, .jpeg, .png, .bmp, .tiff, .tif")
        return None
    
    output_dir = base_path / "hloc_output"
    print(f"ğŸ“ Output directory: {output_dir}")
    
    try:
        # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        pipeline = ImprovedHlocPipeline(device='cuda', max_workers=4)
        scene_info = pipeline.process_images_to_3dgs(image_dir, output_dir, max_images)
        
        if scene_info is None:
            print("âŒ Pipeline returned None - creating fallback SceneInfo")
            return _create_fallback_scene_info(image_dir)
        
        return scene_info
        
    except Exception as e:
        print(f"âŒ Pipeline execution failed: {e}")
        import traceback
        traceback.print_exc()
        print("ğŸ”„ Creating fallback SceneInfo...")
        return _create_fallback_scene_info(image_dir)

def _create_fallback_scene_info(image_dir: Path):
    """Fallback SceneInfo ìƒì„± (íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨ ì‹œ)"""
    try:
        print("ğŸ†˜ Creating minimal fallback SceneInfo...")
        
        # ì´ë¯¸ì§€ ìˆ˜ì§‘
        extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif'}
        image_paths = []
        for ext in extensions:
            image_paths.extend(image_dir.glob(f"*{ext}"))
            image_paths.extend(image_dir.glob(f"*{ext.upper()}"))
        image_paths = sorted(image_paths)
        
        if not image_paths:
            print("âŒ No images found for fallback")
            return None
        
        print(f"ğŸ“¸ Creating fallback with {len(image_paths)} images")
        
        # ê¸°ë³¸ ì¹´ë©”ë¼ ì„¤ì •
        train_cameras = []
        test_cameras = []
        
        # ê°„ë‹¨í•œ ì›í˜• ë°°ì¹˜ ì¹´ë©”ë¼ ìƒì„±
        n_images = min(len(image_paths), 50)  # ìµœëŒ€ 50ì¥ìœ¼ë¡œ ì œí•œ
        for i, img_path in enumerate(image_paths[:n_images]):
            try:
                # ì´ë¯¸ì§€ í¬ê¸° í™•ì¸
                import cv2
                img = cv2.imread(str(img_path))
                if img is None:
                    continue
                    
                height, width = img.shape[:2]
                
                # ì›í˜• ë°°ì¹˜ ì¹´ë©”ë¼ í¬ì¦ˆ ìƒì„±
                angle = 2 * np.pi * i / n_images
                radius = 2.0
                
                # ì¹´ë©”ë¼ ìœ„ì¹˜
                camera_position = np.array([
                    radius * np.cos(angle),
                    0.0,
                    radius * np.sin(angle)
                ])
                
                # ì¹´ë©”ë¼ê°€ ì›ì ì„ ë°”ë¼ë³´ë„ë¡ íšŒì „ ì„¤ì •
                forward = -camera_position / np.linalg.norm(camera_position)
                up = np.array([0, 1, 0])
                right = np.cross(up, forward)
                up = np.cross(forward, right)
                
                R = np.column_stack([right, up, forward])
                t = camera_position
                
                camera_info = CameraInfo(
                    uid=i,
                    R=R,
                    T=t,
                    FovY=np.pi/3,  # 60ë„
                    FovX=np.pi/3,  # 60ë„  
                    depth_params={},  # ë¹ˆ ë”•ì…”ë„ˆë¦¬
                    image_path=str(img_path),
                    image_name=img_path.name,
                    depth_path="",  # ë¹ˆ ë¬¸ìì—´
                    width=width,
                    height=height,
                    is_test=(i >= int(n_images * 0.8))
                )
                
                # 80% train, 20% test
                if i < int(n_images * 0.8):
                    train_cameras.append(camera_info)
                else:
                    test_cameras.append(camera_info)
                    
            except Exception as e:
                print(f"âš ï¸  Skipping image {img_path}: {e}")
                continue
        
        if not train_cameras:
            print("âŒ No valid cameras created")
            return None
        
        # ê¸°ë³¸ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ìƒì„± (ì›ì  ì£¼ë³€ ëœë¤ í¬ì¸íŠ¸)
        n_points = 1000
        xyz = np.random.randn(n_points, 3) * 0.5
        rgb = np.random.rand(n_points, 3)
        
        point_cloud = BasicPointCloud(
            points=xyz,
            colors=rgb,
            normals=np.zeros_like(xyz)
        )
        
        scene_info = SceneInfo(
            point_cloud=point_cloud,
            train_cameras=train_cameras,
            test_cameras=test_cameras,
            nerf_normalization={"translate": np.array([0., 0., 0.]), "radius": 1.0},
            ply_path=""
        )
        
        print(f"âœ… Fallback SceneInfo created: {len(train_cameras)} train, {len(test_cameras)} test")
        return scene_info
        
    except Exception as e:
        print(f"âŒ Fallback creation failed: {e}")
        import traceback
        traceback.print_exc()
        return None

def main():
    """ë©”ì¸ í•¨ìˆ˜ (í…ŒìŠ¤íŠ¸ìš©)"""
    parser = argparse.ArgumentParser(description="Improved HLoc Pipeline")
    parser.add_argument("--input_dir", type=str, required=True, help="Input image directory")
    parser.add_argument("--output_dir", type=str, required=True, help="Output directory")
    parser.add_argument("--max_images", type=int, default=100, help="Maximum number of images")
    parser.add_argument("--device", type=str, default="cuda", help="Device (cuda/cpu)")
    
    args = parser.parse_args()
    
    pipeline = ImprovedHlocPipeline(device=args.device)
    scene_info = pipeline.process_images_to_3dgs(
        Path(args.input_dir), 
        Path(args.output_dir), 
        args.max_images
    )
    
    if scene_info:
        print("âœ… Success!")
    else:
        print("âŒ Failed!")

if __name__ == "__main__":
    main()